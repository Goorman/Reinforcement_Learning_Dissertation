{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ATARI_wrapper():\n",
    "    def __init__(self, gamename = \"Enduro-v0\"):\n",
    "        self.state_size = (105, 80)\n",
    "        self.game_title = gamename\n",
    "        self.actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        grayimage = np.mean(state, axis = 2)\n",
    "        downscale = self.downscale2x(grayimage)\n",
    "        norm = (downscale - 128.0) / 128.0\n",
    "        return norm\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3\n",
    "    \n",
    "    def downscale2x(self, image):\n",
    "        image00 = image[0::2, 0::2]\n",
    "        image01 = image[0::2, 1::2]\n",
    "        image10 = image[1::2, 0::2]\n",
    "        image11 = image[1::2, 1::2]\n",
    "        return (image00 + image01 + image10 + image11) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, aelosscoef = 0.1):\n",
    "        self.input_var = T.tensor4('statebatch')\n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.newstates = T.tensor4(\"newstatebatch\")\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.aelosscoef = aelosscoef\n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape)\n",
    "        self.build_AVQ(grad_clipping, lr)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.encode = lasagne.layers.DenseLayer(self.outlayer, 4, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le3 = lasagne.layers.DenseLayer(self.encode, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le2 = lasagne.layers.DenseLayer(self.le3, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le1 = lasagne.layers.DenseLayer(self.le2, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le0 = lasagne.layers.DenseLayer(self.le1, channels_number * image_shape[0] * image_shape[1])\n",
    "        self.l_aeout = lasagne.layers.ReshapeLayer(self.le0, shape=(-1, channels_number, image_shape[0], image_shape[1]))\n",
    "    \n",
    "        self.actionlayer = lasagne.layers.InputLayer(shape=(None, self.n_actions), input_var = self.actions_onehot)\n",
    "        self.prins = lasagne.layers.ConcatLayer([self.encode, self.actionlayer], axis = 1)\n",
    "        self.pr1 = lasagne.layers.DenseLayer(self.prins, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.prencode = lasagne.layers.DenseLayer(self.pr1, 4)\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        \n",
    "        self.advantage, self.value, self.ae_out, self.enc, self.prenc = lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_aeout, self.encode, self.prencode])\n",
    "        self.targetenc = lasagne.layers.get_output(self.encode, inputs = self.newstates)\n",
    "        \n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Qout = self.advantage + self.value - self.average_advantage\n",
    "        self.Qout = self.advantage\n",
    "        self.predict = T.argmax(self.Qout, axis = 1)\n",
    "        \n",
    "        self.Q = T.sum(self.Qout * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.ae_error = T.mean(T.sqr(self.ae_out - self.input_var))\n",
    "        self.td_error = T.mean(T.sqr(self.targetQ - self.Q))\n",
    "        \n",
    "        self.loss = self.ae_error * self.aelosscoef + self.td_error\n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "        \n",
    "        self.pr_loss_batch = T.mean(T.sqr(self.targetenc - self.prenc), axis = 1)\n",
    "        self.pr_loss = T.mean(self.pr_loss_batch)\n",
    "        pr_params = [self.pr1.W, self.pr1.b, self.prencode.W, self.prencode.b]\n",
    "        self.pr_grads = T.grad(self.pr_loss, pr_params)\n",
    "        self.pr_scaled_grads = lasagne.updates.total_norm_constraint(self.pr_grads, grad_clipping)\n",
    "        self.pr_updates = lasagne.updates.adam(self.pr_scaled_grads, pr_params, learning_rate=lr)\n",
    "        \n",
    "        enc_layers = [self.l2, self.l3, self.outlayer, self.encode, self.le3, self.le2, self.le1, self.le0]\n",
    "        enc_params = [l.W for l in enc_layers] + [l.b for l in enc_layers]\n",
    "        self.enc_grads = T.grad(self.ae_error, enc_params)\n",
    "        self.enc_scaled_grads = lasagne.updates.total_norm_constraint(self.enc_grads, grad_clipping)\n",
    "        self.enc_updates = lasagne.updates.adam(self.enc_scaled_grads, enc_params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Qout)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.train_fn = theano.function([self.input_var, self.targetQ, self.actions], [self.ae_error, self.td_error], updates = self.updates)\n",
    "        self.train_predfn = theano.function([self.input_var, self.actions, self.newstates], self.pr_loss_batch, updates = self.pr_updates)\n",
    "        self.train_encoder = theano.function([self.input_var], self.ae_error, updates = self.enc_updates)\n",
    "        \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values(self.l_advantage, values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.buffer))\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "        self.maxprerror = None\n",
    "        \n",
    "        self.env = env\n",
    "        self.mainQN = AVQ_nn(self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, self.lr)\n",
    "        self.targetQN = AVQ_nn(self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, self.lr)\n",
    "\n",
    "        self.lList = []\n",
    "        self.rList = []\n",
    "        self.aeList = []\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            agent = egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            agent = boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "            \n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "        \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "            episode_aeerrors = np.array([])\n",
    "            \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                experience = np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5])\n",
    "                self.experience_storage.add(experience)\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        errors_vector = self.mainQN.train_predfn(old_state_batch, action_vector, new_state_batch)\n",
    "                        if self.maxprerror == None:\n",
    "                            self.maxprerror = np.max(errors_vector)\n",
    "                        else:\n",
    "                            self.maxprerror = max(self.maxprerror, np.max(errors_vector))\n",
    "                        rewards_vector = rewards_vector + errors_vector * 50.0 / (self.maxprerror * self.total_steps)\n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)\n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        \n",
    "                        doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        aeerror, tderror = self.mainQN.train_fn(old_state_batch, targetQ, action_vector)\n",
    "                        episode_aeerrors = np.append(episode_aeerrors, aeerror)\n",
    "                else:\n",
    "                    train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                    old_state_batch = np.stack(train_batch[:,0])\n",
    "                    new_state_batch = np.stack(train_batch[:,3])\n",
    "                    action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                    aeerror1 = self.mainQN.train_encoder(old_state_batch)\n",
    "                    aeerror2 = self.mainQN.train_encoder(new_state_batch)\n",
    "                    episode_aeerrors = np.append(episode_aeerrors, (aeerror1 + aeerror2)/2)\n",
    "                    errors_vector = self.mainQN.train_predfn(old_state_batch, action_vector, new_state_batch)\n",
    "                    \n",
    "                    if self.maxprerror == None:\n",
    "                        self.maxprerror = np.max(errors_vector)\n",
    "                    else:\n",
    "                        self.maxprerror = max(self.maxprerror, np.max(errors_vector))\n",
    "                    \n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            total_aeerror = np.mean(episode_aeerrors)\n",
    "            self.lList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            self.aeList.append(total_aeerror)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.lList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"AEerror, mean over last 10:\", str(np.mean(self.aeList[-10:]))]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.lList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-15 22:46:35,375] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "ll_env = LunarLanding_wrapper()\n",
    "#cp_env = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 1,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 125,\n",
    "           \"pretrain_steps\" : 500,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.1,\n",
    "                                \"anneling_steps\":1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_experiment(ddql, ddql_init_params, ddql_train_params, experiment_num = 5):\n",
    "    ddql_list = [ddql(**ddql_init_params) for k in range(experiment_num)]\n",
    "    \n",
    "    for k in range(experiment_num):\n",
    "        ddql_list[k].train(**ddql_train_params)\n",
    "        \n",
    "    return ddql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddql_egreedy_params = {\"lparams\":lparams, \"env\":ll_env, \"agent\":egreedyagentinfo}\n",
    "\n",
    "ddql_train_params = {\"num_episodes\":1250, \"frame_limit\":200, \"render\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 124 ================================================\n",
      "Total steps: 8624\n",
      "Episode rewards, last 10: [-153.96999334959034, -153.13061903133786, -183.03572116777679, -274.13428507286216, -144.75797053020992, -178.24794369845335, -173.17506344853368, -180.2605082356217, -153.93084050796278, -146.16374398679682]\n",
      "Mean over last 125 episodes: -186.478496685\n",
      "Episode lengths, last 10: [55, 52, 82, 87, 57, 66, 81, 53, 69, 88]\n",
      "AEerror, mean over last 10: 0.210685057374\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 17640\n",
      "Episode rewards, last 10: [-189.78161657528699, -181.93347269739405, -214.64540377849667, -196.68200055892657, -164.28663817007896, -173.81294134289539, -136.67926264006152, -175.66612843024458, -174.95205581141752, -151.05553775673235]\n",
      "Mean over last 125 episodes: -184.135335336\n",
      "Episode lengths, last 10: [54, 60, 74, 66, 69, 55, 56, 68, 61, 52]\n",
      "AEerror, mean over last 10: 0.196263678122\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 26594\n",
      "Episode rewards, last 10: [-200.90873583070763, -168.99345854448646, -217.7300543783864, -185.30568060259932, -191.81459260317808, -157.7583562807111, -144.91485254395468, -205.55068925841715, -168.09542450951096, -187.98903077950803]\n",
      "Mean over last 125 episodes: -182.609849471\n",
      "Episode lengths, last 10: [63, 61, 82, 56, 60, 64, 60, 67, 55, 86]\n",
      "AEerror, mean over last 10: 0.176698269267\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 35720\n",
      "Episode rewards, last 10: [-159.47239474030491, -219.86566318237817, -129.71670705504226, -186.94658179318304, -168.28583092689581, -158.12021463263392, -167.66714965830045, -125.47832608433976, -174.5650136159548, -202.02198800416426]\n",
      "Mean over last 125 episodes: -174.161058102\n",
      "Episode lengths, last 10: [91, 63, 66, 72, 69, 92, 83, 94, 64, 81]\n",
      "AEerror, mean over last 10: 0.148455059242\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 44686\n",
      "Episode rewards, last 10: [-173.4464310056897, -131.09751450528333, -208.61233133430724, -179.0733371231135, -159.13920879126741, -148.54026500571905, -163.82983932984251, -154.20583041663718, -160.70320554818016, -172.05480442497395]\n",
      "Mean over last 125 episodes: -178.607758614\n",
      "Episode lengths, last 10: [61, 57, 59, 63, 62, 63, 71, 64, 60, 54]\n",
      "AEerror, mean over last 10: 0.205431106091\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 53602\n",
      "Episode rewards, last 10: [-173.36061634716901, -206.65317793135404, -324.68543335902899, -181.0864986856285, -164.18642143569755, -336.28521777311835, -146.60922969826075, -137.14968209258123, -162.73084763282364, -208.30949824654743]\n",
      "Mean over last 125 episodes: -177.201984537\n",
      "Episode lengths, last 10: [85, 83, 75, 79, 56, 101, 52, 89, 72, 69]\n",
      "AEerror, mean over last 10: 0.22711654856\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 62485\n",
      "Episode rewards, last 10: [-258.2071454669466, -156.21156743446238, -180.0059270815818, -147.27878903055679, -175.96629511118635, -260.34157246353845, -206.18713896095898, -170.45429195503769, -175.06123166341428, -189.92486526164257]\n",
      "Mean over last 125 episodes: -174.032625141\n",
      "Episode lengths, last 10: [77, 56, 63, 59, 92, 77, 61, 65, 74, 78]\n",
      "AEerror, mean over last 10: 0.176776930015\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 71161\n",
      "Episode rewards, last 10: [-186.89202237969567, -151.39107490881941, -67.983211560942223, -210.38920024725459, -202.21075029514779, -161.95607344368949, -238.03992505918728, -162.97296738244961, -176.21030553689593, -251.89956004928365]\n",
      "Mean over last 125 episodes: -180.10804143\n",
      "Episode lengths, last 10: [55, 58, 67, 73, 76, 62, 71, 61, 86, 76]\n",
      "AEerror, mean over last 10: 0.162477686434\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 79987\n",
      "Episode rewards, last 10: [-130.86173992958541, -167.09125298288618, -163.70177975204018, -221.49176860022388, -212.34564462518375, -152.66834824170269, -170.11040564686454, -178.21585337435249, -159.31331448605115, -188.6817588538633]\n",
      "Mean over last 125 episodes: -177.013349477\n",
      "Episode lengths, last 10: [90, 79, 54, 67, 82, 83, 89, 93, 55, 73]\n",
      "AEerror, mean over last 10: 0.144466367506\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 88956\n",
      "Episode rewards, last 10: [-225.26071359058753, -137.36741064151849, -178.28945446713817, -225.57214075198522, -160.86826057762622, -118.08288560138757, -195.73598865170607, -230.94648886265344, -203.49434308183834, -212.24898693814652]\n",
      "Mean over last 125 episodes: -174.991081924\n",
      "Episode lengths, last 10: [74, 55, 80, 69, 58, 54, 73, 78, 76, 91]\n",
      "AEerror, mean over last 10: 0.177337462804\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 17489\n",
      "Episode rewards, last 10: [6.4563027565201985, -49.112306367602301, -65.613125440179218, -158.47496783504189, 38.424538694069703, -184.46547027681842, -139.49543204095249, -3.7732257467315486, -23.341876120940565, -140.72500671556031]\n",
      "Mean over last 125 episodes: -121.581018421\n",
      "Episode lengths, last 10: [199, 199, 199, 126, 199, 136, 145, 199, 199, 155]\n",
      "AEerror, mean over last 10: 0.100911238257\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 40003\n",
      "Episode rewards, last 10: [8.8520833772411436, -235.9028598085693, 9.0451087762190774, 10.150722081972734, -14.423802068192995, -2.8969202225409925, -279.21927445446676, -49.310368239184506, 54.367503565828059, 13.376423957418115]\n",
      "Mean over last 125 episodes: -61.8269588549\n",
      "Episode lengths, last 10: [199, 107, 199, 199, 199, 199, 198, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0907467622117\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 62632\n",
      "Episode rewards, last 10: [49.289174330853228, -114.36467752629144, -98.042969106048702, -20.903998325572129, 1.3330743675456453, -2.7034945827819499, -12.536322092673103, -225.81446718676537, 22.312793225254818, -181.36302166960647]\n",
      "Mean over last 125 episodes: -41.0830257134\n",
      "Episode lengths, last 10: [199, 90, 196, 199, 199, 199, 199, 147, 199, 106]\n",
      "AEerror, mean over last 10: 0.100770749358\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 84816\n",
      "Episode rewards, last 10: [-147.47864266650623, -93.967077015478083, -29.105457373676366, -55.200088762774683, -18.35419002269694, -63.131010167717001, -269.46120388302023, -127.48435817753483, -126.16652236692167, -26.118751167762369]\n",
      "Mean over last 125 episodes: -58.6160310997\n",
      "Episode lengths, last 10: [199, 109, 199, 199, 199, 152, 122, 140, 94, 199]\n",
      "AEerror, mean over last 10: 0.13454094685\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 106483\n",
      "Episode rewards, last 10: [-354.07369938464956, -349.97189757455271, -105.55606777741187, -101.13396894288498, -60.211621625614328, 3.5670537363445831, -13.924845778100888, -21.036028166453008, -64.283128476944512, -2.7591689353545119]\n",
      "Mean over last 125 episodes: -81.2916687919\n",
      "Episode lengths, last 10: [116, 125, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.135790161328\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 129362\n",
      "Episode rewards, last 10: [-41.013375072937329, 5.7102577128332808, -4.8119690241434014, 47.119666046639324, -3.3922878511784553, 28.005248439404127, -100.15568210047923, -90.839916998727318, 14.97544093327773, -3.6458215040713355]\n",
      "Mean over last 125 episodes: -31.8430260928\n",
      "Episode lengths, last 10: [85, 199, 199, 199, 199, 199, 191, 178, 199, 199]\n",
      "AEerror, mean over last 10: 0.13040639674\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 152763\n",
      "Episode rewards, last 10: [18.639422707275745, 55.301834508412433, -27.684795132348931, 12.854354903560736, -14.129629282130969, 41.476528722850674, 6.768188910109977, 40.741464021029408, 36.466725878077511, 11.043854385122735]\n",
      "Mean over last 125 episodes: -7.62908674088\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0897822125791\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 175188\n",
      "Episode rewards, last 10: [-103.3660801181364, 16.918631975488289, -12.080664908121395, -225.05501810864115, -17.84883771547613, -53.73575847804581, 25.889992455762432, -36.585178996665455, 10.642904899811452, -10.542523694798783]\n",
      "Mean over last 125 episodes: -61.546754589\n",
      "Episode lengths, last 10: [199, 199, 199, 154, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0954991226279\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 199093\n",
      "Episode rewards, last 10: [23.642841130802275, -27.85769437294546, 25.851573389574867, -35.906264900683468, 17.868053414964781, 22.173396512578531, 37.951122046416302, 46.865948656710934, 8.2460703512606504, 7.6627660720630519]\n",
      "Mean over last 125 episodes: -20.5758179552\n",
      "Episode lengths, last 10: [199, 199, 199, 157, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0686338740541\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 223028\n",
      "Episode rewards, last 10: [22.147854679650923, 35.831041412484019, 47.551391548918829, 8.7841144292753004, -155.113885799738, 11.487069018563455, 29.636729989908652, 26.369352665800029, 26.498493168653084, 25.74753969383201]\n",
      "Mean over last 125 episodes: 7.47178537001\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 115, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0805849953456\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8373\n",
      "Episode rewards, last 10: [-205.18989828782219, -187.30710571362732, -182.00730811806369, -169.99653669373453, -170.51339148677511, -155.3220466493072, -157.42179358845561, -163.24257314902732, -184.02141276923493, -149.57066470770317]\n",
      "Mean over last 125 episodes: -196.779133075\n",
      "Episode lengths, last 10: [90, 78, 55, 59, 55, 61, 54, 60, 83, 59]\n",
      "AEerror, mean over last 10: 0.173958368897\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 17379\n",
      "Episode rewards, last 10: [-176.5222186290527, -152.14358086041241, -47.049501400809746, -186.61713764953157, -231.29682792067331, -162.34453856453081, -184.96440918362796, -46.194583920599285, -151.84696987250243, -176.23342014909213]\n",
      "Mean over last 125 episodes: -177.662413885\n",
      "Episode lengths, last 10: [67, 58, 66, 69, 77, 54, 74, 84, 94, 58]\n",
      "AEerror, mean over last 10: 0.174967171741\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 26203\n",
      "Episode rewards, last 10: [-155.74436751892299, -129.62432409444534, -18.946653558177076, -212.96377593156762, -191.16652515440796, -182.2352341666832, -171.16445856137946, -201.34947017945171, -155.1826098826597, -179.04539747261896]\n",
      "Mean over last 125 episodes: -174.238515362\n",
      "Episode lengths, last 10: [56, 54, 93, 61, 77, 80, 59, 75, 93, 61]\n",
      "AEerror, mean over last 10: 0.172225818297\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 35056\n",
      "Episode rewards, last 10: [-283.59343171021555, -211.59629549141732, -174.70377210482894, -132.49953640016136, -207.0474561423074, -145.36045749600225, -134.89541281002556, -227.2965653178951, -157.21879744273053, -221.80344741790117]\n",
      "Mean over last 125 episodes: -181.890523107\n",
      "Episode lengths, last 10: [82, 66, 59, 54, 77, 58, 57, 67, 56, 59]\n",
      "AEerror, mean over last 10: 0.229405117705\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 43966\n",
      "Episode rewards, last 10: [-175.7021537413537, -180.09259439606939, -178.4699169774575, -150.59432280526829, -176.92460456793157, -163.1029068165758, -221.89241565692214, -309.78195499946696, -187.02335861110964, -149.46334454473089]\n",
      "Mean over last 125 episodes: -178.591073731\n",
      "Episode lengths, last 10: [81, 75, 63, 75, 68, 69, 62, 90, 65, 62]\n",
      "AEerror, mean over last 10: 0.175280006698\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 52850\n",
      "Episode rewards, last 10: [-182.15366068424291, -196.07941789631144, -249.23590878528336, -163.28072329804104, -200.39680917879565, -170.10207615516876, -161.03873027102031, -197.13644813621647, -48.506779034147542, -45.719766632090909]\n",
      "Mean over last 125 episodes: -182.123631439\n",
      "Episode lengths, last 10: [72, 67, 81, 91, 72, 66, 73, 84, 84, 107]\n",
      "AEerror, mean over last 10: 0.176745856828\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 61707\n",
      "Episode rewards, last 10: [-164.52980828848212, -193.66328214097439, -186.09378949169343, -198.52143626035604, -47.740567043421905, -42.446711363007616, -175.80644245693506, -173.87610188197678, -243.60739550065074, -223.51945659383347]\n",
      "Mean over last 125 episodes: -182.006597578\n",
      "Episode lengths, last 10: [69, 64, 67, 65, 95, 97, 50, 77, 77, 88]\n",
      "AEerror, mean over last 10: 0.183616517571\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 70757\n",
      "Episode rewards, last 10: [-183.31149979145309, -138.20570693158211, -171.33841196040319, -180.0949490144904, -202.63069599836427, -169.13678442137299, -173.63514135203218, -216.06788078242437, -140.99850435730883, -171.1808093772288]\n",
      "Mean over last 125 episodes: -182.905950462\n",
      "Episode lengths, last 10: [74, 56, 54, 56, 61, 65, 86, 81, 83, 66]\n",
      "AEerror, mean over last 10: 0.197961615274\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 79570\n",
      "Episode rewards, last 10: [-192.0982950860834, -229.11362138284451, -231.92092872557046, -155.05353167027386, -159.52117285534337, -178.48827929014055, -174.75345055171545, -163.02154535855698, -87.953128483486012, -162.37780110232839]\n",
      "Mean over last 125 episodes: -177.715264572\n",
      "Episode lengths, last 10: [74, 80, 68, 57, 58, 64, 63, 64, 102, 50]\n",
      "AEerror, mean over last 10: 0.217427607573\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 88341\n",
      "Episode rewards, last 10: [-163.45635427984757, -179.27495714277151, -210.311219327571, -175.80759149313485, -193.39031754627879, -221.05259795509932, -184.41935733088903, -204.19689786468803, -242.91018355443839, -156.34890090030871]\n",
      "Mean over last 125 episodes: -185.359798061\n",
      "Episode lengths, last 10: [89, 55, 83, 55, 80, 83, 72, 58, 74, 90]\n",
      "AEerror, mean over last 10: 0.184013756155\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 13966\n",
      "Episode rewards, last 10: [-188.18925349373148, -141.37541502792561, -184.83892551392114, -237.41136302279423, -236.12493090877658, -240.40718297286196, -50.3002670247201, -181.7582977605754, -187.7419245286921, -199.86237799354248]\n",
      "Mean over last 125 episodes: -198.641495742\n",
      "Episode lengths, last 10: [62, 131, 91, 77, 61, 80, 199, 65, 104, 96]\n",
      "AEerror, mean over last 10: 0.186720474205\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 36215\n",
      "Episode rewards, last 10: [17.131703778869614, -150.62729201461818, 81.505521855941709, 30.541534959499316, -91.267826113740583, -29.454818987119275, -281.21253453354018, -204.91473712136508, 1.0104313505241027, -83.278235320320292]\n",
      "Mean over last 125 episodes: -43.4463144913\n",
      "Episode lengths, last 10: [199, 131, 199, 199, 187, 199, 106, 78, 199, 76]\n",
      "AEerror, mean over last 10: 0.409125654869\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 58735\n",
      "Episode rewards, last 10: [-9.8038742258884071, 6.4273197053565703, -283.96402481112119, -162.06447598838324, 10.252993690245935, -180.94652030790502, 10.789104935211371, 36.775691112700031, -24.957848027028863, -193.72634587330501]\n",
      "Mean over last 125 episodes: -37.3818150914\n",
      "Episode lengths, last 10: [199, 199, 119, 95, 199, 131, 199, 199, 199, 61]\n",
      "AEerror, mean over last 10: 0.159732295123\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 81069\n",
      "Episode rewards, last 10: [-131.88904994104834, 37.83549113457596, -42.345392684276227, -115.80839966663129, -18.422186293874894, -17.672285452009731, -1.9662273350893642, -238.17850420244724, -56.895983814146803, -28.729925294440932]\n",
      "Mean over last 125 episodes: -48.7213621511\n",
      "Episode lengths, last 10: [139, 199, 199, 155, 199, 199, 199, 143, 199, 199]\n",
      "AEerror, mean over last 10: 0.211771016708\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 104870\n",
      "Episode rewards, last 10: [36.371878824401279, -2.5951931496312692, 12.632505875346581, 4.5885371326793596, 4.1942833934479982, 37.402329338076846, 13.70453550458797, 32.758676332003958, 79.53110483361877, 38.403618859576461]\n",
      "Mean over last 125 episodes: -12.4597320871\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.132118468029\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 128280\n",
      "Episode rewards, last 10: [-187.51882231673727, -171.01604010727385, -246.14202449789374, 14.548659718275466, 38.885112985619521, 8.7341978300972976, 11.447586513485327, -3.1111442348003244, 43.845873241286213, -14.847583017643895]\n",
      "Mean over last 125 episodes: -12.0973781632\n",
      "Episode lengths, last 10: [91, 72, 77, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.117796716414\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 152126\n",
      "Episode rewards, last 10: [-165.25420479545545, 1.2173775179870141, 22.32388626254469, -27.79610003360283, -212.12752983832416, -203.59497912736947, -18.802987775447999, 13.424989334292345, -56.179149633672722, 45.710429474301407]\n",
      "Mean over last 125 episodes: -10.0557465\n",
      "Episode lengths, last 10: [153, 199, 199, 199, 147, 133, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.136668929508\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 176636\n",
      "Episode rewards, last 10: [64.782601407591699, 21.313065030550415, 13.777567518807999, -11.749666864483983, 36.940308334823598, 2.0429441369582371, 13.415762664051821, 10.305663557385259, 0.8328385119388777, 28.959396179319505]\n",
      "Mean over last 125 episodes: 5.36376051357\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.140469328035\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 200578\n",
      "Episode rewards, last 10: [-1.9798998211125687, -16.941283804548988, 11.173014161652032, 11.947140713296918, -6.9388993236083643, 10.615813878205735, -184.17322436225825, -153.68585060356929, 18.42288281633763, 3.4557108039922628]\n",
      "Mean over last 125 episodes: -22.4889348949\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 134, 175, 199, 199]\n",
      "AEerror, mean over last 10: 0.116369889948\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 225146\n",
      "Episode rewards, last 10: [-1.5015035583365162, 13.152905723379128, 22.217106045464352, -13.20731042308519, -265.40094431200345, -3.415782956811606, -24.671086456360719, 74.614329570422314, -3.7185394928012663, 9.7981609507922585]\n",
      "Mean over last 125 episodes: -1.55935296448\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 134, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.112337028164\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 14488\n",
      "Episode rewards, last 10: [-24.008984851577786, -6.6479467666762968, 36.508746558090877, 7.9644583510699967, -196.13760975317152, 39.349599303869589, 4.7827773711669863, -148.51914327070736, -279.74774837723169, -170.24743706287074]\n",
      "Mean over last 125 episodes: -189.9744224\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 92, 199, 199, 106, 108, 117]\n",
      "AEerror, mean over last 10: 0.163364196657\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 37003\n",
      "Episode rewards, last 10: [-51.678585494107416, -30.678927018372043, 5.782157289036463, -33.999441648269816, 0.36368804430082768, -29.629699929738713, -164.4633404681673, 19.078172185619945, -53.25196882749011, -11.252103450505924]\n",
      "Mean over last 125 episodes: -57.1238639391\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 186, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.131419147888\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 59825\n",
      "Episode rewards, last 10: [-42.969990376261343, -7.58372476051278, -2.7015295291385968, -5.7816666446624918, 18.242749473484043, 14.265618604325375, -5.2200584178970288, 5.0774175854187629, 14.233502618017784, -128.6231430498629]\n",
      "Mean over last 125 episodes: -62.6878613697\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 144]\n",
      "AEerror, mean over last 10: 0.0736320640617\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 83284\n",
      "Episode rewards, last 10: [22.487019649436533, 23.763673631403183, -198.19845778042827, 18.882074757463442, 19.7774655525049, -232.68130901510514, -107.31959241265422, -251.68066995671532, -54.895505540299467, -63.588296648444455]\n",
      "Mean over last 125 episodes: -36.3022729916\n",
      "Episode lengths, last 10: [199, 199, 189, 199, 199, 74, 132, 51, 199, 144]\n",
      "AEerror, mean over last 10: 0.62448034044\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 107189\n",
      "Episode rewards, last 10: [39.391538612700302, 35.34698361702543, 1.3659530730775007, 23.083290875635996, -15.798351040547182, 31.006007608745904, 24.51028884060446, 61.37372648526808, 29.131587934670083, 23.956197483482981]\n",
      "Mean over last 125 episodes: -28.6846676608\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.188086492046\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 131419\n",
      "Episode rewards, last 10: [-61.808001706150769, -5.7527615159535443, -7.6388478508832716, 43.995469661971839, -30.579482268058705, 15.251806359330388, 10.223320224757941, 35.911360441547174, 18.086140253180215, 41.403017969990529]\n",
      "Mean over last 125 episodes: -6.98377974851\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.109948729318\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 155406\n",
      "Episode rewards, last 10: [6.4799218219493513, -97.461183381171907, -177.64081755284903, -11.349216738212146, 24.432366322177135, 5.9570560253015667, 12.930890202252584, -9.615262184415819, -15.014931098169907, -13.382066923468464]\n",
      "Mean over last 125 episodes: -4.52565635199\n",
      "Episode lengths, last 10: [199, 162, 148, 154, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.151547480204\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 179464\n",
      "Episode rewards, last 10: [-26.632812866650141, 28.689369050059788, -1.096253951242625, 11.761623949850614, 6.7639248956778797, 57.946740652166369, 23.74661899359922, -14.514626707788125, -2.570673279545252, -15.085584563425325]\n",
      "Mean over last 125 episodes: -6.70287683307\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.129928058942\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 203539\n",
      "Episode rewards, last 10: [4.9741538847325302, -18.19204174438601, -0.9482122722567965, 32.561723916885235, 7.90087954176962, -24.129497515573647, 40.697798869379149, 4.3777667355648511, -200.43903789326728, 11.503362989593551]\n",
      "Mean over last 125 episodes: -4.85064274609\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 193, 199]\n",
      "AEerror, mean over last 10: 0.134288300604\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 227225\n",
      "Episode rewards, last 10: [32.847339287661832, 26.070510635252148, 56.585051975245698, 46.293978441310855, 48.308518059136162, -0.71816042561368665, 37.047084495746731, -3.7584760187072863, -87.318278402817484, 3.8257139781603797]\n",
      "Mean over last 125 episodes: -4.22910715434\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 74, 105]\n",
      "AEerror, mean over last 10: 0.173436387962\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 12194\n",
      "Episode rewards, last 10: [-115.77146767288596, -200.9481592215156, -135.58947441007976, -163.75747734916132, -181.51670863275638, -171.29841223046157, -154.77953991418545, -236.46782377082639, -194.58283178480326, -77.152260957573702]\n",
      "Mean over last 125 episodes: -179.882043868\n",
      "Episode lengths, last 10: [186, 119, 123, 117, 129, 82, 68, 74, 90, 75]\n",
      "AEerror, mean over last 10: 0.213590035477\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 28521\n",
      "Episode rewards, last 10: [-28.255402838795426, -168.00893061516041, -34.487688565043214, -124.16945063665014, -261.5329261521199, 45.785525680837601, -7.9040376415842104, -45.461517673017823, 5.4703068517908733, -225.05395528665102]\n",
      "Mean over last 125 episodes: -145.437932813\n",
      "Episode lengths, last 10: [199, 94, 199, 151, 106, 199, 199, 199, 199, 112]\n",
      "AEerror, mean over last 10: 0.121339208431\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 51226\n",
      "Episode rewards, last 10: [57.572612229709527, 45.639024095770409, 29.44067166904204, -114.13839182129385, 40.564928690088642, -82.58804272511091, 20.615241478276232, 49.915528183422367, 10.174813825328677, -0.2837518934484109]\n",
      "Mean over last 125 episodes: -28.6735626687\n",
      "Episode lengths, last 10: [199, 199, 199, 133, 199, 163, 144, 199, 199, 151]\n",
      "AEerror, mean over last 10: 0.111893895931\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 74420\n",
      "Episode rewards, last 10: [6.7357506969256802, 35.240807505167624, 42.303109649558678, 22.725191167365054, -78.274524791219818, 22.440464541788636, 15.955030197666762, -182.58049738956311, 43.480050327327461, -46.160970777145252]\n",
      "Mean over last 125 episodes: -27.7953859183\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 93, 199, 199, 72, 199, 122]\n",
      "AEerror, mean over last 10: 0.112732654662\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 97374\n",
      "Episode rewards, last 10: [-15.040052431314834, 6.2207503532790334, -1.1509033823818413, 42.060038882313592, -132.03245531518292, -13.85604862914807, -71.616292209642694, -27.563623379041747, 63.748092089128136, -24.359244922888106]\n",
      "Mean over last 125 episodes: -25.2182664658\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 148, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.137793842017\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 120065\n",
      "Episode rewards, last 10: [-82.283274251752943, 12.647787453971464, -256.71158122982217, -44.263332845946529, -30.129586302343704, -13.321181544173907, -19.377075840042373, -122.40706869038628, -21.986954409577287, 16.921396411462329]\n",
      "Mean over last 125 episodes: -53.9008834356\n",
      "Episode lengths, last 10: [159, 199, 178, 104, 199, 199, 199, 85, 199, 199]\n",
      "AEerror, mean over last 10: 0.139693549871\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 142655\n",
      "Episode rewards, last 10: [-142.68159437559308, -44.956955320095652, -177.12346784544658, -156.39290767607744, -168.32576162005378, 67.42546690197932, 6.1276092391973069, -135.68514623120561, -98.684369798262452, -25.879004599542718]\n",
      "Mean over last 125 episodes: -53.6906686775\n",
      "Episode lengths, last 10: [190, 164, 152, 182, 90, 199, 199, 175, 199, 199]\n",
      "AEerror, mean over last 10: 0.145568423148\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 166469\n",
      "Episode rewards, last 10: [52.683455463248734, 16.976102938999148, 48.270245428714325, -43.161562703444758, -28.663509660580665, -47.314098151246796, 31.787793425848374, -80.404464253936808, 35.193127083641535, -20.801622703989516]\n",
      "Mean over last 125 episodes: -23.7362037788\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 129, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.11286389\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 190492\n",
      "Episode rewards, last 10: [-21.705628224849185, 89.926779622629169, -360.72459212040565, -252.22592283961603, -126.67664179879787, -150.29177413221615, -57.572727551909061, 14.176039848318524, -20.733535443329206, -4.8298174446244833]\n",
      "Mean over last 125 episodes: -0.610227399828\n",
      "Episode lengths, last 10: [199, 199, 163, 163, 84, 130, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.170894233612\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 213661\n",
      "Episode rewards, last 10: [-33.606104222306058, 13.222782359967397, -72.441364336052771, -167.68863375443146, 1.2104957153280793, -144.33858515926477, -131.53507224711262, 34.724604774904662, 0.46891607828774617, 9.708189721573838]\n",
      "Mean over last 125 episodes: -10.4204622005\n",
      "Episode lengths, last 10: [189, 199, 79, 107, 166, 60, 106, 199, 199, 109]\n",
      "AEerror, mean over last 10: 0.126621703895\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8331\n",
      "Episode rewards, last 10: [-183.40409692669809, -157.51060520728379, -231.82322091307586, -231.86597109103738, -258.92768321946198, -177.48361076133426, -194.0273569253462, -144.56028806835766, -157.46191915422219, -178.43336297105125]\n",
      "Mean over last 125 episodes: -194.896884135\n",
      "Episode lengths, last 10: [70, 62, 78, 90, 98, 87, 73, 65, 51, 58]\n",
      "AEerror, mean over last 10: 0.208100936109\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 17068\n",
      "Episode rewards, last 10: [-149.16633209692185, -147.36165998327505, -195.30952354100475, -174.92596329634196, -210.7274375248206, -212.86128564395443, -137.31097290717341, -175.88182508929117, -165.74313104985217, -31.592811081573856]\n",
      "Mean over last 125 episodes: -177.169402017\n",
      "Episode lengths, last 10: [76, 57, 55, 75, 84, 64, 54, 77, 51, 68]\n",
      "AEerror, mean over last 10: 0.196118321041\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 26282\n",
      "Episode rewards, last 10: [-48.116712002660975, -154.03451156868331, -249.07351687323325, -144.15598875693681, -190.6998757866935, -151.93091137925069, -166.42209626864494, -167.77874821799298, -211.70127446019512, -148.04363215663457]\n",
      "Mean over last 125 episodes: -178.402196272\n",
      "Episode lengths, last 10: [65, 51, 91, 55, 74, 81, 66, 68, 75, 67]\n",
      "AEerror, mean over last 10: 0.167139783591\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 35174\n",
      "Episode rewards, last 10: [-196.57023192111319, -159.23837436061893, -234.79531147112903, -205.80880504060895, -175.18777927726143, -215.82828764377228, -185.80311078213578, -124.70807289686894, -212.24097459058413, -183.68043160048603]\n",
      "Mean over last 125 episodes: -180.603378549\n",
      "Episode lengths, last 10: [66, 91, 74, 92, 61, 79, 68, 54, 70, 56]\n",
      "AEerror, mean over last 10: 0.167457607369\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 44440\n",
      "Episode rewards, last 10: [-180.03843910210293, -177.26037587510476, -171.55501037580757, -160.58854830835423, -181.45416252428677, -200.43967970229409, -134.32234563574852, -222.55812581623877, -158.80013187186447, -208.710789985177]\n",
      "Mean over last 125 episodes: -178.376351677\n",
      "Episode lengths, last 10: [74, 81, 83, 74, 55, 82, 60, 79, 60, 89]\n",
      "AEerror, mean over last 10: 0.149517636189\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 53612\n",
      "Episode rewards, last 10: [-176.65391514739522, -75.719676755205853, -176.18760330122046, -161.78179789301839, -159.52624136334941, -159.24097729361588, -206.57719863660293, -190.03949220920487, -207.6604646366219, -170.65872475945071]\n",
      "Mean over last 125 episodes: -181.29413526\n",
      "Episode lengths, last 10: [67, 80, 86, 73, 66, 56, 73, 88, 79, 71]\n",
      "AEerror, mean over last 10: 0.14369513239\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 62625\n",
      "Episode rewards, last 10: [-208.97795430957876, -186.24491228320369, -121.83239879412517, -161.7855437374331, -186.02976028516738, -183.02318267396856, -181.76337325888639, -217.00692464348307, -173.35088885654073, -179.98083943184133]\n",
      "Mean over last 125 episodes: -174.501275202\n",
      "Episode lengths, last 10: [57, 76, 61, 73, 80, 65, 89, 64, 53, 60]\n",
      "AEerror, mean over last 10: 0.213098041712\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 71535\n",
      "Episode rewards, last 10: [-154.02573011716936, -203.03105168328733, -138.42847411194239, -143.72646101310929, -200.31554858350245, -217.26922742284142, -181.66925952362749, -177.80398798447533, -128.2029866701553, -180.91512753179364]\n",
      "Mean over last 125 episodes: -177.1338883\n",
      "Episode lengths, last 10: [64, 61, 52, 52, 76, 67, 76, 76, 53, 55]\n",
      "AEerror, mean over last 10: 0.207137758148\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 80466\n",
      "Episode rewards, last 10: [-211.63810985669355, -205.81315455878544, -204.40949217753996, -261.70833593016653, -164.81544119936078, -144.21145730009164, -174.00781796044501, -147.34546535700392, -217.83983403844837, -196.47922505522041]\n",
      "Mean over last 125 episodes: -183.572584495\n",
      "Episode lengths, last 10: [74, 89, 76, 90, 56, 75, 75, 58, 80, 76]\n",
      "AEerror, mean over last 10: 0.177543552106\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 89158\n",
      "Episode rewards, last 10: [-67.756144991680145, -189.04952779050348, -100.39874706831588, -193.11003839822052, -162.82478249368091, -143.62671934012008, -225.58668966374529, -177.99174732865868, -183.95085633826815, -181.3504264464695]\n",
      "Mean over last 125 episodes: -182.142855727\n",
      "Episode lengths, last 10: [83, 77, 70, 67, 58, 50, 59, 69, 54, 53]\n",
      "AEerror, mean over last 10: 0.182620578786\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8436\n",
      "Episode rewards, last 10: [-78.933202666567368, -240.87949639266279, -170.07410061132046, -140.91377048969466, -245.27432408993818, -183.72330720127965, -142.61580381806667, -168.28434129814423, -325.2089407893659, -166.9062933880312]\n",
      "Mean over last 125 episodes: -178.600963721\n",
      "Episode lengths, last 10: [60, 70, 51, 57, 74, 66, 79, 90, 77, 68]\n",
      "AEerror, mean over last 10: 0.221650865911\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 17707\n",
      "Episode rewards, last 10: [-218.68065407588887, -196.78603045854487, -168.10341132888709, -219.2956622869134, -186.83096838843437, -187.1209510408091, -166.2065470560822, -235.19583193568926, -166.55661314126718, -142.19204094172869]\n",
      "Mean over last 125 episodes: -184.597263881\n",
      "Episode lengths, last 10: [89, 51, 62, 91, 65, 60, 52, 91, 70, 52]\n",
      "AEerror, mean over last 10: 0.189515324227\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 26426\n",
      "Episode rewards, last 10: [-159.17967638662168, -179.94575998091034, -208.31751205703006, -184.48215572128569, -174.28480228981272, -161.59760955654053, -195.20370688252467, -201.02533154604322, -169.24103115179511, -197.41062369223954]\n",
      "Mean over last 125 episodes: -172.14298408\n",
      "Episode lengths, last 10: [56, 58, 60, 59, 58, 56, 83, 66, 59, 58]\n",
      "AEerror, mean over last 10: 0.197768009971\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 35305\n",
      "Episode rewards, last 10: [-208.94555711628718, -194.08931979229754, -177.85279350441419, -218.05317926762217, -158.04457935267925, -194.77383933815082, -217.72970466360712, -192.95850985153953, -261.7758257460755, -258.54024086588686]\n",
      "Mean over last 125 episodes: -185.007486765\n",
      "Episode lengths, last 10: [65, 65, 64, 56, 91, 58, 67, 61, 78, 75]\n",
      "AEerror, mean over last 10: 0.184482373\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 44439\n",
      "Episode rewards, last 10: [-160.03453603934938, -154.80533930696663, -210.77606043126571, -199.28725161111601, -189.72878247288838, -170.21434275683367, -161.09857655062785, -47.319780088069805, -214.17873819677186, -144.50455447759197]\n",
      "Mean over last 125 episodes: -184.392245254\n",
      "Episode lengths, last 10: [59, 62, 69, 91, 65, 58, 54, 61, 61, 51]\n",
      "AEerror, mean over last 10: 0.181630563376\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 53365\n",
      "Episode rewards, last 10: [-139.21706696294964, -372.65704855784031, -214.93832371309247, -208.747810035778, -176.61607226943232, -169.63173091081273, -89.083702589178785, -166.8913446694699, -181.07545513354637, -175.48854993433145]\n",
      "Mean over last 125 episodes: -181.07220724\n",
      "Episode lengths, last 10: [54, 104, 75, 75, 77, 53, 71, 72, 78, 90]\n",
      "AEerror, mean over last 10: 0.197635077974\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 62384\n",
      "Episode rewards, last 10: [-177.07295481610831, -202.06588049038731, -151.36001890244748, -182.84011112655364, -172.98444727506958, -195.11086592253466, -160.74841249743636, -157.50753625363942, -94.833311251204705, -145.44782382524775]\n",
      "Mean over last 125 episodes: -185.865458264\n",
      "Episode lengths, last 10: [57, 93, 68, 85, 74, 89, 72, 51, 87, 82]\n",
      "AEerror, mean over last 10: 0.160245087523\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 71322\n",
      "Episode rewards, last 10: [-175.12254583780529, -193.99973517581714, -182.74128941309391, -243.18470350826595, -191.70693175402226, -212.24178867109737, -172.88817931289168, -206.4569105207276, -233.15125340437655, -200.67989541994794]\n",
      "Mean over last 125 episodes: -179.895164442\n",
      "Episode lengths, last 10: [75, 68, 62, 70, 75, 54, 74, 91, 76, 72]\n",
      "AEerror, mean over last 10: 0.185581765976\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 80303\n",
      "Episode rewards, last 10: [-165.63936861087248, -179.50767237315563, -143.06296016985226, -100.66608652667313, -173.00892446233121, -161.80978960783096, -171.54337638327965, -162.0169409131106, -214.76520408993349, -202.12320771510809]\n",
      "Mean over last 125 episodes: -185.43699194\n",
      "Episode lengths, last 10: [90, 66, 58, 58, 55, 71, 57, 54, 66, 59]\n",
      "AEerror, mean over last 10: 0.193554198478\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 89023\n",
      "Episode rewards, last 10: [-184.28951669318914, -192.93313249399546, -199.72867922295251, -175.34296287524052, -187.91717853154876, -164.82473306204014, -82.011660855489254, -213.59832526699887, -165.72242976172421, -153.20464341521429]\n",
      "Mean over last 125 episodes: -186.953969772\n",
      "Episode lengths, last 10: [63, 77, 73, 74, 82, 59, 55, 66, 79, 88]\n",
      "AEerror, mean over last 10: 0.167405048904\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8630\n",
      "Episode rewards, last 10: [-28.969866710579993, -189.59900025387526, -162.33475163918507, -177.68518899664375, -160.284798269692, -210.80963008466128, -161.37289105345459, -175.0331230586761, -189.13649729999202, 10.505003913639303]\n",
      "Mean over last 125 episodes: -185.874280592\n",
      "Episode lengths, last 10: [97, 73, 72, 70, 80, 72, 63, 66, 73, 68]\n",
      "AEerror, mean over last 10: 0.23717204439\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 17166\n",
      "Episode rewards, last 10: [-145.61773862143272, -188.734234659534, -218.93124824817721, -221.44541770584902, -184.52248898124134, -186.54558476865944, -201.03287695595398, -182.47568204935442, -128.40709641687036, -225.77332105231363]\n",
      "Mean over last 125 episodes: -182.640184444\n",
      "Episode lengths, last 10: [60, 61, 73, 74, 76, 65, 59, 90, 53, 61]\n",
      "AEerror, mean over last 10: 0.252613121306\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 26374\n",
      "Episode rewards, last 10: [-204.65695779388034, -155.75551432658023, -49.284921881746072, -181.21454163748194, -226.24740823229618, -182.58006059604327, -248.25386236088443, -153.61181788011103, -185.01677111035812, -209.4300481013284]\n",
      "Mean over last 125 episodes: -184.368342287\n",
      "Episode lengths, last 10: [90, 79, 67, 62, 65, 83, 83, 86, 62, 77]\n",
      "AEerror, mean over last 10: 0.226207907921\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 35457\n",
      "Episode rewards, last 10: [-170.24715343161844, -131.37186171246151, -160.60075761385895, -205.09786649458954, -214.001542101533, -189.26871619768266, -289.39879184318454, -175.4396894130621, -48.948815215222595, -85.643715893657344]\n",
      "Mean over last 125 episodes: -177.61284348\n",
      "Episode lengths, last 10: [63, 52, 78, 73, 64, 50, 89, 61, 73, 62]\n",
      "AEerror, mean over last 10: 0.257650345606\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 44424\n",
      "Episode rewards, last 10: [-170.56352983869093, -174.97535871325113, -153.41940297477402, -189.17067392525513, -152.33384335201882, -159.57552518049698, -163.94763900184643, -222.72300742548785, -213.21498370898152, -221.5079025530294]\n",
      "Mean over last 125 episodes: -181.415455591\n",
      "Episode lengths, last 10: [72, 55, 65, 83, 51, 61, 57, 76, 72, 80]\n",
      "AEerror, mean over last 10: 0.236810838723\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 53498\n",
      "Episode rewards, last 10: [-138.3546024722657, -169.28161267697686, -182.91224662032755, -184.66418794142854, -141.30275354537625, -186.75280844593337, -158.16788844544106, -209.89311568569107, -209.45432029866004, -171.55674795198385]\n",
      "Mean over last 125 episodes: -185.371901921\n",
      "Episode lengths, last 10: [57, 82, 59, 67, 59, 64, 87, 66, 85, 92]\n",
      "AEerror, mean over last 10: 0.242436861645\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 62497\n",
      "Episode rewards, last 10: [-75.980712832563611, -174.93815170153201, -200.50813693289913, -163.96620623669637, -204.02984502040835, -183.42498400879037, -51.607696270729079, -184.79424167232497, -174.99351536121475, -159.1653075760928]\n",
      "Mean over last 125 episodes: -182.274446438\n",
      "Episode lengths, last 10: [73, 67, 60, 55, 83, 73, 99, 66, 62, 86]\n",
      "AEerror, mean over last 10: 0.23913991789\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 71691\n",
      "Episode rewards, last 10: [-167.37612633934643, -170.78700398626637, -155.77222126331992, -62.669705763639442, -231.06562968603609, -217.4105040155527, -185.85402204599046, -191.34061460290965, -211.72262071707553, -206.3100661367846]\n",
      "Mean over last 125 episodes: -181.807588907\n",
      "Episode lengths, last 10: [83, 83, 90, 52, 71, 60, 79, 51, 68, 78]\n",
      "AEerror, mean over last 10: 0.229799292294\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 80496\n",
      "Episode rewards, last 10: [-238.93392900468558, -192.86542368663916, -189.71050371475607, -226.4123159401619, -189.10386904555875, -179.50474580147568, -146.11753355787971, -170.91440493595795, -158.00334646071587, -191.11594492858862]\n",
      "Mean over last 125 episodes: -179.280111201\n",
      "Episode lengths, last 10: [86, 99, 60, 83, 74, 65, 64, 86, 63, 50]\n",
      "AEerror, mean over last 10: 0.250878400162\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 89203\n",
      "Episode rewards, last 10: [-167.95165287861616, -197.09619917596194, -181.64339286671191, -48.110162888622, -160.77728080190334, -155.82300570009127, -156.32030589444281, -194.30200740982374, -177.12955161587473, -195.43259914883768]\n",
      "Mean over last 125 episodes: -173.629636387\n",
      "Episode lengths, last 10: [61, 65, 58, 67, 52, 53, 54, 62, 79, 54]\n",
      "AEerror, mean over last 10: 0.256899219077\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 20370\n",
      "Episode rewards, last 10: [11.862782630281377, 44.586753295179975, 32.106273225322099, 10.207203948497918, -63.289524990222404, -3.7671115181640822, -175.55890584372224, -6.1438758706425673, 10.935368957863941, 57.415850943602521]\n",
      "Mean over last 125 episodes: -65.1928403613\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 193, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.103662203773\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 43787\n",
      "Episode rewards, last 10: [-36.328659637621101, -93.868076073383349, -203.84927307267031, 30.149507931455851, 9.7790360007116472, -65.429821905368371, -30.549333937728377, -2.3470846658043918, -6.7578429796600927, 30.666630636732741]\n",
      "Mean over last 125 episodes: -33.051939511\n",
      "Episode lengths, last 10: [199, 172, 67, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.173429630527\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 66389\n",
      "Episode rewards, last 10: [-26.884678768468657, -40.760845028706726, -166.50596716160393, -29.0032992283776, -196.83580433650224, -32.269294403589569, -15.857625174178132, -37.968345726885225, 22.938330309151105, 57.036075468054754]\n",
      "Mean over last 125 episodes: -52.619465424\n",
      "Episode lengths, last 10: [199, 199, 68, 199, 138, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.227099433459\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 88622\n",
      "Episode rewards, last 10: [-36.882301544235844, -21.875653024739723, -75.350216483739871, -136.5556812075329, -133.97640020109645, -199.88675955726933, 17.593579900444045, 14.404733666762406, 9.7883798944935698, -100.12323514842936]\n",
      "Mean over last 125 episodes: -30.5280650556\n",
      "Episode lengths, last 10: [199, 199, 184, 92, 148, 156, 199, 199, 199, 116]\n",
      "AEerror, mean over last 10: 0.54485732987\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 111778\n",
      "Episode rewards, last 10: [16.350062638768499, -23.079456084439915, -12.128353993783664, -173.42967719363514, -78.627278072350251, 33.092470422232033, -60.195664664138022, -35.00527451550105, -64.294523205783051, -5.1170274835166545]\n",
      "Mean over last 125 episodes: -32.2150466599\n",
      "Episode lengths, last 10: [199, 199, 199, 196, 101, 199, 199, 199, 72, 199]\n",
      "AEerror, mean over last 10: 0.405402943559\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 130042\n",
      "Episode rewards, last 10: [-165.93472037516904, -359.28356402495211, -222.02746686174464, -172.72279438021576, -163.34279945302663, -180.51766626895107, -197.22416817283158, -163.11976037402147, -162.36100318483639, -167.82491629623848]\n",
      "Mean over last 125 episodes: -121.317163708\n",
      "Episode lengths, last 10: [58, 94, 70, 56, 54, 66, 87, 67, 74, 87]\n",
      "AEerror, mean over last 10: 0.333599490524\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 139025\n",
      "Episode rewards, last 10: [-118.34280469665444, -223.29565538814865, -263.0082601658857, -166.20815636173427, -244.68280283581214, -239.48543632517811, -78.932780788061834, -188.42039459334359, -216.69060212377155, -258.55948356402769]\n",
      "Mean over last 125 episodes: -175.369151215\n",
      "Episode lengths, last 10: [81, 58, 83, 51, 70, 71, 63, 78, 63, 77]\n",
      "AEerror, mean over last 10: 0.197594123569\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 148227\n",
      "Episode rewards, last 10: [-220.17213736677562, -147.39525533296973, -82.979494981542629, -171.52569125515342, -187.20994448077269, -200.06147846445677, -180.24641440285995, -209.20427906019893, -255.13469129978722, -322.85748434572105]\n",
      "Mean over last 125 episodes: -189.945434968\n",
      "Episode lengths, last 10: [63, 58, 55, 72, 57, 79, 83, 77, 68, 87]\n",
      "AEerror, mean over last 10: 0.186966420673\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 157294\n",
      "Episode rewards, last 10: [-193.7989855046512, -228.23786307866192, -165.234325919513, -229.27883547495651, -222.86313667232548, -164.00382898843657, -157.85154746289157, -163.43700972712713, -199.35294892895965, -206.99989067562288]\n",
      "Mean over last 125 episodes: -187.137089378\n",
      "Episode lengths, last 10: [70, 80, 78, 86, 80, 52, 71, 54, 69, 77]\n",
      "AEerror, mean over last 10: 0.172741082465\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 166307\n",
      "Episode rewards, last 10: [-146.41838927052544, -185.18782634259799, -191.29194484130676, -187.72649301222651, -252.51722937555681, -186.80982268040623, -142.37179205661567, -178.63314591804553, -218.17361127428322, -183.79572156927054]\n",
      "Mean over last 125 episodes: -173.27616704\n",
      "Episode lengths, last 10: [62, 59, 81, 71, 85, 77, 54, 85, 64, 72]\n",
      "AEerror, mean over last 10: 0.156518936393\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = produce_experiment(DDQL, ddql_egreedy_params, ddql_train_params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(ddql_res, window = 100, std_coef = 0.2, results_over = 1000):\n",
    "    res_lists = [k.rList for k in ddql_res]\n",
    "    res_lists = np.array(res_lists)\n",
    "    pd.DataFrame(data = res_lists)\n",
    "    mean = res_lists.mean(axis = 0)\n",
    "    std = res_lists.std(axis = 0)\n",
    "    rol_mean = np.nan_to_num(pd.Series(mean).rolling(window = window).mean())[window:]\n",
    "    rol_std = np.nan_to_num(pd.Series(std).rolling(window = window).mean())[window:]\n",
    "    plt.figure()\n",
    "    index = np.arange(window, len(rol_mean) + window)\n",
    "    plt.plot(index, rol_mean)\n",
    "    plt.fill_between(index, rol_mean-std_coef*rol_std, rol_mean+std_coef*rol_std, color='b', alpha=0.1)\n",
    "    return max(rol_mean[window:results_over]), rol_mean[window:results_over].mean(), pd.DataFrame(data = res_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqDBQFEVRUGGgKIqiAP8Pq4ticvxbt4AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1181e8590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inc_max, inc_auc, inc_df = plot_results(res, 100, 0.2)\n",
    "plt.savefig(\"inc_ll.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-104.725879763 -113.667707924\n"
     ]
    }
   ],
   "source": [
    "print inc_max, inc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inc_df.to_csv(\"inc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "0996522c74894bdea11e00adad7d02be": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "0b64aae0cd6e44f0ace87d0d938925c5": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "3fa208d8cedb47e3a8eff4867387555f": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "4339ef29126a4c85b26ca2a606c9c484": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "4e0548e5b002486fb000875cb33e695d": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "5771a63e0c004beaa94a3f758c77a424": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "61b9ebe33cc5469fb4b9219c3e53396c": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "6c4a2513d15942c18e1b60d026feb567": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "6d618154d7584a9ea9121d4e1c7dee0c": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "7d9aa9018e7e4006b65fc163d859807f": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "8a421f16a9884bcab14f5737da83c2b0": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "9513bb14b35e4d3eabc1fb420e6a5b74": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "a7c281f1fb6a46a19fe449c7ca18c3cb": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "f43514ac4ee349348bbeac115470f63d": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    },
    "f5450d3ad7894becb713bfbefb3e265b": {
     "views": [
      {
       "cell_index": 22
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
