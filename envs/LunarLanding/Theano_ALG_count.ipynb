{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ATARI_wrapper():\n",
    "    def __init__(self, gamename = \"Enduro-v0\"):\n",
    "        self.state_size = (105, 80)\n",
    "        self.game_title = gamename\n",
    "        self.actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        grayimage = np.mean(state, axis = 2)\n",
    "        downscale = self.downscale2x(grayimage)\n",
    "        norm = (downscale - 128.0) / 128.0\n",
    "        return norm\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3\n",
    "    \n",
    "    def downscale2x(self, image):\n",
    "        image00 = image[0::2, 0::2]\n",
    "        image01 = image[0::2, 1::2]\n",
    "        image10 = image[1::2, 0::2]\n",
    "        image11 = image[1::2, 1::2]\n",
    "        return (image00 + image01 + image10 + image11) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, \n",
    "                 encodeunits = 10, encodenoise = 0.1, aelosscoef = 0.1, regaelosscoef = 2.0):\n",
    "        self.input_var = T.tensor4('statebatch')\n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.newstates = T.tensor4(\"newstatebatch\")\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.aelosscoef = aelosscoef\n",
    "        self.regaelosscoef = regaelosscoef\n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape, encodeunits, encodenoise)\n",
    "        self.build_AVQ(grad_clipping, lr)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape, encodeunits, encodenoise):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.encode = lasagne.layers.DenseLayer(self.outlayer, encodeunits, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "        self.encodenoise = lasagne.layers.GaussianNoiseLayer(self.encode, sigma = encodenoise)\n",
    "        self.le3 = lasagne.layers.DenseLayer(self.encodenoise, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le2 = lasagne.layers.DenseLayer(self.le3, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le1 = lasagne.layers.DenseLayer(self.le2, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le0 = lasagne.layers.DenseLayer(self.le1, channels_number * image_shape[0] * image_shape[1])\n",
    "        self.l_aeout = lasagne.layers.ReshapeLayer(self.le0, shape=(-1, channels_number, image_shape[0], image_shape[1]))\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        \n",
    "        self.advantage, self.value, self.ae_out, self.enc = lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_aeout, self.encode])\n",
    "        \n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Qout = self.advantage + self.value - self.average_advantage\n",
    "        self.Qout = self.advantage\n",
    "        self.predict = T.argmax(self.Qout, axis = 1)\n",
    "        \n",
    "        self.Q = T.sum(self.Qout * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.regae_error = 0.25 - T.mean(T.sqr(self.enc - 0.5))\n",
    "        self.ae_error = T.mean(T.sqr(self.ae_out - self.input_var))\n",
    "        self.td_error = T.mean(T.sqr(self.targetQ - self.Q))\n",
    "        \n",
    "        self.loss = self.ae_error * self.aelosscoef + self.regaelosscoef * self.regae_error + self.td_error\n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "\n",
    "        enc_layers = [self.l2, self.l3, self.outlayer, self.encode, self.le3, self.le2, self.le1, self.le0]\n",
    "        enc_params = [l.W for l in enc_layers] + [l.b for l in enc_layers]\n",
    "        self.enc_grads = T.grad(self.ae_error, enc_params)\n",
    "        self.enc_scaled_grads = lasagne.updates.total_norm_constraint(self.enc_grads, grad_clipping)\n",
    "        self.enc_updates = lasagne.updates.adam(self.enc_scaled_grads, enc_params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Qout)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.train_fn = theano.function([self.input_var, self.targetQ, self.actions], [self.ae_error, self.td_error], updates = self.updates)\n",
    "        self.train_encoder = theano.function([self.input_var], self.ae_error, updates = self.enc_updates)\n",
    "        self.get_encode = theano.function([self.input_var], [self.enc, self.regae_error])\n",
    "        \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values(self.l_advantage, values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.buffer))\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.encode_unitnum = lparams[\"encode_unitnum\"]\n",
    "        self.encode_noise = lparams[\"encode_noise\"]\n",
    "        self.encode_reward_multiplier = lparams[\"encode_reward_multiplier\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "\n",
    "        \n",
    "        self.env = env\n",
    "        AVQ_params = [self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, \n",
    "                      self.lr, self.encode_unitnum, self.encode_noise]\n",
    "        self.mainQN = AVQ_nn(*AVQ_params)\n",
    "        self.targetQN = AVQ_nn(*AVQ_params)\n",
    "\n",
    "        self.lList = []\n",
    "        self.rList = []\n",
    "        self.aeList = []\n",
    "        self.encode_counts = defaultdict(int)\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            agent = egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            agent = boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "            \n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "        \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "            episode_aeerrors = np.array([])\n",
    "            \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                experience = np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5])\n",
    "                self.experience_storage.add(experience)\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        \n",
    "                        encodes, reg_errors = self.mainQN.get_encode(old_state_batch)\n",
    "                        encodes = [\"\".join([str(k) for k in encode]) for encode in (encodes > 0.5).astype(int)]\n",
    "                        for encode in encodes:\n",
    "                            self.encode_counts[encode] += 1\n",
    "                        encode_rewards = self.encode_reward_multiplier / np.sqrt(np.array([self.encode_counts[encode] for encode in encodes]))\n",
    "                        rewards_vector = rewards_vector + encode_rewards \n",
    "                        \n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)\n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        \n",
    "                        doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        aeerror, tderror = self.mainQN.train_fn(old_state_batch, targetQ, action_vector)\n",
    "                        episode_aeerrors = np.append(episode_aeerrors, aeerror)\n",
    "                else:\n",
    "                    train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                    old_state_batch = np.stack(train_batch[:,0])\n",
    "                    new_state_batch = np.stack(train_batch[:,3])\n",
    "                    action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                    aeerror1 = self.mainQN.train_encoder(old_state_batch)\n",
    "                    aeerror2 = self.mainQN.train_encoder(new_state_batch)\n",
    "                    episode_aeerrors = np.append(episode_aeerrors, (aeerror1 + aeerror2)/2)\n",
    "                    \n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            total_aeerror = np.mean(episode_aeerrors)\n",
    "            self.lList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            self.aeList.append(total_aeerror)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.lList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"AEerror, mean over last 10:\", str(np.mean(self.aeList[-10:]))]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.lList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-15 21:55:55,882] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "ll_env = LunarLanding_wrapper()\n",
    "#cp_env = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 3,\n",
    "           \"encode_unitnum\": 10,\n",
    "           \"encode_noise\": 0.05,\n",
    "           \"encode_reward_multiplier\": 10.0,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 125,\n",
    "           \"pretrain_steps\" : 500,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.06,\n",
    "                                \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_experiment(ddql, ddql_init_params, ddql_train_params, experiment_num = 5):\n",
    "    ddql_list = [ddql(**ddql_init_params) for k in range(experiment_num)]\n",
    "    \n",
    "    for k in range(experiment_num):\n",
    "        ddql_list[k].train(**ddql_train_params)\n",
    "        \n",
    "    return ddql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql_egreedy_params = {\"lparams\":lparams, \"env\":ll_env, \"agent\":egreedyagentinfo}\n",
    "\n",
    "ddql_train_params = {\"num_episodes\":1250, \"frame_limit\":200, \"render\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 124 ================================================\n",
      "Total steps: 20264\n",
      "Episode rewards, last 10: [-163.32897684979974, -129.58259652804691, -12.277299618360315, -31.384408129543445, 4.8277598422368309, 14.072905750971085, -350.83438393216488, -199.10056019103695, -143.972633882183, -473.04652775337706]\n",
      "Mean over last 125 episodes: -136.204735144\n",
      "Episode lengths, last 10: [196, 58, 199, 199, 199, 199, 194, 133, 199, 105]\n",
      "AEerror, mean over last 10: 0.11497030175\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 42535\n",
      "Episode rewards, last 10: [-74.845399076091013, 17.013836540556767, -1.9985797183456171, -190.10644050763509, -184.78638697967051, -225.03380654657059, -77.6731853502001, 1.2383766735283608, -79.931417600830315, -9.5420754197271691]\n",
      "Mean over last 125 episodes: -98.306575905\n",
      "Episode lengths, last 10: [199, 199, 199, 154, 70, 67, 160, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.121905139713\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 53577\n",
      "Episode rewards, last 10: [-169.07939981581922, -192.3725850631721, -135.16533161054448, -188.68928978563309, -152.48138417046593, -167.08883089025716, -175.93835675601446, -173.39612891606586, -184.71024242303145, -146.93897173271225]\n",
      "Mean over last 125 episodes: -187.018294798\n",
      "Episode lengths, last 10: [52, 63, 90, 82, 60, 59, 81, 66, 61, 57]\n",
      "AEerror, mean over last 10: 0.418565517927\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 62499\n",
      "Episode rewards, last 10: [-192.71345614638176, -183.03890034826787, -183.32731075099304, -52.652032132210721, -154.28012327799411, -188.00259742500336, -189.53183602486644, -173.72140240958609, -319.16399135140506, -83.989193825344529]\n",
      "Mean over last 125 episodes: -176.960383736\n",
      "Episode lengths, last 10: [86, 66, 69, 71, 55, 50, 67, 86, 75, 76]\n",
      "AEerror, mean over last 10: 0.477833105099\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 71146\n",
      "Episode rewards, last 10: [-373.64072342074462, -223.37272681372156, -231.09312150237142, -155.02130860759684, -166.28837950061288, -224.62932598664599, -217.95951162152525, -208.97817998451146, -207.8155997958238, -194.90375989662263]\n",
      "Mean over last 125 episodes: -175.381059954\n",
      "Episode lengths, last 10: [82, 81, 79, 53, 57, 77, 89, 87, 67, 76]\n",
      "AEerror, mean over last 10: 0.463884305587\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 80083\n",
      "Episode rewards, last 10: [-181.47774478232247, -170.49617403152564, -148.29504594624422, -185.18003525253206, -334.8914882536551, -200.56991823159893, -239.07160039626052, -156.1555452159995, -151.4786746606589, -222.73545773674454]\n",
      "Mean over last 125 episodes: -180.497824403\n",
      "Episode lengths, last 10: [66, 53, 60, 84, 101, 74, 82, 55, 87, 92]\n",
      "AEerror, mean over last 10: 0.428616185538\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 88872\n",
      "Episode rewards, last 10: [-143.65959790018439, -212.94461164268483, -141.41189816546762, -202.35961346131532, -241.29718315878748, -197.25785666626146, -178.26975029596647, -176.16198260964239, -180.62040670546253, -239.93868893577158]\n",
      "Mean over last 125 episodes: -180.496872819\n",
      "Episode lengths, last 10: [57, 82, 53, 80, 78, 66, 59, 70, 84, 80]\n",
      "AEerror, mean over last 10: 0.42248789987\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 97659\n",
      "Episode rewards, last 10: [-161.85029226274901, -292.23692446500479, -214.62900348986432, -203.18859934020747, -205.85068920183983, -222.71316516287072, -160.25786117645464, -156.29540395699587, -165.3850253814548, -171.23165281686659]\n",
      "Mean over last 125 episodes: -175.527131191\n",
      "Episode lengths, last 10: [53, 95, 75, 77, 72, 67, 85, 87, 53, 53]\n",
      "AEerror, mean over last 10: 0.446907098949\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 106536\n",
      "Episode rewards, last 10: [-190.4830731802071, -78.634111773677176, -155.16695415784093, -163.65166339615604, -178.50739764405492, -161.59290496418339, -127.27681999585408, -172.07521370999649, -148.32252550314166, -155.32450280713547]\n",
      "Mean over last 125 episodes: -185.381451401\n",
      "Episode lengths, last 10: [61, 74, 50, 69, 76, 75, 52, 90, 55, 85]\n",
      "AEerror, mean over last 10: 0.453822520952\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 115530\n",
      "Episode rewards, last 10: [-158.43545367002301, -328.97126506339168, -140.1098436339887, -184.80151797410349, -197.85124805210717, -189.262409566749, -279.80451391656561, -206.65020025402947, -209.12203983997807, -176.18769107357326]\n",
      "Mean over last 125 episodes: -180.743183177\n",
      "Episode lengths, last 10: [84, 96, 54, 65, 73, 62, 84, 78, 85, 55]\n",
      "AEerror, mean over last 10: 0.451411180914\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 20296\n",
      "Episode rewards, last 10: [26.059735694332471, 56.044130536076814, 3.0949690953678193, -20.089587251222476, -126.63196965540982, -228.58356556511217, -114.33500789439177, -19.024957777320225, -7.06754653335528, 0.22170698553529178]\n",
      "Mean over last 125 episodes: -80.3207504871\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 134, 184, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0781650508034\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 42855\n",
      "Episode rewards, last 10: [-60.4264014914353, -44.308911333604158, 16.307211613887198, -516.90390082997374, -63.215059478118931, -205.46450451064933, 13.941982791442285, -1.0295753201002746, -15.15671711611251, 15.602654689853606]\n",
      "Mean over last 125 episodes: -81.0881341926\n",
      "Episode lengths, last 10: [199, 199, 199, 81, 199, 87, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.120891493028\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 57042\n",
      "Episode rewards, last 10: [-136.3522082581303, -163.48010014476571, -162.99368268847672, -176.96493220353079, -198.57618614870816, -63.519097100310134, -264.17975527489273, -236.7567312207006, -203.90902013647164, -176.55376527226667]\n",
      "Mean over last 125 episodes: -163.839747585\n",
      "Episode lengths, last 10: [56, 68, 61, 68, 66, 91, 75, 89, 102, 61]\n",
      "AEerror, mean over last 10: 0.22284851546\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 73239\n",
      "Episode rewards, last 10: [-459.15396803050021, 5.3260777219225517, -236.88416837089406, -360.59079937331592, -227.54670868014006, -287.16297418995305, -46.95114621937406, -210.67149547979457, -68.296631776189031, -139.73020592317658]\n",
      "Mean over last 125 episodes: -175.807958527\n",
      "Episode lengths, last 10: [106, 199, 173, 199, 119, 145, 130, 86, 199, 170]\n",
      "AEerror, mean over last 10: 0.196754350858\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 95391\n",
      "Episode rewards, last 10: [-44.530773866708401, 33.920282602464795, -196.18717800623679, -339.58870364839527, -11.027628312495304, -192.80446648446613, -0.81100198355054953, 23.709734902806602, -83.991216333181725, 1.422457914484113]\n",
      "Mean over last 125 episodes: -126.770112857\n",
      "Episode lengths, last 10: [199, 199, 180, 97, 199, 123, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.123211013689\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 117933\n",
      "Episode rewards, last 10: [42.414476816371476, 66.940125805976322, 27.68479079385407, 23.175169434245962, 23.932520591574722, -41.020614702925315, -275.78135356866892, 54.733505734551812, 69.000925480044458, -64.696215577656929]\n",
      "Mean over last 125 episodes: -90.9645124408\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 190, 199, 199, 134]\n",
      "AEerror, mean over last 10: 0.138109631775\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 140197\n",
      "Episode rewards, last 10: [13.378680622978024, -90.490682014149556, -20.585247679164013, -6.6084529778832763, -30.941438012435352, -307.84399563441229, -44.770767361811629, -498.80291335669716, -25.03819098039309, -12.327238284655216]\n",
      "Mean over last 125 episodes: -59.3065942408\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 149, 199, 100, 199, 199]\n",
      "AEerror, mean over last 10: 0.142905700161\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 162921\n",
      "Episode rewards, last 10: [-15.470856170087977, 11.303499247418941, 19.737193116318146, -240.06594900650416, 17.570386415314587, -47.709573123872431, 15.755763804053323, 22.819751017504814, -7.9889254874484728, -8.0871329948280302]\n",
      "Mean over last 125 episodes: -51.9092242655\n",
      "Episode lengths, last 10: [199, 199, 199, 152, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0745440616981\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 186555\n",
      "Episode rewards, last 10: [-24.154378762657434, -15.223624887812907, -20.0385137106909, 34.57785402837937, 4.5957994416716348, 21.234612742353967, 59.392674429680596, 13.215349137688783, -104.92694907984514, 29.865977345673688]\n",
      "Mean over last 125 episodes: -16.9174021986\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 101, 199]\n",
      "AEerror, mean over last 10: 0.0641978466777\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 210009\n",
      "Episode rewards, last 10: [32.825595614037731, 48.920910904470986, 52.15306999964654, -14.543466730950509, 66.558633805282568, -120.42496496458658, 20.922031975136356, 40.998125681160218, 54.472931058162509, -53.483195917207226]\n",
      "Mean over last 125 episodes: -26.4357760805\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 89, 199, 199, 199, 100]\n",
      "AEerror, mean over last 10: 0.0708002923841\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 19986\n",
      "Episode rewards, last 10: [-86.054027799059156, 3.0943487531978855, 48.175965774299485, 19.374017985371061, -3.0519131825046344, -115.48784475523048, -134.46267480617092, 30.81051541131572, 35.279389136238962, 23.842570231282856]\n",
      "Mean over last 125 episodes: -102.62533651\n",
      "Episode lengths, last 10: [179, 199, 199, 199, 199, 89, 148, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0623648701519\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 40653\n",
      "Episode rewards, last 10: [-112.78932404668106, -213.30769456524379, -170.97827288304003, -10.846173185906721, -5.3085706373342845, 53.478100010789596, 37.497299772802556, -356.90776776521915, 6.1322083727383614, 5.1226471625368077]\n",
      "Mean over last 125 episodes: -107.652854239\n",
      "Episode lengths, last 10: [144, 102, 75, 79, 199, 199, 199, 86, 199, 199]\n",
      "AEerror, mean over last 10: 0.104913427397\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 63095\n",
      "Episode rewards, last 10: [-21.974692456228247, -30.155606734159846, -25.953901479518766, 0.52668251507702024, -49.915147336375171, -97.133649557103411, -31.808959784853588, 23.242523878783455, -81.152899329909189, -111.5842035447875]\n",
      "Mean over last 125 episodes: -60.8597354028\n",
      "Episode lengths, last 10: [199, 199, 175, 199, 192, 199, 199, 199, 199, 114]\n",
      "AEerror, mean over last 10: 0.0758827988298\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 84385\n",
      "Episode rewards, last 10: [-164.9060796961503, -214.72861462895398, -367.03559095172341, -5.4681470802824332, -1.7782878855834705, -148.72209167202874, -58.491972376917843, 9.2422348446816738, -266.96213001770332, -386.99629294998977]\n",
      "Mean over last 125 episodes: -102.008085469\n",
      "Episode lengths, last 10: [194, 156, 102, 199, 199, 199, 199, 199, 159, 156]\n",
      "AEerror, mean over last 10: 0.108623442613\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 105189\n",
      "Episode rewards, last 10: [-13.105431290013001, -89.472829608661442, -98.588442221763898, -65.759895896480941, 10.949550491692538, -224.38766102052116, 8.9151098066619721, -123.97934390736728, -24.340544300379875, -29.093770250378455]\n",
      "Mean over last 125 episodes: -148.97150634\n",
      "Episode lengths, last 10: [199, 199, 149, 199, 199, 195, 199, 199, 144, 199]\n",
      "AEerror, mean over last 10: 0.0712464335814\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 128031\n",
      "Episode rewards, last 10: [-11.406764959024812, -188.21180192624922, 5.2772606994960753, -13.367074292936142, -14.363842124030539, -22.446011468004848, -51.55876696272162, -14.166789479364517, -179.0562055336963, -43.110498680961697]\n",
      "Mean over last 125 episodes: -78.6903780584\n",
      "Episode lengths, last 10: [199, 116, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0677101782107\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 148823\n",
      "Episode rewards, last 10: [-118.14081991608985, -30.992312448375472, 16.158875533015092, -27.475182440587545, -23.181011594674455, -289.25861262588717, -238.03696848630511, -305.51324677804229, -251.54471646226057, 12.726038851825102]\n",
      "Mean over last 125 episodes: -119.356558561\n",
      "Episode lengths, last 10: [164, 199, 199, 199, 199, 195, 173, 96, 174, 199]\n",
      "AEerror, mean over last 10: 0.0956003321246\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 170962\n",
      "Episode rewards, last 10: [60.519891571427173, -2.0291403640463272, -180.16350849260738, -116.39368150827133, 14.735677943390762, 56.570947464259888, -96.052792224368105, -247.91326037642, -191.37311343148963, -18.576296971097275]\n",
      "Mean over last 125 episodes: -61.1000921978\n",
      "Episode lengths, last 10: [199, 199, 73, 184, 199, 199, 149, 84, 120, 199]\n",
      "AEerror, mean over last 10: 0.113736501543\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 184800\n",
      "Episode rewards, last 10: [-211.30092573399529, -185.64445422907085, -199.00723956238951, -133.63911685098077, -166.78368158957591, -222.49465866640571, -78.114253473426828, -204.10531748631965, -150.03868089324044, -173.98437610312379]\n",
      "Mean over last 125 episodes: -162.551715096\n",
      "Episode lengths, last 10: [87, 62, 83, 93, 69, 83, 82, 95, 72, 87]\n",
      "AEerror, mean over last 10: 0.230265868004\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 193619\n",
      "Episode rewards, last 10: [-168.90523318561836, -193.98576329886725, -201.61273814607028, -157.88489494651091, -185.92600732225443, -164.2523165641791, -252.49419228040523, -181.90982604689063, -186.3164654549677, -165.51158561409426]\n",
      "Mean over last 125 episodes: -174.683071238\n",
      "Episode lengths, last 10: [52, 73, 81, 77, 53, 88, 81, 77, 73, 71]\n",
      "AEerror, mean over last 10: 0.188754084586\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 18232\n",
      "Episode rewards, last 10: [-167.38611947448553, 1.8176908187614789, -41.598034455019345, 4.8797651442373073, -8.1034486081872252, -158.44273292659494, -44.607235572935622, -21.742891265312821, -41.65225495670056, -333.33729924871506]\n",
      "Mean over last 125 episodes: -129.507126327\n",
      "Episode lengths, last 10: [139, 199, 199, 199, 199, 104, 199, 199, 186, 181]\n",
      "AEerror, mean over last 10: 0.0754394906865\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 39594\n",
      "Episode rewards, last 10: [-335.98519249937721, -198.06624499977269, 24.190226842251143, -9.6581665849014016, -4.2824301732254426, -302.02424435195462, -36.576252998170638, -13.671289997246706, -213.88429052454586, 38.224322488661954]\n",
      "Mean over last 125 episodes: -80.8386498383\n",
      "Episode lengths, last 10: [107, 165, 199, 199, 199, 183, 199, 199, 82, 199]\n",
      "AEerror, mean over last 10: 0.109190618508\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 54143\n",
      "Episode rewards, last 10: [-190.38079752281487, -139.68345330967512, -186.87137233396348, -159.37114886361147, -208.00703477675279, -172.20512801043918, -176.20322829697363, -191.78054093155231, -185.27779432878862, -209.88869866513471]\n",
      "Mean over last 125 episodes: -153.099507361\n",
      "Episode lengths, last 10: [82, 87, 66, 86, 88, 51, 56, 74, 66, 83]\n",
      "AEerror, mean over last 10: 0.198851224863\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 63250\n",
      "Episode rewards, last 10: [-180.16855429223381, -166.68097843420702, -222.62149564616558, -184.3223906460168, -38.205080503944686, -181.14489892988195, -166.66187761858762, -183.63475944748535, -249.72654942864659, -163.29224307641289]\n",
      "Mean over last 125 episodes: -186.373675254\n",
      "Episode lengths, last 10: [77, 76, 70, 79, 89, 84, 92, 53, 92, 71]\n",
      "AEerror, mean over last 10: 0.205580405553\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 72121\n",
      "Episode rewards, last 10: [-200.03745989389139, -136.16452945679973, -167.96692552182097, -171.97548089420835, -168.86179575882107, -184.44690401587718, -193.13177132683222, -137.66415935698993, -187.31876401277057, -206.90107994173428]\n",
      "Mean over last 125 episodes: -188.237888336\n",
      "Episode lengths, last 10: [55, 52, 70, 68, 49, 87, 88, 89, 55, 66]\n",
      "AEerror, mean over last 10: 0.216142483305\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 81189\n",
      "Episode rewards, last 10: [-204.58155729538748, -173.11867411167924, -261.30037651373021, -25.526138039089602, -181.9001056046254, -489.53286134038387, -235.48285208166004, -154.22036840662594, -257.44955714802347, -147.65857438200391]\n",
      "Mean over last 125 episodes: -183.376556105\n",
      "Episode lengths, last 10: [72, 79, 84, 78, 55, 126, 69, 60, 83, 88]\n",
      "AEerror, mean over last 10: 0.315145274876\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 90061\n",
      "Episode rewards, last 10: [-188.24530057145256, -86.507251851949803, -180.82606760551852, -154.42915445473304, -219.24209793746289, -189.52250422928148, -317.33959613847833, -158.37147759478233, -188.29831730144221, -197.3634777332544]\n",
      "Mean over last 125 episodes: -182.894710787\n",
      "Episode lengths, last 10: [67, 56, 95, 88, 80, 72, 100, 63, 55, 92]\n",
      "AEerror, mean over last 10: 0.226976115875\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 99174\n",
      "Episode rewards, last 10: [-160.32774675427885, -216.84660485363861, -173.08323345571466, -182.11214976253376, -163.70737164822182, -162.59090826013215, -195.31618001689949, -179.11796830413735, -214.6387910932232, -183.44437416101988]\n",
      "Mean over last 125 episodes: -179.524179551\n",
      "Episode lengths, last 10: [54, 86, 53, 53, 58, 62, 75, 59, 73, 89]\n",
      "AEerror, mean over last 10: 0.221435458817\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 108208\n",
      "Episode rewards, last 10: [-183.1772417056676, -218.16948462770495, -162.98071569940498, -201.7483216367315, -164.39149499945313, -144.20085424004463, -244.61920434402884, -229.64625052020011, -196.28993822836799, -218.3518954470357]\n",
      "Mean over last 125 episodes: -180.645676531\n",
      "Episode lengths, last 10: [71, 81, 53, 62, 82, 90, 90, 77, 66, 76]\n",
      "AEerror, mean over last 10: 0.219356938937\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 117134\n",
      "Episode rewards, last 10: [-188.32228046681698, -190.62039058492462, -153.31818999652796, -185.31629631057598, -183.49357321542021, -171.01604279694541, -163.13275155826241, -151.09220981275209, -69.06164573391554, -202.02312473413167]\n",
      "Mean over last 125 episodes: -174.23476961\n",
      "Episode lengths, last 10: [76, 90, 63, 67, 65, 62, 84, 92, 84, 71]\n",
      "AEerror, mean over last 10: 0.186777672293\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 19221\n",
      "Episode rewards, last 10: [-3.2099391390434491, -26.491949982183137, -37.641789053849003, -288.52867059307971, 23.7775317384923, -34.158535812602125, -250.99942665298681, 1.1305540604138002, -11.458653321476181, -161.31385567945833]\n",
      "Mean over last 125 episodes: -153.23801272\n",
      "Episode lengths, last 10: [199, 199, 199, 195, 199, 199, 99, 199, 199, 122]\n",
      "AEerror, mean over last 10: 0.0655773180437\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 36376\n",
      "Episode rewards, last 10: [-247.70421083607044, -193.50468170577818, -269.55550077150718, -229.64884770091487, -207.58391482564792, -173.03714226132504, -169.70653954137603, -165.20137537472363, -171.3581091351144, -188.15177669699608]\n",
      "Mean over last 125 episodes: -153.933559301\n",
      "Episode lengths, last 10: [69, 60, 92, 83, 64, 61, 81, 55, 50, 52]\n",
      "AEerror, mean over last 10: 0.317485159114\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 45041\n",
      "Episode rewards, last 10: [-154.95174590332277, -131.61259649048861, -199.26089158837351, -112.07177074537867, -148.01810461711207, -59.692473012817445, -169.01417825312595, -205.73625224573686, -161.61007653638313, -335.33562404288773]\n",
      "Mean over last 125 episodes: -176.643006294\n",
      "Episode lengths, last 10: [60, 51, 59, 71, 55, 89, 76, 66, 64, 111]\n",
      "AEerror, mean over last 10: 0.296033886988\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 53941\n",
      "Episode rewards, last 10: [-153.33163464452005, -156.73173457839903, -187.30270989833193, -199.08163099684981, -153.16645611341849, -193.58053750650822, -156.33415579991319, -205.90946777126553, -189.1057024768337, -190.68472229303106]\n",
      "Mean over last 125 episodes: -177.806291019\n",
      "Episode lengths, last 10: [58, 79, 55, 65, 61, 71, 53, 86, 62, 70]\n",
      "AEerror, mean over last 10: 0.331529097152\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 62768\n",
      "Episode rewards, last 10: [-197.53109403316031, -206.42434337446093, -208.8241225783899, -197.05532796547578, -176.2306283416595, -184.90878032478261, -216.54918285281437, -140.44946119003868, -170.21379283268058, -147.12829545325587]\n",
      "Mean over last 125 episodes: -180.346282781\n",
      "Episode lengths, last 10: [74, 64, 63, 59, 78, 59, 60, 59, 50, 58]\n",
      "AEerror, mean over last 10: 0.322282029053\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 71964\n",
      "Episode rewards, last 10: [-34.684953779926772, -168.4944715335227, -191.49821578969474, -209.20250293865615, -202.87851887500329, -101.94109195628825, -170.42215310217307, -140.36721573852671, -159.82610766404264, -157.16521962334156]\n",
      "Mean over last 125 episodes: -178.938054328\n",
      "Episode lengths, last 10: [99, 62, 79, 90, 60, 98, 59, 48, 62, 78]\n",
      "AEerror, mean over last 10: 0.326631823765\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 81006\n",
      "Episode rewards, last 10: [-179.98612439868805, -169.6608147084321, -193.36717423445566, -193.33900831730045, -174.19488246275148, -176.33269413127931, -190.30557933094093, -193.68875165950251, -210.99061265880101, -57.297018222963445]\n",
      "Mean over last 125 episodes: -186.436585242\n",
      "Episode lengths, last 10: [70, 80, 84, 63, 82, 57, 87, 67, 87, 51]\n",
      "AEerror, mean over last 10: 0.270298379348\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 89770\n",
      "Episode rewards, last 10: [-193.34175201778487, -166.70550219800108, -162.95941308759259, -187.38468082957843, -158.96630026489288, -213.72795480818408, -47.265338893516883, -186.34779124764088, -164.17548485780523, -134.95475085664617]\n",
      "Mean over last 125 episodes: -178.835922848\n",
      "Episode lengths, last 10: [73, 73, 88, 76, 80, 76, 99, 85, 84, 89]\n",
      "AEerror, mean over last 10: 0.270942634382\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 98639\n",
      "Episode rewards, last 10: [-221.56242624278337, -115.11190995917158, -196.35284504904408, -212.27883815327371, -219.62761767495488, -173.02041892538369, -192.846798240759, -183.94233494820892, -192.7813457920698, -69.360563599086134]\n",
      "Mean over last 125 episodes: -176.184741591\n",
      "Episode lengths, last 10: [86, 56, 61, 86, 75, 78, 58, 73, 75, 67]\n",
      "AEerror, mean over last 10: 0.306350560081\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 107486\n",
      "Episode rewards, last 10: [-187.27720099643483, -230.74803130311682, -145.25589319227785, -242.96763497514868, -149.23533572294838, -257.59861147286836, -189.76034890786318, -189.93723005216771, -258.79662698696893, -177.36401372881983]\n",
      "Mean over last 125 episodes: -180.509342008\n",
      "Episode lengths, last 10: [72, 83, 85, 85, 64, 80, 68, 52, 95, 57]\n",
      "AEerror, mean over last 10: 0.316626780779\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 19158\n",
      "Episode rewards, last 10: [-269.68081702637915, -194.63462135081267, -212.59453519665391, -150.1748767366127, -82.705517949455356, -118.2325093053133, -146.30961046208893, 14.591980186189424, 50.903473717293537, -168.70431072457836]\n",
      "Mean over last 125 episodes: -149.43331553\n",
      "Episode lengths, last 10: [102, 131, 102, 104, 170, 171, 164, 199, 199, 179]\n",
      "AEerror, mean over last 10: 0.160078780415\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 41044\n",
      "Episode rewards, last 10: [47.641701178549752, 16.019018606510269, 20.514639846050521, -93.854155833529489, -137.4673227606097, 2.4181703472061997, -98.4490837905457, -154.3115130999401, 11.941102392769, -15.375090833129523]\n",
      "Mean over last 125 episodes: -92.6803060174\n",
      "Episode lengths, last 10: [199, 199, 199, 189, 190, 199, 197, 149, 199, 199]\n",
      "AEerror, mean over last 10: 0.0491356605997\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 60998\n",
      "Episode rewards, last 10: [15.12241752200184, -124.62850832022025, -135.93887272434947, -23.393418244175358, 47.736156362251975, 34.957822417339884, -10.719576013775502, 16.145528444222776, -217.40469661751064, -24.596693497091959]\n",
      "Mean over last 125 episodes: -90.6257411378\n",
      "Episode lengths, last 10: [199, 104, 76, 199, 199, 199, 199, 199, 192, 199]\n",
      "AEerror, mean over last 10: 0.0586910652716\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 83954\n",
      "Episode rewards, last 10: [-89.807420397852397, 13.0374736417957, -159.54644576626592, -27.025242469262515, -8.4127549796888452, -148.32735835842701, -73.752305889368202, 62.1086892024517, -123.87808168829881, -61.988295475304682]\n",
      "Mean over last 125 episodes: -23.1579979257\n",
      "Episode lengths, last 10: [130, 199, 154, 199, 199, 181, 116, 199, 151, 156]\n",
      "AEerror, mean over last 10: 0.0793429751241\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 107612\n",
      "Episode rewards, last 10: [16.305858337990493, -17.913067984740792, -185.71795398740466, -164.18185497424787, -18.866714756344841, -57.741031571811128, -202.21109208973547, -45.790630620426036, 26.386401484734812, -45.168532241495484]\n",
      "Mean over last 125 episodes: -18.7255102404\n",
      "Episode lengths, last 10: [199, 84, 102, 182, 199, 199, 156, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0739019570881\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 131030\n",
      "Episode rewards, last 10: [-64.532385083111151, 31.48721155791889, -4.0520089187860773, -1.5078463399549591, 2.8758520595880874, 35.222253101487695, 63.590273336500971, 29.341462664501091, 28.233996929551815, 30.602311824159987]\n",
      "Mean over last 125 episodes: -28.4669420852\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0329730150616\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 153861\n",
      "Episode rewards, last 10: [6.1518812048191673, 12.372161312346178, -131.72184974483136, 0.71438978596584235, 54.169221750832051, -2.9048756001975358, -4.7319913252604868, 9.6889647061308253, 15.060226204019923, -127.3182664665881]\n",
      "Mean over last 125 episodes: -33.3695685638\n",
      "Episode lengths, last 10: [199, 199, 160, 199, 199, 199, 199, 199, 199, 188]\n",
      "AEerror, mean over last 10: 0.0491620968145\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 177807\n",
      "Episode rewards, last 10: [-6.6320704459579805, 9.9090567418611144, 17.771949577479603, 17.956892556302694, -0.95503316744829903, -25.294625789233731, -58.239048173361581, -139.17299656334285, 30.134060612930497, -34.606674525925676]\n",
      "Mean over last 125 episodes: -15.6386127762\n",
      "Episode lengths, last 10: [199, 152, 199, 199, 199, 199, 199, 75, 199, 199]\n",
      "AEerror, mean over last 10: 0.0491220396515\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 201132\n",
      "Episode rewards, last 10: [-13.835685666785157, -360.95585248731061, -38.07855971332188, -108.57807627893634, -12.752861527510223, -70.854295836567488, -129.61557671257586, -27.56879614290122, 11.807218630813876, 45.242861065404824]\n",
      "Mean over last 125 episodes: -31.8757042373\n",
      "Episode lengths, last 10: [199, 134, 199, 199, 199, 165, 108, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.076632115456\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 225040\n",
      "Episode rewards, last 10: [-44.786514248389793, -160.74843973137649, 7.1591960177684619, -1.728984297901647, -25.781191665696646, -19.207488387769782, 40.134169576556914, -21.521149328883688, 13.731012380274798, -48.715291499858381]\n",
      "Mean over last 125 episodes: -23.1220161543\n",
      "Episode lengths, last 10: [199, 147, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0524111151191\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 20246\n",
      "Episode rewards, last 10: [-122.67651604154992, -75.5879366601025, -172.75867132775747, -350.20281867435028, -6.81702206445987, 6.6892373505878382, -218.96687455746155, -209.78302148908807, -74.97752569471902, -138.16070458298935]\n",
      "Mean over last 125 episodes: -84.3375919254\n",
      "Episode lengths, last 10: [179, 199, 181, 151, 199, 199, 107, 95, 199, 129]\n",
      "AEerror, mean over last 10: 0.134618412586\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 41337\n",
      "Episode rewards, last 10: [-3.0880661710837245, -23.92048085254946, -109.20551608079788, 22.709518179540851, 27.328176947778054, -14.167945344736292, -8.9900354827646023, -52.031173485814747, -5.6641642886186698, -11.200233320701397]\n",
      "Mean over last 125 episodes: -99.8601567079\n",
      "Episode lengths, last 10: [199, 199, 124, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0834010147001\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 61522\n",
      "Episode rewards, last 10: [-283.53337246445403, -143.82501468925409, -258.91000502897219, -107.62243003161744, -112.93839055200026, -306.0617821877301, -44.262585923171578, 45.595289844090601, -73.425722937855923, 29.703531507536525]\n",
      "Mean over last 125 episodes: -129.323602801\n",
      "Episode lengths, last 10: [141, 124, 81, 141, 199, 135, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.123340400378\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 80214\n",
      "Episode rewards, last 10: [-33.881001793468556, 3.0682553336894638, -140.72470820879363, -201.23083171390618, -58.023656911990699, -15.315358392225482, -39.687193058099936, -401.30995460963936, -266.36797770058433, -6.6177989940300108]\n",
      "Mean over last 125 episodes: -147.913586639\n",
      "Episode lengths, last 10: [199, 199, 158, 199, 136, 199, 199, 150, 154, 199]\n",
      "AEerror, mean over last 10: 0.126438763984\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 102160\n",
      "Episode rewards, last 10: [34.457276224713098, -5.0361102858462132, -175.53141340894749, 2.7004630429916396, -12.372146605043143, -371.18579971251398, -68.389723218393115, -243.78403298149152, -98.092552575799743, -16.425745405173743]\n",
      "Mean over last 125 episodes: -80.5378985607\n",
      "Episode lengths, last 10: [199, 199, 176, 199, 199, 96, 199, 141, 140, 125]\n",
      "AEerror, mean over last 10: 0.0997203185094\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 123178\n",
      "Episode rewards, last 10: [-25.944046691228362, -266.80112177624994, -10.048891804293291, -130.41239823183412, -438.47432717932293, -115.98335432426512, -56.919233035871962, -43.264807986807398, 14.953958044881354, -150.12696304779018]\n",
      "Mean over last 125 episodes: -106.570989249\n",
      "Episode lengths, last 10: [199, 125, 199, 57, 85, 176, 199, 199, 199, 122]\n",
      "AEerror, mean over last 10: 0.10373327398\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 147219\n",
      "Episode rewards, last 10: [25.350653694595451, -9.5487431428981502, -134.46686062770564, 5.2430499648378008, -4.2156232309530068, -1.7813319278431763, 20.595364456887751, -315.20937423260898, -98.113917869605828, 13.890795841985899]\n",
      "Mean over last 125 episodes: -21.0612785091\n",
      "Episode lengths, last 10: [199, 199, 167, 199, 199, 199, 199, 166, 94, 199]\n",
      "AEerror, mean over last 10: 0.106132359242\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 167819\n",
      "Episode rewards, last 10: [-145.81390710554152, -131.86737034112699, -156.18342733700865, -198.97340035344351, -194.58008006063085, -170.70419145384378, -180.2926574841612, -128.58713890824924, -273.59861332859157, -419.84569854160418]\n",
      "Mean over last 125 episodes: -85.3493447101\n",
      "Episode lengths, last 10: [157, 150, 143, 116, 86, 60, 154, 57, 89, 121]\n",
      "AEerror, mean over last 10: 0.15481245714\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 180941\n",
      "Episode rewards, last 10: [-194.95607073922324, -203.76275893176836, -188.36601147630105, -378.7960887264789, -291.6280284220008, -278.60855000104704, -285.42030437605263, -223.25447481755964, -186.44721470209313, -260.8121944253391]\n",
      "Mean over last 125 episodes: -194.656825262\n",
      "Episode lengths, last 10: [121, 72, 89, 136, 147, 188, 104, 113, 66, 132]\n",
      "AEerror, mean over last 10: 0.190825224781\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 194172\n",
      "Episode rewards, last 10: [-154.5228232317682, -178.7228664254082, -65.169969758079134, -182.67146932372609, -172.45147322456387, -209.33037543887446, -144.1420308296922, -216.02104031473891, -186.80038553965716, -195.0070026573469]\n",
      "Mean over last 125 episodes: -200.32205356\n",
      "Episode lengths, last 10: [55, 75, 66, 71, 84, 58, 54, 58, 85, 72]\n",
      "AEerror, mean over last 10: 0.203500679542\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 20029\n",
      "Episode rewards, last 10: [14.290509616704373, -299.0934096341976, 52.239100828396111, 29.984273410935664, -204.54062982303461, 35.651518130736797, -56.063414109067132, 28.15875633781901, -1.8733765902883945, 8.8694570764458405]\n",
      "Mean over last 125 episodes: -111.478876584\n",
      "Episode lengths, last 10: [199, 86, 199, 199, 152, 199, 134, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0631649184001\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 42103\n",
      "Episode rewards, last 10: [-41.324668131450672, -263.91021756750087, -181.27770976922335, -137.74239177712661, -7.955412498390551, -141.22969755190246, 59.185029424925446, -41.955426631779673, -33.895870982728979, -48.688395339366835]\n",
      "Mean over last 125 episodes: -57.1728027073\n",
      "Episode lengths, last 10: [199, 168, 148, 150, 199, 71, 199, 180, 199, 180]\n",
      "AEerror, mean over last 10: 0.097608776106\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 63238\n",
      "Episode rewards, last 10: [-332.08909571173751, -324.44328968833679, -13.68525867853092, -328.37529355970815, -14.513444214756269, 2.3622187306675695, -28.233574055937041, -7.7052648760308475, -118.0638600099893, -299.36360469585554]\n",
      "Mean over last 125 episodes: -100.207324257\n",
      "Episode lengths, last 10: [98, 132, 199, 147, 199, 199, 90, 199, 126, 74]\n",
      "AEerror, mean over last 10: 0.0992414993081\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 76249\n",
      "Episode rewards, last 10: [-146.19973968928645, -203.04682006598384, -212.34795022553644, -186.07711801994037, -207.40694379587666, -244.16532855268824, -20.512175938215581, -189.57148263052241, -114.0234218540277, -182.36653639871216]\n",
      "Mean over last 125 episodes: -174.936029505\n",
      "Episode lengths, last 10: [62, 68, 87, 60, 73, 87, 85, 63, 59, 56]\n",
      "AEerror, mean over last 10: 0.277588634078\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 85371\n",
      "Episode rewards, last 10: [-187.55849385774704, -177.54583855084024, -137.86256784078074, -74.572422991810669, -188.07944664177433, -177.97807280825404, -191.30201682694107, -125.04798224903975, -164.15239898931856, -149.10138682269292]\n",
      "Mean over last 125 episodes: -179.799903557\n",
      "Episode lengths, last 10: [60, 70, 59, 64, 57, 49, 66, 89, 55, 79]\n",
      "AEerror, mean over last 10: 0.20629066202\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 94148\n",
      "Episode rewards, last 10: [-208.85739657855279, -177.61398070165467, -203.14301408268037, -188.18498532692956, -222.54689706464984, -170.58402641439926, -176.89052325426314, -225.58419859143618, -130.57742639840598, -181.58739401678548]\n",
      "Mean over last 125 episodes: -185.89393515\n",
      "Episode lengths, last 10: [83, 70, 61, 80, 77, 55, 81, 75, 56, 60]\n",
      "AEerror, mean over last 10: 0.201681312456\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 102849\n",
      "Episode rewards, last 10: [-165.75692266967579, -230.99658029757032, -201.42865953388244, -187.2876470257392, -212.80740374701747, -220.01425609953074, -218.20842000115084, -170.93603848615368, -173.6348346531945, -170.6070288262529]\n",
      "Mean over last 125 episodes: -183.430080264\n",
      "Episode lengths, last 10: [89, 80, 89, 67, 70, 66, 82, 89, 68, 82]\n",
      "AEerror, mean over last 10: 0.192539436294\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 111532\n",
      "Episode rewards, last 10: [-218.38300519967723, -195.47933431950335, -165.81726510817921, -208.63237224418651, -172.36623145282235, -179.79164376239999, -158.68904222450192, -204.3636610119845, -185.08926357203032, -215.36336621860525]\n",
      "Mean over last 125 episodes: -184.419720181\n",
      "Episode lengths, last 10: [82, 66, 82, 63, 57, 68, 75, 60, 61, 76]\n",
      "AEerror, mean over last 10: 0.218890709301\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 120344\n",
      "Episode rewards, last 10: [-201.98892709360862, -152.40022390704809, -173.78523125099341, -189.97423331482725, -140.87130828260882, -195.4628281672706, -214.35393994588722, -152.18823324265338, -202.98759665311621, -207.09137673795087]\n",
      "Mean over last 125 episodes: -174.090556014\n",
      "Episode lengths, last 10: [63, 54, 66, 70, 52, 60, 71, 57, 73, 74]\n",
      "AEerror, mean over last 10: 0.20957117945\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 129223\n",
      "Episode rewards, last 10: [-175.79228660737812, -54.197268422949819, -234.85340207339362, -166.3870959463392, -156.85101411360995, -188.92755644935977, -160.60600463918894, -199.06103924028858, -154.39134114754771, -201.39311197607006]\n",
      "Mean over last 125 episodes: -175.011285504\n",
      "Episode lengths, last 10: [70, 64, 72, 64, 61, 68, 68, 65, 64, 55]\n",
      "AEerror, mean over last 10: 0.214439900822\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 9969\n",
      "Episode rewards, last 10: [-226.06982397233674, -175.61624406282624, -167.11342997264964, -237.43880161894336, -177.31358775983529, -184.59405647824167, -209.79994401643981, -212.68481060642819, -163.31512725619081, -202.15402495498131]\n",
      "Mean over last 125 episodes: -220.947449809\n",
      "Episode lengths, last 10: [93, 80, 68, 91, 57, 67, 76, 86, 61, 99]\n",
      "AEerror, mean over last 10: 0.347683953599\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 18799\n",
      "Episode rewards, last 10: [-202.02244184728312, -180.17991819564014, -140.11202343246964, -186.88976804961419, -154.26071518458488, -180.5112485176883, -146.78720429979307, -227.75248517514663, -219.8715749384794, -195.45160363787033]\n",
      "Mean over last 125 episodes: -177.927034356\n",
      "Episode lengths, last 10: [66, 50, 53, 68, 51, 62, 52, 65, 87, 71]\n",
      "AEerror, mean over last 10: 0.348711214552\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 27637\n",
      "Episode rewards, last 10: [-195.50819254322494, -225.41461643787923, -177.79226421974695, -187.31901370519773, -131.30230177382904, -338.6481776904443, -181.57359030545413, -159.77815790190266, -215.95324575323033, -290.98978626993107]\n",
      "Mean over last 125 episodes: -185.366199919\n",
      "Episode lengths, last 10: [65, 62, 82, 73, 53, 83, 70, 54, 73, 78]\n",
      "AEerror, mean over last 10: 0.462822869174\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 36452\n",
      "Episode rewards, last 10: [-193.18112196183961, -165.97698062881693, -279.72903692839282, -207.3920229854601, -227.76020647309571, -227.06112989437941, -224.20084007015086, -188.30737044771519, -146.11129164625817, -165.55014638338511]\n",
      "Mean over last 125 episodes: -184.184511768\n",
      "Episode lengths, last 10: [60, 53, 89, 62, 72, 84, 75, 71, 52, 65]\n",
      "AEerror, mean over last 10: 0.425894848412\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 45280\n",
      "Episode rewards, last 10: [-200.94564395872112, -152.03878702101426, -185.49644023783031, -219.57362346245822, -172.6780165908319, -238.8970919280581, -177.71104774665116, -198.74690332691344, -207.0517661097972, -157.52684744133569]\n",
      "Mean over last 125 episodes: -180.859833351\n",
      "Episode lengths, last 10: [64, 89, 82, 76, 59, 86, 64, 78, 89, 65]\n",
      "AEerror, mean over last 10: 0.296357393359\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 54128\n",
      "Episode rewards, last 10: [-198.63910828822668, -190.40498683034576, -260.51671966176139, -193.5205563765829, -75.76001417537006, -242.8775750848277, -184.95118721699629, -160.9831527356601, -191.76616893294596, -185.70235670996709]\n",
      "Mean over last 125 episodes: -176.409438218\n",
      "Episode lengths, last 10: [58, 76, 86, 76, 80, 82, 68, 63, 66, 60]\n",
      "AEerror, mean over last 10: 0.292245151707\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 63045\n",
      "Episode rewards, last 10: [-163.16623919848357, -187.89814040744233, -207.13372917770809, -151.376235724662, -165.5776735746812, -221.7660299467982, -144.87930447262016, -162.7501959374473, -172.1543819965211, -176.95042285842584]\n",
      "Mean over last 125 episodes: -176.994526152\n",
      "Episode lengths, last 10: [78, 72, 67, 52, 74, 98, 85, 58, 59, 62]\n",
      "AEerror, mean over last 10: 0.311006544719\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 71952\n",
      "Episode rewards, last 10: [-194.44939643953484, -226.23331293605946, -197.85452241521099, -227.69277608026925, -205.52893344736822, -171.39417609075394, -224.24819291991253, -188.42512156114165, -187.68678037404902, -156.91643029048907]\n",
      "Mean over last 125 episodes: -179.976802426\n",
      "Episode lengths, last 10: [73, 76, 79, 78, 62, 59, 80, 58, 68, 50]\n",
      "AEerror, mean over last 10: 0.411274209403\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 80930\n",
      "Episode rewards, last 10: [-343.06343328436623, -204.96716282090605, -164.15718047476108, -168.86082560210428, -202.64255653637954, -178.50399732702689, -205.90458807685386, -198.29600076063812, -158.01076746790858, -258.01037453733136]\n",
      "Mean over last 125 episodes: -187.403340156\n",
      "Episode lengths, last 10: [102, 79, 60, 64, 81, 59, 91, 81, 75, 89]\n",
      "AEerror, mean over last 10: 0.397735438667\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 89803\n",
      "Episode rewards, last 10: [-142.65521919065185, -169.92111558231937, -132.639563923428, -181.07580917901305, -187.47746884365768, -199.894124383193, -178.59395159657552, -179.07955034049434, -204.41493457652629, -191.00253533543651]\n",
      "Mean over last 125 episodes: -180.806894805\n",
      "Episode lengths, last 10: [81, 77, 55, 86, 65, 89, 87, 54, 73, 62]\n",
      "AEerror, mean over last 10: 0.398180911124\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 17942\n",
      "Episode rewards, last 10: [-8.9694698348691073, -7.4179062853178452, 35.454402700542779, -20.864329334901832, -321.03311206980709, -400.78273346443115, 0.67997398204755655, -144.06076438187341, -117.78220132876136, 43.642502597964736]\n",
      "Mean over last 125 episodes: -152.216056456\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 178, 73, 199, 152, 199, 199]\n",
      "AEerror, mean over last 10: 0.115788499344\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 40671\n",
      "Episode rewards, last 10: [-100.69803631189816, -5.0169298158342812, -229.99056944466159, -83.933150851543274, -125.35481283539333, -361.57892954660883, -194.29172400793081, -330.69983882649694, -345.00422425501426, -325.0790689013798]\n",
      "Mean over last 125 episodes: -66.8189684473\n",
      "Episode lengths, last 10: [164, 199, 157, 199, 148, 151, 199, 197, 146, 81]\n",
      "AEerror, mean over last 10: 0.151254909519\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 61321\n",
      "Episode rewards, last 10: [36.744277797344779, -69.820693171211829, -4.1709881780768416, -279.76577302301575, -299.78754751745521, -165.95422810884963, -154.58698430379638, -78.766028576973383, -23.923292422190716, -1.5996145097971883]\n",
      "Mean over last 125 episodes: -91.3736573886\n",
      "Episode lengths, last 10: [199, 199, 199, 143, 112, 152, 85, 91, 199, 199]\n",
      "AEerror, mean over last 10: 0.123415240324\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 84137\n",
      "Episode rewards, last 10: [-29.415424313522262, -60.078881082701386, -19.383981997650736, 13.395104532534379, -96.541491438864711, -88.085988230310349, 21.677159300533503, 38.803487117671324, -525.75557500711852, -78.719294813130432]\n",
      "Mean over last 125 episodes: -54.5318519659\n",
      "Episode lengths, last 10: [138, 199, 153, 199, 163, 166, 199, 199, 135, 137]\n",
      "AEerror, mean over last 10: 0.131881639359\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 106255\n",
      "Episode rewards, last 10: [-4.7071669576064821, -137.28587231189761, -109.9034372686844, -113.78340763400229, -47.382607498422011, -376.53564936245664, -149.03382311647451, -25.202082445992438, -309.86788137092975, -327.73110730721112]\n",
      "Mean over last 125 episodes: -69.0197508523\n",
      "Episode lengths, last 10: [199, 82, 146, 134, 199, 80, 199, 199, 142, 115]\n",
      "AEerror, mean over last 10: 0.133062072218\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 128025\n",
      "Episode rewards, last 10: [27.125988467684937, -57.015313453248218, 1.0408205004739042, -61.433196152786763, 4.0309474877079543, 20.561407107882218, 5.0469632046002371, -170.77867469712001, -18.793254213548021, 12.855095772793131]\n",
      "Mean over last 125 episodes: -97.0722847542\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 101, 199, 199]\n",
      "AEerror, mean over last 10: 0.0786231747041\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 151945\n",
      "Episode rewards, last 10: [-34.743599589049296, -45.899641201469741, 50.963401085360395, -216.37928632308746, -151.21561704768504, 61.866454066331563, 3.8246141614684825, 13.786723134485159, -8.0093905194347812, 17.070833952342504]\n",
      "Mean over last 125 episodes: -47.3607451583\n",
      "Episode lengths, last 10: [199, 199, 199, 112, 153, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0810016755133\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 175014\n",
      "Episode rewards, last 10: [2.4923540431271629, 19.80782737053072, -10.12280358026433, -46.521862150710383, 32.209042133308465, -8.7605728470289428, -18.619859944435813, -9.5677410763690904, 12.953649609037431, -51.901613528814934]\n",
      "Mean over last 125 episodes: -27.4424857701\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 199, 199, 199, 199, 199, 126]\n",
      "AEerror, mean over last 10: 0.0771161449061\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 198295\n",
      "Episode rewards, last 10: [-128.73297432818975, 98.635781864942231, 15.282800812877682, -11.872656408885586, -201.9006035057879, 3.8217059113083351, 38.565141282072247, -59.683254658722156, 51.755868617749783, 11.886085759775263]\n",
      "Mean over last 125 episodes: -34.0375028427\n",
      "Episode lengths, last 10: [144, 199, 199, 199, 199, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0886927943255\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 222174\n",
      "Episode rewards, last 10: [1.7707192213605882, 20.140025452470443, 36.889288927271416, -36.425816835502694, -193.34589320951309, 4.0480238409537881, -3.672958274630707, -21.57651664793682, -6.5131643886327133, -25.13386692811461]\n",
      "Mean over last 125 episodes: -12.7197576093\n",
      "Episode lengths, last 10: [199, 199, 199, 199, 169, 199, 199, 199, 199, 199]\n",
      "AEerror, mean over last 10: 0.0818463472012\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = produce_experiment(DDQL, ddql_egreedy_params, ddql_train_params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(ddql_res, window = 100, std_coef = 0.2, results_over = 1000):\n",
    "    res_lists = [k.rList for k in ddql_res]\n",
    "    res_lists = np.array(res_lists)\n",
    "    pd.DataFrame(data = res_lists)\n",
    "    mean = res_lists.mean(axis = 0)\n",
    "    std = res_lists.std(axis = 0)\n",
    "    rol_mean = np.nan_to_num(pd.Series(mean).rolling(window = window).mean())[window:]\n",
    "    rol_std = np.nan_to_num(pd.Series(std).rolling(window = window).mean())[window:]\n",
    "    plt.figure()\n",
    "    index = np.arange(window, len(rol_mean) + window)\n",
    "    plt.plot(index, rol_mean)\n",
    "    plt.fill_between(index, rol_mean-std_coef*rol_std, rol_mean+std_coef*rol_std, color='b', alpha=0.1)\n",
    "    return max(rol_mean[window:results_over]), rol_mean[window:results_over].mean(), pd.DataFrame(data = res_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMAzDgIUBwzAMAxYGDMMwDID/D4dhdkMDop5YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1452bbf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_max, count_auc, count_df = plot_results(res, 100, 0.2)\n",
    "plt.savefig(\"count_ll.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-98.1820955319 -126.076107007\n"
     ]
    }
   ],
   "source": [
    "print count_max, count_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_df.to_csv(\"count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "240d01c84ba64f889dafe8f4c457ab1d": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "4bd69076e57a452e82861700ca4e23d6": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "5267e74bc2ac47b2aecb1d643149665b": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "59d634fb21a9458c81d8c879deea5b0e": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "92cca418426b4a53835c437c2467a617": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "99042b0733814588b3a9068757477084": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "ae36ebb3ca32488eb928a9d9cf04fdad": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "ce39d87348c84beba43e2fbd683c4649": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "eb11acb900a141578e4089ecf07dad08": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "ff107dcf7513461cb308095ce1e8ac27": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
