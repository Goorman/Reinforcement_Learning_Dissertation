{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, h = 50, alpha = 25):\n",
    "        self.input_var = T.tensor4('input')\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape)\n",
    "        self.build_AVQ(grad_clipping, lr, h, alpha)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 50, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr, h, alpha):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        self.l_varq = lasagne.layers.DenseLayer(self.outlayer, self.n_actions, nonlinearity=lasagne.nonlinearities.softplus)\n",
    "        \n",
    "        self.advantage, self.value, self.varq = lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_varq])\n",
    "\n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Q = self.advantage + self.value - self.average_advantage\n",
    "        self.Q = self.advantage\n",
    "        self.predict = T.argmax(self.Q, axis = 1)\n",
    "        \n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, self.n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.Q0 = T.sum(self.Q * self.actions_onehot, axis = 1)\n",
    "        self.varQ0 = T.sum(self.varq * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.Q1 = self.Q0 + (self.targetQ - self.Q0) / (h + 1)\n",
    "        self.varQ1 = (h * (alpha - 1)) / ((h + 1) * (alpha - 0.5)) * \\\n",
    "                     (self.varQ0 + T.sqr(self.targetQ - self.Q0) / (2 * (h+1) * (alpha - 1)))\n",
    "        \n",
    "        self.td_error = T.mean(T.sqr(self.Q1 - self.Q0))\n",
    "        self.var_error = T.mean(T.sqr(self.varQ1 - self.varQ0))\n",
    "        self.loss = self.td_error + self.var_error\n",
    "        \n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Q)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.train_fn = theano.function([self.input_var, self.targetQ, self.actions], [self.loss, self.Q1, self.varQ1], updates = self.updates)\n",
    "        self.var_fn = theano.function([self.input_var], self.varq)\n",
    "    \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params([self.l_advantage, self.l_varq], trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_varq], trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_varq], values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bayes_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, var_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "        self.var_fn = var_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            scores = self.var_fn(np.expand_dims(state, axis = 0))\n",
    "            a = np.argmax(scores)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.h = lparams[\"lambda\"]\n",
    "        self.alpha = lparams[\"alpha\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "        \n",
    "        self.env = env\n",
    "        AVQ_params = [self.window_size, self.env.state_size, self.env.n_actions, \n",
    "                      self.grad_clip, self.lr, self.h, self.alpha]\n",
    "        self.mainQN = AVQ_nn(*AVQ_params)\n",
    "        self.targetQN = AVQ_nn(*AVQ_params)\n",
    "\n",
    "        self.jList = []\n",
    "        self.rList = []\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            return egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            return boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        elif agent == \"bayes\":\n",
    "            return bayes_agent(self.env.n_actions, self.mainQN.actionpred_fn, self.mainQN.var_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "\n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "            \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "                \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                self.experience_storage.add(np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5]))\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        \n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)   \n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        \n",
    "                        \n",
    "                        doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        loss, Qval, Qvar = self.mainQN.train_fn(old_state_batch, targetQ, action_vector)\n",
    "                         \n",
    "                else:\n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            self.jList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.jList[-10:])]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.jList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-15 22:04:37,076] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "ll_env = LunarLanding_wrapper()\n",
    "#cp_env = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 1,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"lambda\": 4,\n",
    "           \"alpha\": 2,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 125,\n",
    "           \"pretrain_steps\" : 500,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.1,\n",
    "                                \"anneling_steps\":1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayesagentinfo = {\"agent\" : \"bayes\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.1,\n",
    "                                \"anneling_steps\":1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_experiment(ddql, ddql_init_params, ddql_train_params, experiment_num = 5):\n",
    "    ddql_list = [ddql(**ddql_init_params) for k in range(experiment_num)]\n",
    "    \n",
    "    for k in range(experiment_num):\n",
    "        ddql_list[k].train(**ddql_train_params)\n",
    "        \n",
    "    return ddql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql_bayes_params = {\"lparams\":lparams, \"env\":ll_env, \"agent\":bayesagentinfo}\n",
    "\n",
    "ddql_train_params = {\"num_episodes\":1250, \"frame_limit\":200, \"render\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 124 ================================================\n",
      "Total steps: 8419\n",
      "Episode rewards, last 10: [-382.11978008814924, -261.45631898080148, -146.38175140305071, -245.29091755849794, -180.6841090642798, -241.30156325107305, -216.32882757742206, -189.0741103944369, -139.51861020622886, -128.18747683783073]\n",
      "Mean over last 125 episodes: -209.283554822\n",
      "Episode lengths, last 10: [81, 64, 47, 87, 68, 63, 64, 71, 57, 61]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 18044\n",
      "Episode rewards, last 10: [-131.12869645742973, -181.89102753294915, -172.397074995183, -199.50478838819646, -163.17928961728958, -270.16701142741482, -258.70466432190369, -160.21599126252511, -244.56931559734252, -330.96213580569082]\n",
      "Mean over last 125 episodes: -217.439851729\n",
      "Episode lengths, last 10: [76, 85, 64, 61, 68, 80, 57, 87, 65, 89]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 27423\n",
      "Episode rewards, last 10: [-140.16569767936411, -181.99786543368114, -120.37734225704028, -102.672082681323, -97.781893430259203, -335.98933594851371, -69.425750300510899, -515.49532468979066, -171.32550587090344, -69.585732407856966]\n",
      "Mean over last 125 episodes: -182.115913623\n",
      "Episode lengths, last 10: [88, 61, 59, 55, 59, 77, 89, 92, 114, 68]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 38887\n",
      "Episode rewards, last 10: [-179.81992414649159, -134.16895273310266, -157.29716340135312, -234.0793770920728, -154.89502137374879, -181.6921332298968, -52.203394146971746, -254.16797829019106, -271.22265370422764, -37.733570016154758]\n",
      "Mean over last 125 episodes: -172.256235869\n",
      "Episode lengths, last 10: [85, 108, 91, 132, 129, 92, 93, 150, 100, 99]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 49317\n",
      "Episode rewards, last 10: [-144.07302779054015, -252.52637564371628, -182.85345024297052, -193.23977011166176, -301.9677924932397, -149.93733595673567, -167.2002733192287, -171.18057568343144, -201.58467073568153, -227.92104826217388]\n",
      "Mean over last 125 episodes: -191.922392334\n",
      "Episode lengths, last 10: [51, 83, 81, 59, 83, 56, 85, 86, 62, 70]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 58080\n",
      "Episode rewards, last 10: [-165.36893386113988, -217.65179925992612, -207.68574397959495, -138.30629128441424, -195.61329291410581, -185.60665099286163, -238.82706215586668, -291.64294351547926, -188.6147901307923, -261.14275756977167]\n",
      "Mean over last 125 episodes: -199.940856164\n",
      "Episode lengths, last 10: [65, 70, 57, 49, 73, 61, 83, 74, 83, 82]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 66926\n",
      "Episode rewards, last 10: [-151.21294907660362, -47.01917435145436, -139.76448352331548, -160.16287291399539, -160.55480972912662, -189.39387388170832, -208.25947380853626, -197.78047686476492, -232.64692653285996, -223.65014902889538]\n",
      "Mean over last 125 episodes: -203.100389689\n",
      "Episode lengths, last 10: [56, 97, 49, 71, 60, 61, 50, 87, 86, 73]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 75607\n",
      "Episode rewards, last 10: [-157.10301111160967, -212.25948174253443, -215.4336094407137, -203.82904098986097, -184.08539583791423, -191.13986695929773, -188.36799534054089, -178.52943398300229, -224.23690634809532, -157.01351450910076]\n",
      "Mean over last 125 episodes: -210.153654039\n",
      "Episode lengths, last 10: [70, 69, 72, 68, 64, 87, 86, 65, 60, 57]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 84526\n",
      "Episode rewards, last 10: [-155.59662739619372, -288.41750515566446, -177.89233148070414, -146.85339615879482, -161.50409125998729, -230.50067888671109, -152.79015163034913, -278.34125487574886, -180.60977258291459, -285.51308973504251]\n",
      "Mean over last 125 episodes: -206.013983744\n",
      "Episode lengths, last 10: [52, 83, 80, 53, 83, 66, 53, 77, 86, 96]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 93212\n",
      "Episode rewards, last 10: [-165.52833374904719, -180.39749264886134, -159.24007722026508, -242.7438958650194, -177.10916604313576, -227.60779180469757, -132.70482880722, -198.93129304288695, -209.94831736422867, -204.45529517264498]\n",
      "Mean over last 125 episodes: -190.066218485\n",
      "Episode lengths, last 10: [84, 89, 79, 77, 85, 66, 59, 77, 99, 64]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 9132\n",
      "Episode rewards, last 10: [-142.47150309720175, -222.7825171846045, -208.20632608205071, -222.70316849345221, -175.78119827668465, -217.42866946171335, -183.13343977750549, -147.10039244447287, -185.25984706508996, -155.87988154986368]\n",
      "Mean over last 125 episodes: -191.469057364\n",
      "Episode lengths, last 10: [97, 83, 72, 77, 67, 74, 57, 86, 59, 60]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 18394\n",
      "Episode rewards, last 10: [-187.7339947284413, -168.87071325983186, -214.25399090850212, -133.05962415681751, -131.521020022356, -115.73393462535262, -140.13453751066845, -181.23457083810627, -152.84061688169623, -219.35957300396581]\n",
      "Mean over last 125 episodes: -187.244234688\n",
      "Episode lengths, last 10: [56, 60, 82, 59, 57, 57, 62, 96, 84, 81]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 27702\n",
      "Episode rewards, last 10: [-193.5101080271069, -231.59483974984104, -330.49108730467412, -237.08497630522297, -214.24494859857776, -205.54917982486663, -171.13212479305932, -161.93362206797758, -163.10942164111032, -33.75030017804977]\n",
      "Mean over last 125 episodes: -184.925447376\n",
      "Episode lengths, last 10: [84, 70, 81, 64, 63, 63, 59, 96, 58, 76]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 37335\n",
      "Episode rewards, last 10: [-152.51293963605087, -299.97820129739318, -165.16106137062744, -154.21378083290898, -137.50394420512774, -178.28698189162282, -224.23021907290229, -284.95964335965687, -153.92999249288189, -130.20172439028525]\n",
      "Mean over last 125 episodes: -185.06351523\n",
      "Episode lengths, last 10: [64, 93, 105, 92, 94, 79, 75, 103, 65, 59]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 46991\n",
      "Episode rewards, last 10: [-226.94194119922633, -260.88588082995756, -52.037073005104844, -165.59534090472542, -177.16263747175799, -132.44242375755738, -91.198959286987176, -201.66258629741094, -34.505531654148413, -318.14538988054511]\n",
      "Mean over last 125 episodes: -169.764942651\n",
      "Episode lengths, last 10: [80, 74, 92, 78, 61, 54, 95, 87, 68, 86]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 56080\n",
      "Episode rewards, last 10: [-308.34036445671325, -215.19569178406766, -175.59057606618691, -183.11671369035261, -161.83723171433576, -197.67563899458554, -226.31045129489817, -181.91579500470584, -213.29277237738697, -247.17661327190987]\n",
      "Mean over last 125 episodes: -192.390412264\n",
      "Episode lengths, last 10: [92, 66, 59, 51, 55, 74, 69, 57, 63, 92]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 64957\n",
      "Episode rewards, last 10: [-422.5094147144103, -11.74490664706974, -174.41820626779648, -394.28214300651553, -210.82542698455509, -171.54768762683477, -166.92365217865171, -168.07651271982508, -330.82873447107295, -196.11161159141568]\n",
      "Mean over last 125 episodes: -192.480976383\n",
      "Episode lengths, last 10: [101, 95, 57, 78, 76, 72, 61, 88, 84, 75]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 73700\n",
      "Episode rewards, last 10: [-190.94689093392861, -168.63902191144584, -155.10430094121588, -239.65479229094609, -210.34616242525234, -176.43072548679413, -159.11729421276587, -188.50033699037493, -193.98372930966542, -205.72164530548125]\n",
      "Mean over last 125 episodes: -209.395214679\n",
      "Episode lengths, last 10: [65, 50, 88, 83, 86, 60, 54, 64, 65, 90]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 82574\n",
      "Episode rewards, last 10: [-183.25853961687284, -58.446888788277946, -158.11533648343698, -144.35430848983373, -199.77805142633898, -225.29046017542376, -119.4766815724453, -171.77693179945595, -188.0274315389307, -208.98417138063377]\n",
      "Mean over last 125 episodes: -176.89419651\n",
      "Episode lengths, last 10: [53, 57, 76, 85, 53, 81, 66, 55, 67, 70]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 91145\n",
      "Episode rewards, last 10: [-132.61186613649213, -214.45773795356976, -181.59015662465521, -177.42756197519444, -164.15285330404447, -175.00220306878816, -174.18323139956561, -183.36418734979978, -197.9801624304545, -178.59012114342039]\n",
      "Mean over last 125 episodes: -180.793144483\n",
      "Episode lengths, last 10: [52, 72, 65, 59, 65, 78, 58, 62, 61, 87]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8322\n",
      "Episode rewards, last 10: [-159.31599452612429, -203.99264905867079, -193.11090691622431, -218.1772996000459, -220.36297251631504, -187.54535837925857, -176.72105669799649, -232.31785736548497, -213.81958243051642, -305.50254327383459]\n",
      "Mean over last 125 episodes: -212.619570683\n",
      "Episode lengths, last 10: [64, 70, 57, 71, 69, 64, 59, 62, 50, 79]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 16812\n",
      "Episode rewards, last 10: [-203.31016962107105, -188.30945708222234, -192.91342841251748, -184.11877924756854, -177.00015534735388, -190.72400011804197, -177.37656869426812, -162.24829345423385, -169.83939351053911, -295.05172681300951]\n",
      "Mean over last 125 episodes: -202.435485292\n",
      "Episode lengths, last 10: [53, 65, 63, 49, 75, 52, 65, 51, 78, 78]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 25593\n",
      "Episode rewards, last 10: [-237.05931493882747, -231.76945798886749, -165.16354851865981, -170.41343236040385, -234.00362803140152, -163.40875903512932, -165.83616650344712, -160.42137253861998, -234.60972962151621, -188.03303353249729]\n",
      "Mean over last 125 episodes: -195.223994732\n",
      "Episode lengths, last 10: [73, 72, 72, 57, 64, 63, 60, 66, 71, 77]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 34243\n",
      "Episode rewards, last 10: [-143.57636307859042, -184.44662986112411, -175.26231103686428, -175.51610091048497, -212.41492767222354, -157.02526695720064, -205.03917552430889, -177.0505826459783, -209.86088509134109, -142.10329957963299]\n",
      "Mean over last 125 episodes: -181.109522169\n",
      "Episode lengths, last 10: [88, 69, 64, 63, 84, 57, 64, 74, 77, 86]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 42849\n",
      "Episode rewards, last 10: [-161.11977437569189, -197.29361814462902, -246.03904324096092, -186.81477615151189, -148.60029151556199, -170.52365531185885, -159.82816789955453, -209.92966863352925, -200.42109822523253, -164.58558765924562]\n",
      "Mean over last 125 episodes: -177.151082849\n",
      "Episode lengths, last 10: [87, 63, 81, 64, 80, 49, 64, 60, 71, 62]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 51471\n",
      "Episode rewards, last 10: [-169.13000498201021, -164.39605444907369, -202.6260905190486, -202.11381853709321, -190.53962354780532, -119.17818134134909, -226.51222831791284, -154.18942863619657, -196.52064314206629, -312.26165527751584]\n",
      "Mean over last 125 episodes: -177.161101449\n",
      "Episode lengths, last 10: [56, 61, 62, 54, 58, 51, 76, 77, 60, 102]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 60103\n",
      "Episode rewards, last 10: [-220.31124771115381, -169.4612110043679, -167.32709389002747, -194.67262713546836, -166.72136393086629, -241.65924644343966, -233.99349983403391, -141.69083642003696, -160.46494021413716, -175.38136578967456]\n",
      "Mean over last 125 episodes: -180.507075708\n",
      "Episode lengths, last 10: [79, 61, 74, 90, 56, 89, 86, 57, 64, 77]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 68841\n",
      "Episode rewards, last 10: [-173.12759270466168, -171.66162522316142, -192.73216056599676, -222.10211358182005, -241.25631116387819, -154.13690213973314, -203.40531063570813, -159.39679101658703, -190.87741853951275, -202.46740608687099]\n",
      "Mean over last 125 episodes: -178.490587545\n",
      "Episode lengths, last 10: [80, 81, 72, 84, 85, 53, 77, 87, 73, 78]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 77484\n",
      "Episode rewards, last 10: [-142.88473494598142, -172.80807208743565, -193.16304124625609, -171.94195169505045, -160.1678762754471, -194.2850936223831, -278.36610343531777, -181.6506489255413, -148.02758243147412, -191.8882306977892]\n",
      "Mean over last 125 episodes: -170.363773163\n",
      "Episode lengths, last 10: [53, 51, 78, 59, 51, 78, 107, 62, 84, 62]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 86246\n",
      "Episode rewards, last 10: [-192.50711022310625, -186.25680865172262, -160.11046632596191, -185.12856516238142, -230.62805068171349, -181.74551925754898, -168.77202551522302, -140.98904553900653, -192.19950238607049, -311.9884748140218]\n",
      "Mean over last 125 episodes: -174.801798258\n",
      "Episode lengths, last 10: [73, 69, 61, 68, 85, 71, 76, 57, 60, 81]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 9092\n",
      "Episode rewards, last 10: [-189.4218173045935, -192.64399969597761, -211.08259708466142, -187.18075053754274, -209.767315405945, -193.02715394129223, -243.75218761111495, -201.20017302498297, 7.2658894056098333, -274.39118863182262]\n",
      "Mean over last 125 episodes: -212.493980351\n",
      "Episode lengths, last 10: [60, 64, 84, 77, 55, 76, 52, 60, 174, 94]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 18424\n",
      "Episode rewards, last 10: [-173.22638715908789, -173.23548518629613, -182.3518515681761, -168.56215348235071, -105.05082070897839, -199.30397684524462, -274.93919366203522, -167.94297274096607, -173.29530930175983, -151.08929590405918]\n",
      "Mean over last 125 episodes: -180.462359536\n",
      "Episode lengths, last 10: [68, 54, 72, 58, 68, 74, 84, 73, 75, 62]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 28036\n",
      "Episode rewards, last 10: [-124.31610196263193, -136.5789586330016, -173.29128822756516, -163.45591459948767, -126.02504831202529, -145.35762379750702, -164.21388710103506, -227.6301507381298, -120.85654402093618, -168.77827110173865]\n",
      "Mean over last 125 episodes: -183.219146101\n",
      "Episode lengths, last 10: [66, 66, 57, 85, 61, 97, 55, 83, 59, 63]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 37946\n",
      "Episode rewards, last 10: [-180.18863177888119, -246.86357421473735, -181.25048126719295, -152.00987875854884, -152.39342381030582, -152.65624180376562, -130.89783236008608, -146.84103729745976, -175.05564065267191, -126.72802142731226]\n",
      "Mean over last 125 episodes: -177.248318226\n",
      "Episode lengths, last 10: [88, 100, 99, 58, 54, 81, 54, 64, 64, 95]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 47750\n",
      "Episode rewards, last 10: [-162.5472128335656, -315.10124981177569, -156.49287971776926, -212.74424131492918, -136.51149373539215, -174.08323084523781, -141.66826400057457, -200.66415425545927, -195.37447783083704, -148.52821237985708]\n",
      "Mean over last 125 episodes: -178.652925703\n",
      "Episode lengths, last 10: [56, 103, 62, 83, 62, 90, 57, 69, 103, 83]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 57020\n",
      "Episode rewards, last 10: [-194.22204143083943, -169.78044371165896, -208.3129377679283, -245.53309847484104, -211.17014573110177, -202.78473947322919, -129.48711202988528, -196.46642870783612, -165.6617840635129, -237.88900353918501]\n",
      "Mean over last 125 episodes: -169.805340287\n",
      "Episode lengths, last 10: [74, 54, 84, 75, 66, 58, 99, 88, 76, 88]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 66659\n",
      "Episode rewards, last 10: [-189.38450378983077, -165.50626147384534, -195.59775334370917, -65.226022891056346, -227.16179981752643, -172.61404429382213, -171.16013221874977, -268.87401720523144, -246.94176268026655, -168.8796139344787]\n",
      "Mean over last 125 episodes: -175.934034974\n",
      "Episode lengths, last 10: [66, 58, 98, 84, 69, 65, 67, 87, 79, 91]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 75971\n",
      "Episode rewards, last 10: [-173.96660522474258, -154.63809389385503, -222.92249024678105, -212.81649347359735, -164.00609330327978, -141.44032567988302, -240.6865408468023, -215.16826140661578, -203.19925035701849, -127.87968339383735]\n",
      "Mean over last 125 episodes: -176.681513524\n",
      "Episode lengths, last 10: [67, 66, 91, 64, 64, 61, 68, 78, 65, 66]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 85613\n",
      "Episode rewards, last 10: [-170.53299130436159, -366.25345208105932, -253.38235905604859, -152.40036145690016, -130.1544682852724, -142.09265426212085, -158.08293103157382, -190.54836197873766, -190.82209719300087, -226.4681508378867]\n",
      "Mean over last 125 episodes: -175.441928486\n",
      "Episode lengths, last 10: [57, 104, 117, 64, 68, 66, 105, 60, 92, 86]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 94774\n",
      "Episode rewards, last 10: [-146.29485309745678, -187.7445542713605, -195.69218032248745, -230.05895645920867, -39.268218091390992, -158.11564957096556, -147.34158891473504, -205.55418303707185, -191.62869047107728, -182.8610489769024]\n",
      "Mean over last 125 episodes: -170.512282942\n",
      "Episode lengths, last 10: [50, 79, 81, 82, 78, 62, 57, 64, 72, 62]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8349\n",
      "Episode rewards, last 10: [-223.9865689513388, -240.2656842045771, -192.11660113027025, -158.23915418265355, -166.55305806945503, -183.80140967900971, -189.27734193241932, -164.22358308733354, -212.59481513912681, -216.95125086032505]\n",
      "Mean over last 125 episodes: -222.072168899\n",
      "Episode lengths, last 10: [83, 60, 77, 69, 72, 86, 78, 58, 50, 69]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 16914\n",
      "Episode rewards, last 10: [-343.59647001394853, -220.22569789689612, -182.78455401825693, -15.871253383773364, -188.38201665059719, -201.41442483686586, -154.16174182308364, -159.35429761961723, -328.83530948808749, -223.45805367931575]\n",
      "Mean over last 125 episodes: -206.059306099\n",
      "Episode lengths, last 10: [83, 67, 57, 73, 65, 68, 59, 62, 90, 67]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 25671\n",
      "Episode rewards, last 10: [-186.97257205544369, -234.49850448188425, -178.84519050970795, -69.237695246381648, -170.70372714018552, -223.9165997787523, -204.74820594264116, -170.98124918011121, -190.27862430416823, -243.47187024417107]\n",
      "Mean over last 125 episodes: -205.438332179\n",
      "Episode lengths, last 10: [67, 62, 75, 88, 59, 79, 78, 53, 88, 78]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 34253\n",
      "Episode rewards, last 10: [-44.32729793403071, -221.60002814331563, -161.88253755809296, -28.755709963340223, -207.84203298578507, -47.23241774812378, -457.80830672578151, -153.99177836652137, -207.14354174097889, -250.86999078613744]\n",
      "Mean over last 125 episodes: -200.624276452\n",
      "Episode lengths, last 10: [94, 55, 59, 68, 78, 80, 89, 81, 85, 66]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 42964\n",
      "Episode rewards, last 10: [-167.16150664020248, -28.143929611260162, -202.61864783626373, -170.39813943174889, -180.51023748544898, -174.29798391659702, -214.47955953297395, -329.80122245245161, -192.34036668460374, -199.29294303377148]\n",
      "Mean over last 125 episodes: -204.091951621\n",
      "Episode lengths, last 10: [54, 71, 60, 52, 79, 64, 65, 80, 73, 60]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 51743\n",
      "Episode rewards, last 10: [-193.90308135472296, -129.84061566702539, -273.39742308318171, -145.44471346685901, -248.81824983687903, -196.79177691651634, -203.98540199794644, -253.69957544965206, -219.30955955439185, -170.40031202621242]\n",
      "Mean over last 125 episodes: -204.76620293\n",
      "Episode lengths, last 10: [55, 51, 63, 54, 91, 92, 55, 70, 67, 78]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 60684\n",
      "Episode rewards, last 10: [-196.39269635284307, -189.78142794926637, -264.42953833626046, -177.96874805268692, -168.94397190271883, -161.82503633300306, -405.72537638780267, -174.15176276725231, -168.61166039802316, -178.16117460343244]\n",
      "Mean over last 125 episodes: -203.39559589\n",
      "Episode lengths, last 10: [50, 57, 86, 62, 71, 85, 91, 53, 60, 79]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 69160\n",
      "Episode rewards, last 10: [-140.99422838618119, -232.23673865303942, -272.62736022406432, -180.5795905100253, -174.45854091513917, -237.14774734449699, -250.83993971980726, -200.51483143299811, -185.54064021458169, -185.6957091376504]\n",
      "Mean over last 125 episodes: -201.092508193\n",
      "Episode lengths, last 10: [51, 65, 59, 71, 56, 60, 66, 57, 64, 50]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 78085\n",
      "Episode rewards, last 10: [-230.90860164727533, -249.33631517680729, -185.02076601525505, -168.42435577890001, -83.858187733112771, -244.59620547659799, -199.57071935547157, -187.06665908421323, -150.60152744845956, -234.57773511495986]\n",
      "Mean over last 125 episodes: -219.456133147\n",
      "Episode lengths, last 10: [70, 95, 57, 80, 60, 79, 78, 52, 51, 64]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 86717\n",
      "Episode rewards, last 10: [-141.28374788536379, -301.51926166702259, -243.66763417675367, -194.83748516128261, -177.74535635892585, -195.41986921782433, -198.86546364812452, -233.87915843690723, -183.50706043521672, -220.96446479129418]\n",
      "Mean over last 125 episodes: -207.412859127\n",
      "Episode lengths, last 10: [50, 87, 70, 74, 62, 88, 55, 73, 51, 65]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8284\n",
      "Episode rewards, last 10: [-188.42616832754968, -181.1437035301085, -263.7359155888787, -159.82136045821886, -279.28630742550069, -175.05394221696568, -201.02134090215301, -195.29972407156396, -172.27499182216712, -204.960134768628]\n",
      "Mean over last 125 episodes: -207.548666394\n",
      "Episode lengths, last 10: [54, 67, 68, 66, 81, 59, 73, 85, 56, 65]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 17197\n",
      "Episode rewards, last 10: [-246.73788018500903, -254.93857598005772, -204.5247206286908, -210.46758606086007, -215.18640228856373, -273.83819030725181, -235.43723748643922, -259.59995894637655, -191.3841159033783, -196.04344747998999]\n",
      "Mean over last 125 episodes: -213.426508224\n",
      "Episode lengths, last 10: [56, 78, 72, 52, 62, 63, 65, 76, 85, 66]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 26077\n",
      "Episode rewards, last 10: [-192.76452306112273, -201.44904461354997, -33.310031285290421, -226.3936792133845, -161.2953233713074, -161.80230566171963, -219.88800199029973, -207.61803236878046, -154.72013616575782, -196.12651190585879]\n",
      "Mean over last 125 episodes: -194.649143197\n",
      "Episode lengths, last 10: [71, 84, 59, 74, 61, 76, 60, 58, 83, 58]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 35083\n",
      "Episode rewards, last 10: [-212.27998292266474, -168.73151724538252, -151.05234922888823, -180.24100648025711, -188.42798791262774, -224.46972021748928, -183.79977296946328, -162.86728634741041, -146.70485932236187, -169.71802611084854]\n",
      "Mean over last 125 episodes: -183.724228313\n",
      "Episode lengths, last 10: [71, 50, 63, 64, 67, 76, 75, 80, 51, 59]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 43980\n",
      "Episode rewards, last 10: [-188.79281407680554, -204.01800266553533, -181.02044329953378, -169.80626962020298, -183.77242484729396, -176.50902981730886, -171.78657774635997, -174.15416477787451, -149.10651734888032, -167.26953303114595]\n",
      "Mean over last 125 episodes: -177.330734615\n",
      "Episode lengths, last 10: [63, 76, 65, 72, 77, 47, 70, 58, 54, 68]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 53000\n",
      "Episode rewards, last 10: [-212.47063286542817, -260.65515045930096, -207.42670034862681, -204.796333439764, -180.59072647123952, -169.69614259489535, -168.71068884647335, -196.89452984048998, -199.19722989710442, -161.43920874959338]\n",
      "Mean over last 125 episodes: -181.912993012\n",
      "Episode lengths, last 10: [81, 83, 74, 61, 73, 73, 60, 66, 67, 59]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 61692\n",
      "Episode rewards, last 10: [-148.3541464072604, -252.9456088334843, -180.72810094269738, -166.33485510149683, -136.88613666480884, -190.38030860751223, -184.60051035455206, -161.70202532014466, -166.59690309931952, -204.24101102801217]\n",
      "Mean over last 125 episodes: -178.481553337\n",
      "Episode lengths, last 10: [58, 90, 60, 74, 56, 61, 70, 69, 57, 70]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 70344\n",
      "Episode rewards, last 10: [-173.91908525234464, -191.12268361477325, -214.78596417297089, -180.15662966273848, -176.41737530059874, -152.08182290370331, -178.52764212976729, -261.06827857788551, -193.14275796979356, -165.50370887674597]\n",
      "Mean over last 125 episodes: -174.83689986\n",
      "Episode lengths, last 10: [85, 57, 69, 85, 67, 81, 76, 128, 68, 65]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 78792\n",
      "Episode rewards, last 10: [-406.42903570059696, -189.92694001847872, -162.32212009386294, -144.81321014786943, -192.27331949056645, -171.21298643443879, -181.72934602741915, -169.94964889952723, -202.08400656624002, -161.35056235299595]\n",
      "Mean over last 125 episodes: -179.885940812\n",
      "Episode lengths, last 10: [91, 60, 56, 56, 66, 68, 58, 57, 76, 56]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 87716\n",
      "Episode rewards, last 10: [-187.55891657371566, -177.59376589166578, -200.93898594101859, -171.42707402006928, -199.29003557587902, -167.85663137851941, -181.9008437713305, -172.90558780614131, -168.37982414454768, -168.11320471868612]\n",
      "Mean over last 125 episodes: -172.077606823\n",
      "Episode lengths, last 10: [56, 59, 67, 60, 56, 62, 62, 89, 54, 63]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8930\n",
      "Episode rewards, last 10: [-218.86744697729091, -187.70408386382044, -214.74512334129426, -380.58693612286095, -146.03726137330833, -161.32396695110967, -265.43131083374806, -145.74058272486624, -193.76434362307418, -167.00376476810661]\n",
      "Mean over last 125 episodes: -225.384797002\n",
      "Episode lengths, last 10: [61, 50, 53, 94, 52, 80, 87, 55, 89, 88]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 17824\n",
      "Episode rewards, last 10: [-189.39597230574287, -186.48219414860503, -185.58808171887424, -198.02169827984443, -167.16852326118945, -161.5417168517402, -170.60011250168253, -147.27002181879001, -158.55584030803175, -160.61973192397426]\n",
      "Mean over last 125 episodes: -194.799809722\n",
      "Episode lengths, last 10: [73, 69, 59, 65, 54, 60, 62, 68, 53, 72]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 26837\n",
      "Episode rewards, last 10: [-180.00538619428272, -93.757070621322555, -440.00394123042668, -22.713017719816882, -190.70880446321621, -171.87620874921029, -290.29867946447894, -47.796867502777388, -286.50233318826383, -229.32278191097066]\n",
      "Mean over last 125 episodes: -207.332265807\n",
      "Episode lengths, last 10: [57, 91, 89, 99, 57, 63, 76, 84, 77, 66]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 35610\n",
      "Episode rewards, last 10: [-327.80829282143395, -161.75849936606681, -192.16796847715116, -161.24524011854544, -278.3515754473641, -205.29278590237217, -185.22915474880963, -189.36411674425449, -144.65309913160507, -140.13568328003475]\n",
      "Mean over last 125 episodes: -206.584972786\n",
      "Episode lengths, last 10: [89, 61, 84, 54, 64, 70, 78, 89, 68, 54]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 44402\n",
      "Episode rewards, last 10: [-204.97843314656075, -162.75053612070047, -203.35720440353552, -181.67847759778073, -238.6501551649763, -8.423781805381708, -303.52464258056455, -287.24556262377382, -192.77401331131583, -195.38753194917203]\n",
      "Mean over last 125 episodes: -209.570593463\n",
      "Episode lengths, last 10: [65, 59, 53, 69, 78, 91, 74, 65, 71, 73]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 53032\n",
      "Episode rewards, last 10: [-168.24581952479122, -157.51069860340252, -211.9427754181167, -137.44744949639264, -149.70109590942431, -43.556003589207492, -170.46420409359791, -176.22284362304356, -245.36894606369324, -160.53237268302018]\n",
      "Mean over last 125 episodes: -206.444572433\n",
      "Episode lengths, last 10: [52, 53, 51, 51, 56, 73, 85, 66, 87, 51]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 61889\n",
      "Episode rewards, last 10: [-201.10295339937252, -166.08270398358187, -232.0509040358169, -155.72163290706266, -211.26247322090532, -184.1808531146645, -152.58653834466855, -199.76477277864555, -137.59677729226962, -179.76270147710986]\n",
      "Mean over last 125 episodes: -192.793488429\n",
      "Episode lengths, last 10: [78, 51, 86, 75, 86, 74, 53, 88, 53, 54]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 70729\n",
      "Episode rewards, last 10: [-151.69364805425536, -194.77552015667993, -129.11233625603691, -161.58386419479075, -225.00281729772087, -187.62425306999057, -216.07455484654221, -183.37289259431969, -246.21799644348741, -202.56587100429894]\n",
      "Mean over last 125 episodes: -178.6560971\n",
      "Episode lengths, last 10: [50, 71, 53, 52, 66, 52, 64, 70, 88, 59]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 79311\n",
      "Episode rewards, last 10: [-51.380617938682775, -158.26941219287312, -178.5094335245482, -142.10938043140828, -154.65767523334759, -182.28120401606719, -197.29283723689974, -210.60213471619451, -174.07946285104111, -183.78725190718632]\n",
      "Mean over last 125 episodes: -178.506875197\n",
      "Episode lengths, last 10: [64, 55, 87, 55, 53, 50, 78, 89, 67, 68]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 87989\n",
      "Episode rewards, last 10: [-170.56120887528664, -204.56518117829563, -186.92056039846943, -170.98928151619316, -192.74816006675087, -170.64344509036937, -204.40369441633018, -179.34134067370343, -170.32097314117905, -208.70884340903001]\n",
      "Mean over last 125 episodes: -182.161009276\n",
      "Episode lengths, last 10: [77, 70, 78, 55, 63, 69, 67, 71, 56, 67]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8593\n",
      "Episode rewards, last 10: [-208.06019421823285, -242.40991734472004, -229.45062489709301, -67.580337594996706, -214.86663238313693, -253.04661930080186, -155.49391705297128, -135.81118855026926, -195.33165319983542, -204.01403797925974]\n",
      "Mean over last 125 episodes: -218.392093273\n",
      "Episode lengths, last 10: [66, 71, 65, 82, 74, 82, 60, 51, 57, 75]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 17199\n",
      "Episode rewards, last 10: [-194.12969541755899, -191.6174663591207, -181.04960559008262, -182.26435016318638, -181.49074460500319, -196.48476557049432, -251.59918892124057, -151.7517770908745, -138.1691781070337, -183.4796319178121]\n",
      "Mean over last 125 episodes: -198.636146324\n",
      "Episode lengths, last 10: [58, 55, 51, 55, 55, 90, 57, 56, 52, 81]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 26728\n",
      "Episode rewards, last 10: [-116.94800453993574, -203.73168011685522, -65.717759358129214, -148.99574525343795, -136.93663912335489, -167.34239850702619, -146.76742728602952, -194.72384031594663, -144.1784212546018, -178.33367365916979]\n",
      "Mean over last 125 episodes: -182.161517163\n",
      "Episode lengths, last 10: [89, 64, 79, 66, 54, 87, 103, 76, 57, 67]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 35972\n",
      "Episode rewards, last 10: [-180.86439388672343, -147.64951820049316, -70.845737832397262, -167.47486879441266, -180.54223467948884, -198.65116812555721, -140.16844883809318, -173.96950524686784, -133.04451195618731, -171.62167659943117]\n",
      "Mean over last 125 episodes: -177.044792133\n",
      "Episode lengths, last 10: [77, 83, 84, 71, 63, 75, 49, 58, 51, 60]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 44930\n",
      "Episode rewards, last 10: [-180.82274174152278, -205.31686248656931, -157.14990022851623, -185.62031193719983, -225.16684824510077, -186.44411067014306, -193.69899056907894, -161.06671028659451, -166.12205033701531, -242.07516325394295]\n",
      "Mean over last 125 episodes: -166.280055483\n",
      "Episode lengths, last 10: [52, 78, 86, 54, 65, 69, 79, 55, 63, 65]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 53581\n",
      "Episode rewards, last 10: [-134.78599576436588, -226.8846261764244, -223.86981933618264, -213.8843473140991, -153.98835158070955, -243.05356718895871, -162.15471948048733, -214.53489739772138, -164.08010572602097, -166.76882327000365]\n",
      "Mean over last 125 episodes: -180.926411527\n",
      "Episode lengths, last 10: [84, 77, 79, 66, 62, 74, 70, 77, 70, 52]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 62407\n",
      "Episode rewards, last 10: [-171.88310300881216, -181.17882457504717, -125.59089467879993, -208.56043366192603, -34.37320794370379, -204.59222046668782, -210.70001798252682, -195.11502535029891, -170.01163589901415, -225.01262735775572]\n",
      "Mean over last 125 episodes: -174.36370278\n",
      "Episode lengths, last 10: [68, 68, 89, 72, 92, 69, 62, 62, 58, 70]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 71217\n",
      "Episode rewards, last 10: [-144.71420734415349, -134.08609009844241, -134.12356203317387, -174.45690382356938, -134.7945256289359, -265.68262232060351, -234.36183951765261, -137.2137696794905, -164.54408657378784, -156.87855604717774]\n",
      "Mean over last 125 episodes: -176.88018123\n",
      "Episode lengths, last 10: [88, 63, 51, 65, 85, 91, 89, 57, 52, 80]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 80293\n",
      "Episode rewards, last 10: [-427.39363371417028, -176.66947151603685, -202.96740050470993, -171.83049113106725, -125.17146393506147, -53.895338090275558, -155.80788905911791, -150.25558326632913, -182.36861474284063, -159.56803494947042]\n",
      "Mean over last 125 episodes: -173.338285829\n",
      "Episode lengths, last 10: [109, 57, 60, 91, 49, 82, 55, 78, 63, 76]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 89149\n",
      "Episode rewards, last 10: [-168.44196422151339, -135.12698298326788, -195.75220563253384, -63.878326838141888, -184.01731422700573, -171.92565088284007, -183.65838141542514, -175.10519078032934, -185.08251300453793, -141.96491460424343]\n",
      "Mean over last 125 episodes: -172.552625229\n",
      "Episode lengths, last 10: [55, 56, 85, 62, 64, 89, 70, 57, 53, 88]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 8256\n",
      "Episode rewards, last 10: [-169.55601079213662, -206.72146067742682, -205.01919896841406, -211.06965901748981, -177.6623590417741, -204.5828339231507, -76.671623884055009, -197.86672456540489, -244.28051951914551, -170.23249971958205]\n",
      "Mean over last 125 episodes: -268.030092975\n",
      "Episode lengths, last 10: [97, 61, 75, 85, 61, 84, 67, 71, 72, 56]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 16978\n",
      "Episode rewards, last 10: [-151.7119897819525, -307.16152705679087, -208.93653712464825, -199.9312624749106, -190.40686314535748, -202.21621930247616, -307.82105581918393, -162.5932903656784, -181.21734448837631, -231.52809037306372]\n",
      "Mean over last 125 episodes: -184.858763335\n",
      "Episode lengths, last 10: [57, 92, 78, 79, 63, 62, 79, 55, 65, 74]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 25623\n",
      "Episode rewards, last 10: [-169.93162170607511, -191.6519234529743, -200.3902130578295, -220.30540176010607, -161.0703407866547, -179.10257932130472, -175.20595148861909, -173.10946509084872, -193.76009238962246, -158.89399155596357]\n",
      "Mean over last 125 episodes: -176.665473105\n",
      "Episode lengths, last 10: [71, 57, 69, 84, 60, 62, 52, 51, 64, 65]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 34254\n",
      "Episode rewards, last 10: [-200.48756762470623, -196.03769799021973, -194.78112922088647, -152.74500724150812, -202.35046635349048, -175.82484154580703, -188.9932109946291, -197.2679661680969, -168.97016224534542, -156.82057291838328]\n",
      "Mean over last 125 episodes: -179.648703161\n",
      "Episode lengths, last 10: [83, 73, 64, 52, 75, 84, 58, 56, 61, 54]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 42884\n",
      "Episode rewards, last 10: [-168.65058624825045, -199.13634801455765, -141.53133638194072, -205.10279297657164, -17.595418738542051, -150.27012911043349, -185.63900428252185, -181.9318949068751, -198.53379154708449, -157.19963280964001]\n",
      "Mean over last 125 episodes: -183.132810772\n",
      "Episode lengths, last 10: [55, 69, 54, 74, 72, 58, 59, 58, 68, 60]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 51735\n",
      "Episode rewards, last 10: [-206.93130960636006, -158.9733633931142, -204.67052041846054, -195.38994123579889, -198.08800206393357, -210.52323316763602, -167.12786526596861, -175.89755224952137, -199.72201120789816, -174.26812555891644]\n",
      "Mean over last 125 episodes: -186.374380304\n",
      "Episode lengths, last 10: [58, 77, 74, 81, 70, 71, 64, 60, 72, 69]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 60363\n",
      "Episode rewards, last 10: [-214.0175068284114, -213.02099902444229, -195.07784095549229, -174.12517114176947, -188.77567316989976, -189.70998064119911, -161.84782453172622, -147.40678523730151, -204.77161820291249, -237.10412401757293]\n",
      "Mean over last 125 episodes: -181.263995027\n",
      "Episode lengths, last 10: [68, 76, 83, 54, 55, 82, 52, 56, 68, 76]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 69198\n",
      "Episode rewards, last 10: [-179.1533107472564, -153.28699272931428, -166.63152328866914, -171.49732097554588, -163.78160391811372, -180.42316119006659, -160.58541665341582, -155.82864450272643, -179.00285860617782, -170.39715142113408]\n",
      "Mean over last 125 episodes: -176.598052649\n",
      "Episode lengths, last 10: [81, 57, 71, 54, 62, 79, 86, 58, 84, 60]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 77855\n",
      "Episode rewards, last 10: [-152.33076934887757, -180.12897800992431, -167.85781469275474, -196.20541432227512, -198.54799934560549, -152.95674168351064, -172.73799532650381, -192.76136797432807, -158.7706198896023, -168.70551219427921]\n",
      "Mean over last 125 episodes: -176.274135981\n",
      "Episode lengths, last 10: [63, 66, 72, 66, 60, 53, 90, 63, 53, 76]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 86552\n",
      "Episode rewards, last 10: [-41.530075947439059, -139.84730672468271, -162.99153458804037, -184.67162060387611, -190.90514437611449, -171.13818795807182, -148.33653886773479, -203.82125171115803, -210.8478356432139, -184.10015289647123]\n",
      "Mean over last 125 episodes: -174.170343528\n",
      "Episode lengths, last 10: [93, 51, 75, 77, 80, 58, 56, 71, 76, 53]\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 124 ================================================\n",
      "Total steps: 9240\n",
      "Episode rewards, last 10: [-83.257477241853962, -16.47881138085738, -407.64649251280105, -112.71918882090991, -83.143241702779122, -117.87289804828714, -378.86711340911711, -414.08128852408743, -414.17830868614953, -171.99171596310546]\n",
      "Mean over last 125 episodes: -213.318265218\n",
      "Episode lengths, last 10: [62, 84, 99, 75, 86, 81, 79, 66, 112, 88]\n",
      "======================================================================\n",
      "========= Episode 249 ================================================\n",
      "Total steps: 20704\n",
      "Episode rewards, last 10: [-209.54277370791982, -142.70602007826926, -149.35785644751439, -233.44434081989442, -196.03533589345878, -139.970875127917, -148.14010749548976, -176.24086284531788, -200.02484765055641, -327.51518998352498]\n",
      "Mean over last 125 episodes: -243.692891798\n",
      "Episode lengths, last 10: [54, 95, 110, 81, 107, 107, 120, 63, 55, 85]\n",
      "======================================================================\n",
      "========= Episode 374 ================================================\n",
      "Total steps: 30570\n",
      "Episode rewards, last 10: [-530.22472530236496, -191.77106482397969, -212.5033804579206, -210.09423880440511, -183.39047279549402, -238.27886889986519, -194.06478779589452, -217.4676091692964, -201.36416781442247, -160.9238771294626]\n",
      "Mean over last 125 episodes: -208.208525976\n",
      "Episode lengths, last 10: [102, 85, 72, 53, 67, 64, 53, 70, 84, 50]\n",
      "======================================================================\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 39232\n",
      "Episode rewards, last 10: [-168.48945261958252, -206.1908842378441, -188.28279704770162, -242.33277193063316, -145.36662304896572, -180.20183801697439, -210.70254334568881, -50.524371187985267, -456.82777251060537, -169.780055856725]\n",
      "Mean over last 125 episodes: -200.948725963\n",
      "Episode lengths, last 10: [52, 51, 68, 63, 56, 67, 58, 80, 85, 56]\n",
      "======================================================================\n",
      "========= Episode 624 ================================================\n",
      "Total steps: 48018\n",
      "Episode rewards, last 10: [-148.9505496722673, -149.49413707958541, -359.3450320753758, -246.32504176116521, -261.89923364293577, -253.63833633957785, -266.94096153669318, -185.17341519870311, -297.1669252737849, -238.87373262717023]\n",
      "Mean over last 125 episodes: -210.820011825\n",
      "Episode lengths, last 10: [55, 59, 90, 68, 81, 85, 77, 64, 91, 64]\n",
      "======================================================================\n",
      "========= Episode 749 ================================================\n",
      "Total steps: 56722\n",
      "Episode rewards, last 10: [-159.01549080580423, -227.8873562229717, -173.70083040206546, -185.89179861336339, -128.24025998882976, -173.28043270308825, -261.71604996361907, -157.79060584095345, -179.06612988563361, -178.33082455680932]\n",
      "Mean over last 125 episodes: -202.445542558\n",
      "Episode lengths, last 10: [68, 64, 53, 78, 93, 61, 63, 60, 67, 88]\n",
      "======================================================================\n",
      "========= Episode 874 ================================================\n",
      "Total steps: 65734\n",
      "Episode rewards, last 10: [-172.70611631356309, -327.6952012172419, -201.72530639884445, -184.87206435207554, -209.31563285305867, -207.0625711182382, -216.81244653588305, -182.93350069542228, -164.54850610126843, -167.8007040334063]\n",
      "Mean over last 125 episodes: -200.342453955\n",
      "Episode lengths, last 10: [59, 82, 60, 63, 89, 54, 70, 80, 53, 50]\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 74547\n",
      "Episode rewards, last 10: [-165.00133043294386, -176.57419640284272, -176.94599881723332, -182.79784414519517, -200.39459907147824, -198.12647374935702, -163.31695013460288, -195.07001360163051, -198.5584117258847, -194.722852908991]\n",
      "Mean over last 125 episodes: -207.979478307\n",
      "Episode lengths, last 10: [67, 70, 82, 53, 56, 88, 52, 53, 77, 85]\n",
      "======================================================================\n",
      "========= Episode 1124 ================================================\n",
      "Total steps: 83058\n",
      "Episode rewards, last 10: [-195.70853843056295, -165.67032690522734, -154.87044440832767, -168.74303858920464, -159.22333670937365, -160.23659532982128, -188.54580661453906, -197.75848414056412, -162.51398026464665, -160.42908857249458]\n",
      "Mean over last 125 episodes: -197.305691068\n",
      "Episode lengths, last 10: [78, 77, 68, 60, 74, 53, 80, 62, 74, 84]\n",
      "=======================================================================\n",
      "========= Episode 1249 ================================================\n",
      "Total steps: 91747\n",
      "Episode rewards, last 10: [-199.31646878319708, -228.12844236333859, -244.7398860343454, -197.98204234230124, -287.19250051670355, -190.64050416315945, -205.94182530482325, -334.50734511305023, -181.39184959699176, -158.27387790724282]\n",
      "Mean over last 125 episodes: -192.271585588\n",
      "Episode lengths, last 10: [62, 68, 60, 81, 70, 88, 85, 69, 76, 57]\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = produce_experiment(DDQL, ddql_bayes_params, ddql_train_params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(ddql_res, window = 100, std_coef = 0.2, results_over = 1000):\n",
    "    res_lists = [k.rList for k in ddql_res]\n",
    "    res_lists = np.array(res_lists)\n",
    "    pd.DataFrame(data = res_lists)\n",
    "    mean = res_lists.mean(axis = 0)\n",
    "    std = res_lists.std(axis = 0)\n",
    "    rol_mean = np.nan_to_num(pd.Series(mean).rolling(window = window).mean())[window:]\n",
    "    rol_std = np.nan_to_num(pd.Series(std).rolling(window = window).mean())[window:]\n",
    "    plt.figure()\n",
    "    index = np.arange(window, len(rol_mean) + window)\n",
    "    plt.plot(index, rol_mean)\n",
    "    plt.fill_between(index, rol_mean-std_coef*rol_std, rol_mean+std_coef*rol_std, color='b', alpha=0.1)\n",
    "    return max(rol_mean[window:results_over]), rol_mean[window:results_over].mean(), pd.DataFrame(data = res_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKgaKoigK8P8Bx5rNqtVoc38AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1281a3050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bayes_max, bayes_auc, bayes_df = plot_results(res, 100, 0.2)\n",
    "plt.savefig(\"bayes_ll.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-183.174142428 -190.270384792\n"
     ]
    }
   ],
   "source": [
    "print bayes_max, bayes_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_df.to_csv(\"bayes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "1bdc437bb00f46d697474bd3e3ce8a3d": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "4312e3aa0a33459691aecb3d20a26d15": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "48f7158a72884a959651f58b08408cf0": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "51ec9c62a4394545b3972f0b74a00345": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "525068185be04e06ab887a941c982e81": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "6e9e864f406d4232a8b6d7477557600d": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "81f5a2a134cf4ce483593d5315528ae4": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "83f87ae720e24985b4ac8f27e21fc4f7": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "846db60f5f2244e7a0ba78fa73c34064": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "851c978906e7420aab87cc9647087bdb": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "88c0742b09044efbb484f662b8f3eff4": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "a204408ed61d4be89dd5da55a31e67ce": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "d373f052314f42cca55ab393df2cb7eb": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "d78f87ddb8d042d0830bda5174142220": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "d85827260aef4842b1a5292a96efdc61": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "e22369888fd243ab8ba5314438b6725d": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "f90fca8dd50648108965ee51f89ecc8e": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
