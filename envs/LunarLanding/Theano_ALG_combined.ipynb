{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ATARI_wrapper():\n",
    "    def __init__(self, gamename = \"Enduro-v0\"):\n",
    "        self.state_size = (105, 80)\n",
    "        self.game_title = gamename\n",
    "        self.actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        grayimage = np.mean(state, axis = 2)\n",
    "        downscale = self.downscale2x(grayimage)\n",
    "        norm = (downscale - 128.0) / 128.0\n",
    "        return norm\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3\n",
    "    \n",
    "    def downscale2x(self, image):\n",
    "        image00 = image[0::2, 0::2]\n",
    "        image01 = image[0::2, 1::2]\n",
    "        image10 = image[1::2, 0::2]\n",
    "        image11 = image[1::2, 1::2]\n",
    "        return (image00 + image01 + image10 + image11) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, \n",
    "                 encodeunits = 10, encodenoise = 0.1, aelosscoef = 0.1, regaelosscoef = 2.0, \n",
    "                 explosscoef = 0.1, h = 50, alpha = 25):\n",
    "        self.input_var = T.tensor4('statebatch')\n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.targetE = T.fvector('targetE')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.newstates = T.tensor4(\"newstatebatch\")\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.aelosscoef = aelosscoef\n",
    "        self.regaelosscoef = regaelosscoef\n",
    "        self.explosscoef = explosscoef\n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape, encodeunits, encodenoise)\n",
    "        self.build_AVQ(grad_clipping, lr, h, alpha)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape, encodeunits, encodenoise):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.encode = lasagne.layers.DenseLayer(self.outlayer, encodeunits, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "        self.encodenoise = lasagne.layers.GaussianNoiseLayer(self.encode, sigma = encodenoise)\n",
    "        self.le3 = lasagne.layers.DenseLayer(self.encodenoise, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le2 = lasagne.layers.DenseLayer(self.le3, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le1 = lasagne.layers.DenseLayer(self.le2, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le0 = lasagne.layers.DenseLayer(self.le1, channels_number * image_shape[0] * image_shape[1])\n",
    "        self.l_aeout = lasagne.layers.ReshapeLayer(self.le0, shape=(-1, channels_number, image_shape[0], image_shape[1]))\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr, h, alpha):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        self.l_varq = lasagne.layers.DenseLayer(self.outlayer, self.n_actions, nonlinearity=lasagne.nonlinearities.softplus)\n",
    "        self.l_exp_out = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        \n",
    "        self.advantage, self.value, self.ae_out, self.enc, self.varq, self.exp_out = \\\n",
    "            lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_aeout, self.encode, self.l_varq, self.l_exp_out])\n",
    "        \n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Qout = self.advantage + self.value - self.average_advantage\n",
    "#-----\n",
    "        self.Q = self.advantage\n",
    "        self.predict = T.argmax(self.Q, axis = 1)\n",
    "        \n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, self.n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.E = self.exp_out\n",
    "        self.Epredict = T.argmax(self.E, axis = 1)\n",
    "        \n",
    "        self.E0 = T.sum(self.E * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.Q0 = T.sum(self.Q * self.actions_onehot, axis = 1)\n",
    "        self.varQ0 = T.sum(self.varq * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.Q1 = self.Q0 + (self.targetQ - self.Q0) / (h + 1)\n",
    "        self.varQ1 = (h * (alpha - 1)) / ((h + 1) * (alpha - 0.5)) * \\\n",
    "                     (self.varQ0 + T.sqr(self.targetQ - self.Q0) / (2 * (h+1) * (alpha - 1)))\n",
    "        \n",
    "        self.exp_error = T.mean(T.sqr(self.targetE - self.E0))\n",
    "        self.td_error = T.mean(T.sqr(self.Q1 - self.Q0))\n",
    "        self.var_error = T.mean(T.sqr(self.varQ1 - self.varQ0))\n",
    "        self.regae_error = 0.25 - T.mean(T.sqr(self.enc - 0.5))\n",
    "        self.ae_error = T.mean(T.sqr(self.ae_out - self.input_var))\n",
    "        \n",
    "        self.loss = self.ae_error * self.aelosscoef + self.regaelosscoef * self.regae_error + self.td_error + self.var_error + self.explosscoef * self.exp_error\n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "\n",
    "        enc_layers = [self.l2, self.l3, self.outlayer, self.encode, self.le3, self.le2, self.le1, self.le0]\n",
    "        enc_params = [l.W for l in enc_layers] + [l.b for l in enc_layers]\n",
    "        self.enc_grads = T.grad(self.ae_error, enc_params)\n",
    "        self.enc_scaled_grads = lasagne.updates.total_norm_constraint(self.enc_grads, grad_clipping)\n",
    "        self.enc_updates = lasagne.updates.adam(self.enc_scaled_grads, enc_params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Q)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.full_train_fn = theano.function([self.input_var, self.targetQ, self.targetE, self.actions], [self.ae_error, self.td_error], updates = self.updates)\n",
    "        self.train_encoder = theano.function([self.input_var], self.ae_error, updates = self.enc_updates)\n",
    "        self.get_encode = theano.function([self.input_var], [self.enc, self.regae_error])\n",
    "        self.var_fn = theano.function([self.input_var], self.varq)\n",
    "        \n",
    "        self.Eout_fn = theano.function([self.input_var], self.E)\n",
    "        self.Eactionpred_fn = theano.function([self.input_var], self.Epredict)\n",
    "        \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values(self.l_advantage, values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.buffer))\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class combined_agent:\n",
    "    def __init__(self, n_actions, actionpred_fn, Eactionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "        self.Eactionpred_fn = Eactionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = self.Eactionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.encode_unitnum = lparams[\"encode_unitnum\"]\n",
    "        self.encode_noise = lparams[\"encode_noise\"]\n",
    "        self.encode_reward_multiplier = lparams[\"encode_reward_multiplier\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.egamma = lparams[\"egamma\"]\n",
    "        self.aelosscoef = lparams[\"aelosscoef\"]\n",
    "        self.regaelosscoef = lparams[\"regaelosscoef\"]\n",
    "        self.explosscoef = lparams[\"explosscoef\"]\n",
    "        self.h = lparams[\"lambda\"]\n",
    "        self.alpha = lparams[\"alpha\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "\n",
    "        self.env = env\n",
    "        \n",
    "        AVQ_params = [self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, \n",
    "                      self.lr, self.encode_unitnum, self.encode_noise, self.aelosscoef, self.regaelosscoef, \n",
    "                      self.explosscoef, self.h, self.alpha]\n",
    "        self.mainQN = AVQ_nn(*AVQ_params)\n",
    "        self.targetQN = AVQ_nn(*AVQ_params)\n",
    "\n",
    "        self.lList = []\n",
    "        self.rList = []\n",
    "        self.aeList = []\n",
    "        self.encode_counts = defaultdict(int)\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            agent = egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            agent = boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        elif agent == \"combined\":\n",
    "            agent = combined_agent(self.env.n_actions, self.mainQN.actionpred_fn, self.mainQN.Eactionpred_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "            \n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "        \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "            episode_aeerrors = np.array([])\n",
    "            \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                experience = np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5])\n",
    "                self.experience_storage.add(experience)\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        \n",
    "                        encodes, reg_errors = self.mainQN.get_encode(old_state_batch)\n",
    "                        encodes = [\"\".join([str(k) for k in encode]) for encode in (encodes > 0.5).astype(int)]\n",
    "                        for encode in encodes:\n",
    "                            self.encode_counts[encode] += 1\n",
    "                        encode_rewards = self.encode_reward_multiplier / np.sqrt(np.array([self.encode_counts[encode] for encode in encodes]))\n",
    "                        \n",
    "                        E1 = self.mainQN.Eactionpred_fn(new_state_batch)\n",
    "                        E2 = self.targetQN.Eout_fn(new_state_batch)\n",
    "                        doubleE = E2[range(self.batch_size), E1]\n",
    "                        var1 = self.mainQN.var_fn(old_state_batch)\n",
    "                        doublevar = var1[range(self.batch_size), E1]\n",
    "                        exp_rewards_vector = doublevar + 0.01 * encode_rewards\n",
    "                        \n",
    "                        targetE = (exp_rewards_vector + (self.egamma * doubleE * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)\n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        doubleQ = Q2[range(self.batch_size), Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        aeerror, tderror = self.mainQN.full_train_fn(old_state_batch, targetQ, targetE, action_vector)\n",
    "                        episode_aeerrors = np.append(episode_aeerrors, aeerror)\n",
    "                else:\n",
    "                    train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                    old_state_batch = np.stack(train_batch[:,0])\n",
    "                    new_state_batch = np.stack(train_batch[:,3])\n",
    "                    action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                    aeerror1 = self.mainQN.train_encoder(old_state_batch)\n",
    "                    aeerror2 = self.mainQN.train_encoder(new_state_batch)\n",
    "                    episode_aeerrors = np.append(episode_aeerrors, (aeerror1 + aeerror2)/2)\n",
    "                    \n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            total_aeerror = np.mean(episode_aeerrors)\n",
    "            self.lList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            self.aeList.append(total_aeerror)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.lList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"AEerror, mean over last 10:\", str(np.mean(self.aeList[-10:]))]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.lList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-15 20:57:09,170] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "ll_env = LunarLanding_wrapper()\n",
    "#cp_env = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 3,\n",
    "           \"encode_unitnum\": 10,\n",
    "           \"encode_noise\": 0.05,\n",
    "           \"encode_reward_multiplier\": 10.0,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"aelosscoef\" : 0.1,\n",
    "           \"regaelosscoef\" : 2.0,\n",
    "           \"explosscoef\" : 0.1,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"egamma\" : 0.01,\n",
    "           \"lambda\": 4,\n",
    "           \"alpha\": 2,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 125,\n",
    "           \"pretrain_steps\" : 500,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.06,\n",
    "                                \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinedagentinfo = {\"agent\" : \"combined\",\n",
    "                     \"params\" : {\"startE\": 0.5,\n",
    "                                 \"endE\" : 0.1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_experiment(ddql, ddql_init_params, ddql_train_params, experiment_num = 5):\n",
    "    ddql_list = [ddql(**ddql_init_params) for k in range(experiment_num)]\n",
    "    \n",
    "    for k in range(experiment_num):\n",
    "        ddql_list[k].train(**ddql_train_params)\n",
    "        \n",
    "    return ddql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql_combined_params = {\"lparams\":lparams, \"env\":ll_env, \"agent\":combinedagentinfo}\n",
    "ddql_egreedy_params = {\"lparams\":lparams, \"env\":ll_env, \"agent\":egreedyagentinfo}\n",
    "\n",
    "ddql_train_params = {\"num_episodes\":1250, \"frame_limit\":200, \"render\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 499 ================================================\n",
      "Total steps: 34662\n",
      "Episode rewards, last 10: [-222.95289733383066, -171.85998432979778, -197.84107840139239, -190.51407165642013, -122.26801588789567, -179.17098381191983, -157.18672506602263, -123.24954437394095, -144.20315536237689, -143.88341384930749]\n",
      "Mean over last 500 episodes: -271.919246031\n",
      "Episode lengths, last 10: [67, 78, 99, 69, 92, 81, 58, 97, 95, 89]\n",
      "AEerror, mean over last 10: 0.465327530893\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 72931\n",
      "Episode rewards, last 10: [-184.38393874027813, -205.55096416410498, -145.6294956834285, -239.34368808406745, -186.01719372680557, -214.69560664824752, -220.67263246639567, -205.30533139082613, -126.25284497540883, -152.55186133905283]\n",
      "Mean over last 500 episodes: -217.383480529\n",
      "Episode lengths, last 10: [60, 65, 61, 70, 71, 63, 87, 72, 57, 61]\n",
      "AEerror, mean over last 10: 0.528660151214\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 108520\n",
      "Episode rewards, last 10: [-168.87205909304993, -172.26845714342926, -140.88385729108035, -200.60993280736335, -156.251829601662, -186.35962422867073, -182.71335482466313, -183.6867846970475, -165.96858676596528, -80.273752519059457]\n",
      "Mean over last 500 episodes: -180.697413242\n",
      "Episode lengths, last 10: [50, 76, 48, 67, 83, 71, 55, 57, 54, 106]\n",
      "AEerror, mean over last 10: 0.438385687163\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 144064\n",
      "Episode rewards, last 10: [-162.98235266040189, -189.70275618868266, -83.051714808068539, -160.15142615232719, -182.48165480231609, -192.56082500653733, -167.74501508816505, -540.9980652283482, -183.86952791189603, -197.9750248474916]\n",
      "Mean over last 500 episodes: -182.586669791\n",
      "Episode lengths, last 10: [71, 76, 74, 59, 66, 54, 82, 97, 65, 83]\n",
      "AEerror, mean over last 10: 0.327787759835\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 178906\n",
      "Episode rewards, last 10: [-207.59659675292826, -146.91295678072748, -169.37771808236593, -220.99094764254841, -210.15813538934873, -167.85991250553735, -163.84333669892743, -181.18006410453509, -187.8378053271822, -186.45908465519653]\n",
      "Mean over last 500 episodes: -180.458560971\n",
      "Episode lengths, last 10: [83, 49, 74, 76, 86, 68, 59, 63, 69, 52]\n",
      "AEerror, mean over last 10: 0.25180288888\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 32188\n",
      "Episode rewards, last 10: [-139.97434029509023, -321.12418878323388, -176.02569522342196, -214.29618553520666, -204.43413418703932, -189.09949146750392, -172.03430074609403, -205.453230414198, -170.15909451531442, -169.81974128726324]\n",
      "Mean over last 500 episodes: -203.793683899\n",
      "Episode lengths, last 10: [55, 90, 59, 60, 63, 87, 61, 76, 72, 50]\n",
      "AEerror, mean over last 10: 0.680683596254\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 67607\n",
      "Episode rewards, last 10: [-164.74217358488931, -197.51628001854874, -242.7272907161057, -128.02152641461879, -88.995910027673943, -169.05523806671374, -177.6089415365729, -39.739591324158837, -175.01505052777406, -158.16442861547313]\n",
      "Mean over last 500 episodes: -175.473777021\n",
      "Episode lengths, last 10: [81, 61, 84, 52, 84, 79, 77, 79, 60, 70]\n",
      "AEerror, mean over last 10: 0.792152022385\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 102536\n",
      "Episode rewards, last 10: [-171.51752835877147, -195.4925594145472, -160.55246404109064, -176.27398917312135, -157.77144116788173, -61.744189927046079, -132.3768132558466, -186.96597888879427, -159.82603287519007, -152.18837024019899]\n",
      "Mean over last 500 episodes: -178.873001579\n",
      "Episode lengths, last 10: [67, 65, 86, 78, 51, 90, 60, 65, 52, 51]\n",
      "AEerror, mean over last 10: 0.722001906359\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 137844\n",
      "Episode rewards, last 10: [-148.63327660707702, -194.90335947823002, -152.52862340146768, -162.73074389316776, -183.18599721721316, -178.1416114719012, -151.00231191856233, -148.44983343638671, -134.37592191382831, -234.4784397013014]\n",
      "Mean over last 500 episodes: -177.29034313\n",
      "Episode lengths, last 10: [52, 54, 87, 72, 56, 52, 52, 59, 55, 78]\n",
      "AEerror, mean over last 10: 0.702856120644\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 172893\n",
      "Episode rewards, last 10: [-170.69424069905594, -181.81442672077495, -228.43425885153817, -231.961040698418, -185.51276298015131, -245.23736416584998, -187.17855167775286, -190.11480965391686, -164.62385601827123, -190.95180147259649]\n",
      "Mean over last 500 episodes: -176.184620234\n",
      "Episode lengths, last 10: [55, 70, 81, 78, 54, 87, 59, 76, 73, 57]\n",
      "AEerror, mean over last 10: 0.803632224369\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 30390\n",
      "Episode rewards, last 10: [-172.45173020549808, -190.68805371420012, -89.598850229089777, -165.8980828276583, -140.64260932442093, -196.95150581078798, -151.49315603330095, -191.44052644089382, -164.68424546011943, -128.91479232740863]\n",
      "Mean over last 500 episodes: -211.243779172\n",
      "Episode lengths, last 10: [65, 83, 71, 68, 70, 66, 51, 79, 81, 51]\n",
      "AEerror, mean over last 10: 0.206308929137\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 65529\n",
      "Episode rewards, last 10: [-175.6247760473498, -184.64215721903938, -155.88821120398981, -198.15116862032698, -163.0815906350096, -152.31614913505632, -179.6520808172491, -190.87410912843117, -187.27375467856501, -213.21464113609656]\n",
      "Mean over last 500 episodes: -178.619945132\n",
      "Episode lengths, last 10: [54, 56, 64, 62, 86, 58, 60, 89, 61, 64]\n",
      "AEerror, mean over last 10: 0.212773068883\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 100665\n",
      "Episode rewards, last 10: [-172.72909586807759, -173.64974810883672, -206.85350556880343, -256.13391992326575, -194.32350341877239, -140.7538200611678, -211.80461823266211, -179.6030694273847, -196.6755773872398, -171.39526661388288]\n",
      "Mean over last 500 episodes: -181.138743989\n",
      "Episode lengths, last 10: [56, 60, 78, 86, 55, 88, 84, 55, 90, 69]\n",
      "AEerror, mean over last 10: 0.21199602752\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 135460\n",
      "Episode rewards, last 10: [-181.797076272053, -152.67509459479615, -85.61692965391407, -179.98208756339909, -134.61034171209246, -158.92590082790241, -184.39786285123299, -165.73030663311516, -165.12524083364849, -181.94837421482347]\n",
      "Mean over last 500 episodes: -174.309608603\n",
      "Episode lengths, last 10: [86, 49, 89, 92, 53, 52, 73, 54, 61, 57]\n",
      "AEerror, mean over last 10: 0.196801175408\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 170238\n",
      "Episode rewards, last 10: [-147.61977774705409, -171.3718958129339, -195.54715537991584, -155.85233307770488, -137.86633944417071, -259.368453059888, -131.34761225094047, -166.22326079176938, -194.02177543770063, -147.66543647253675]\n",
      "Mean over last 500 episodes: -177.268464216\n",
      "Episode lengths, last 10: [57, 57, 68, 53, 83, 91, 68, 73, 61, 81]\n",
      "AEerror, mean over last 10: 0.210297086634\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 33968\n",
      "Episode rewards, last 10: [-273.04555119076991, -302.84262221948092, -416.66044737337086, -52.72409559120041, -177.33685464642008, -176.65790773470866, -182.14915498822333, -206.70341730720202, -263.18153615487023, -237.90547297296783]\n",
      "Mean over last 500 episodes: -311.066826192\n",
      "Episode lengths, last 10: [92, 65, 61, 71, 58, 55, 88, 59, 63, 66]\n",
      "AEerror, mean over last 10: 0.276079711465\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 69300\n",
      "Episode rewards, last 10: [-174.8868952946475, -47.317193023603451, -191.66965119405921, -199.35614759292233, -364.50675801645059, -182.79838858627014, -155.59146476158327, -187.54592320628595, -187.95622188959783, -25.388845534076808]\n",
      "Mean over last 500 episodes: -187.412905855\n",
      "Episode lengths, last 10: [65, 79, 54, 79, 102, 68, 79, 64, 74, 123]\n",
      "AEerror, mean over last 10: 0.243334409622\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 104207\n",
      "Episode rewards, last 10: [-194.59841981878421, -188.45070842106833, -177.90271696182998, -240.73983978534343, -200.64237412588241, -185.15656248790606, -173.99371014779058, -242.59522523999911, -180.8370168497768, -190.6052730832821]\n",
      "Mean over last 500 episodes: -180.051680534\n",
      "Episode lengths, last 10: [71, 75, 60, 76, 67, 77, 60, 80, 60, 66]\n",
      "AEerror, mean over last 10: 0.226411820508\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 139199\n",
      "Episode rewards, last 10: [-67.806719759000316, -200.11744057347167, -195.59129162565011, -215.27671735491975, -206.64071304824546, -183.04654194572907, -183.69271784263591, -198.48591426597497, -185.31463279440163, -227.66584035904572]\n",
      "Mean over last 500 episodes: -177.541125659\n",
      "Episode lengths, last 10: [64, 60, 63, 73, 71, 70, 66, 62, 65, 78]\n",
      "AEerror, mean over last 10: 0.223363137435\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 174476\n",
      "Episode rewards, last 10: [-29.060286274948083, -249.37077312389317, -208.86052101657998, -191.10315607336031, -163.62990015498409, -176.38213767908971, -181.11524413233124, -201.0136832114606, -183.1645995271812, -214.25786903793832]\n",
      "Mean over last 500 episodes: -178.727577882\n",
      "Episode lengths, last 10: [97, 76, 63, 65, 77, 76, 67, 79, 86, 74]\n",
      "AEerror, mean over last 10: 0.223362972241\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 31603\n",
      "Episode rewards, last 10: [-198.88418502952223, -228.53247199013998, -163.9550109570788, -189.60527910014554, -186.59181747383244, -150.76650730794896, -148.64295718266663, -182.94209166393713, -182.70845118028558, -80.844372867575828]\n",
      "Mean over last 500 episodes: -237.710727458\n",
      "Episode lengths, last 10: [89, 84, 63, 77, 72, 85, 58, 64, 51, 57]\n",
      "AEerror, mean over last 10: 0.638751417591\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 66395\n",
      "Episode rewards, last 10: [-209.16694706502807, -176.47049806841119, -196.87710396501873, -195.07772897232911, -155.7145242320189, -160.24840111731427, -178.77047003782161, -214.77475129050504, -175.94055127724206, -158.84292530994469]\n",
      "Mean over last 500 episodes: -178.663305823\n",
      "Episode lengths, last 10: [77, 56, 74, 53, 59, 54, 51, 72, 76, 58]\n",
      "AEerror, mean over last 10: 0.709242697614\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 101388\n",
      "Episode rewards, last 10: [-181.60979489990905, -202.0035864639926, -156.03678093631888, -187.07101255363216, -208.31662907166381, -184.52719243513425, -172.40964966413028, -383.31212893572354, -169.29098543898488, -167.44642866533167]\n",
      "Mean over last 500 episodes: -177.83054396\n",
      "Episode lengths, last 10: [54, 67, 89, 70, 71, 57, 54, 98, 69, 77]\n",
      "AEerror, mean over last 10: 0.785218334347\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 136357\n",
      "Episode rewards, last 10: [-209.9997972132752, -168.35975868389397, -174.1860019665921, -232.53709911770022, -186.61523872169363, -234.27712657223927, -158.83173251651687, -174.29220610379309, -209.2965953057768, -185.11509948099948]\n",
      "Mean over last 500 episodes: -180.823965109\n",
      "Episode lengths, last 10: [72, 66, 66, 74, 87, 82, 79, 51, 65, 85]\n",
      "AEerror, mean over last 10: 0.720319424053\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 170807\n",
      "Episode rewards, last 10: [-119.73199172358679, -193.26585476913658, -225.9004617670447, -190.82738729949449, -207.47036635837276, -179.47338683701679, -180.79470334473473, -207.86523825931627, -229.17281105640546, -182.41713408446748]\n",
      "Mean over last 500 episodes: -180.820097913\n",
      "Episode lengths, last 10: [54, 87, 82, 64, 58, 50, 88, 76, 89, 57]\n",
      "AEerror, mean over last 10: 0.749338418035\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 30020\n",
      "Episode rewards, last 10: [-165.90137170293008, -194.48446725034651, -153.91423280454529, -206.90920708450307, -200.69507020697992, -170.71154720734893, -268.89036523939524, -169.11953429524988, -196.64859655884132, -182.88482950342245]\n",
      "Mean over last 500 episodes: -201.901631891\n",
      "Episode lengths, last 10: [56, 64, 59, 78, 60, 65, 93, 68, 78, 78]\n",
      "AEerror, mean over last 10: 0.632496876383\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 65054\n",
      "Episode rewards, last 10: [-209.73058348560207, -171.79932516557244, -151.7303503933552, -189.13639606708051, -212.38124425357174, -206.59443705925798, -194.41118202734063, -221.88379228446379, -174.82659330267697, -200.0011799172471]\n",
      "Mean over last 500 episodes: -177.034496095\n",
      "Episode lengths, last 10: [81, 52, 85, 71, 82, 88, 79, 71, 52, 62]\n",
      "AEerror, mean over last 10: 0.67472186475\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 100450\n",
      "Episode rewards, last 10: [-214.19521697662282, -183.55754596067288, -184.75937257292611, -170.72671862064419, -179.13877081926034, -231.8811173335354, -140.19399391812357, -164.21005000092987, -177.45141987337041, -159.36219707333896]\n",
      "Mean over last 500 episodes: -176.273836414\n",
      "Episode lengths, last 10: [63, 82, 49, 73, 55, 74, 86, 87, 64, 64]\n",
      "AEerror, mean over last 10: 0.617598910754\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 135588\n",
      "Episode rewards, last 10: [-174.56129219616406, -31.138717123602788, -189.20791311142625, -226.6765482793856, -178.76425823935108, -134.69367520914824, -202.86475769434111, -195.3663292377183, -204.45956024484312, -186.02051774598257]\n",
      "Mean over last 500 episodes: -173.898300465\n",
      "Episode lengths, last 10: [51, 68, 82, 83, 60, 51, 67, 81, 81, 67]\n",
      "AEerror, mean over last 10: 0.636389769451\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 170246\n",
      "Episode rewards, last 10: [-297.657593871448, -181.93443892867242, -127.0550507174581, -194.96105836757457, -175.21405922420001, -206.99206047829836, -168.11188778351689, -174.22410081088671, -179.63891543661597, -182.9849683192528]\n",
      "Mean over last 500 episodes: -176.405512395\n",
      "Episode lengths, last 10: [99, 81, 51, 55, 60, 66, 70, 71, 62, 76]\n",
      "AEerror, mean over last 10: 0.608512211641\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 31945\n",
      "Episode rewards, last 10: [-178.30141555696667, -246.13302883518901, -144.42733749128064, -197.54146311678375, -229.331240488383, -169.4365845042656, -157.39849631034858, -142.04365764829029, -184.8679207685048, -182.00225541217119]\n",
      "Mean over last 500 episodes: -222.712483151\n",
      "Episode lengths, last 10: [62, 86, 52, 72, 88, 89, 78, 54, 56, 83]\n",
      "AEerror, mean over last 10: 0.23764477621\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 67001\n",
      "Episode rewards, last 10: [-176.55173785585248, -153.27966106634, -173.2283603319018, -232.47778936999737, -191.43385298296815, -305.03266194447951, -161.78190736132314, -147.15916075439722, -219.7715469851687, -189.78976234215475]\n",
      "Mean over last 500 episodes: -182.081726628\n",
      "Episode lengths, last 10: [66, 81, 80, 83, 70, 81, 54, 53, 81, 68]\n",
      "AEerror, mean over last 10: 0.243357338683\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 102276\n",
      "Episode rewards, last 10: [-195.011013815361, -23.774808309717457, -203.90663970669564, -173.43103314106312, -187.98195759045697, -170.99785994858539, -60.261198379182218, -215.66911199374624, -134.85726120157591, -151.40637323446811]\n",
      "Mean over last 500 episodes: -174.82801527\n",
      "Episode lengths, last 10: [76, 92, 88, 58, 83, 56, 80, 63, 51, 78]\n",
      "AEerror, mean over last 10: 0.244303343458\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 137302\n",
      "Episode rewards, last 10: [-181.38808505869002, -133.59039311778193, -26.747557543657251, -201.03508951713476, -204.05024774955118, -174.05864823254313, -150.10674755186662, -147.49405102356758, -174.87883916402467, -207.26131777110072]\n",
      "Mean over last 500 episodes: -178.779918688\n",
      "Episode lengths, last 10: [84, 89, 79, 74, 68, 89, 85, 82, 82, 72]\n",
      "AEerror, mean over last 10: 0.22770524785\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 172009\n",
      "Episode rewards, last 10: [-212.65187306455107, -188.41237305840446, -189.92952274823978, -200.92037319612214, -214.23557327090916, -209.7842899272112, -185.94713846381146, -175.97921300162866, -187.03979359897269, -178.26638977836433]\n",
      "Mean over last 500 episodes: -177.502237266\n",
      "Episode lengths, last 10: [67, 53, 72, 82, 66, 83, 86, 58, 69, 52]\n",
      "AEerror, mean over last 10: 0.249037501904\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 32225\n",
      "Episode rewards, last 10: [-166.85238164698279, -156.95281893253204, -143.07576639702316, -183.47488520741979, -184.95489013927451, -236.28782423543157, -185.41441084344683, -177.81412194022974, -66.540527867290308, -179.92596417096183]\n",
      "Mean over last 500 episodes: -274.914874092\n",
      "Episode lengths, last 10: [51, 56, 81, 70, 67, 89, 56, 51, 67, 66]\n",
      "AEerror, mean over last 10: 0.263385232522\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 66935\n",
      "Episode rewards, last 10: [-168.2028560631627, -170.72490243569214, -187.84715847671737, -179.35222117395546, -129.96622527539409, -165.75351416499373, -183.4232369796006, -183.89144080076713, -43.957271659094076, -224.03813421391303]\n",
      "Mean over last 500 episodes: -179.112097848\n",
      "Episode lengths, last 10: [57, 69, 56, 89, 53, 48, 60, 61, 94, 87]\n",
      "AEerror, mean over last 10: 0.249441067085\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 102025\n",
      "Episode rewards, last 10: [-175.48749494400522, -150.2394257842337, -243.89787227143671, -156.38340721476601, -160.79318686785481, -158.9268445363854, -176.55169759883509, -203.5267313413689, -333.00842258933011, -158.09508665051428]\n",
      "Mean over last 500 episodes: -177.786440457\n",
      "Episode lengths, last 10: [68, 82, 88, 78, 75, 89, 67, 77, 90, 57]\n",
      "AEerror, mean over last 10: 0.235064359373\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 136598\n",
      "Episode rewards, last 10: [-207.87081496594021, -182.9710970040648, -163.69606772128751, -179.50584865362967, -186.68404300372416, -180.32328226687162, -56.436358208086602, -154.00564776744841, -192.91030603236334, -195.33137788630839]\n",
      "Mean over last 500 episodes: -179.002434503\n",
      "Episode lengths, last 10: [78, 63, 55, 57, 65, 76, 64, 56, 52, 84]\n",
      "AEerror, mean over last 10: 0.235086766111\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 171455\n",
      "Episode rewards, last 10: [-187.34842920652332, -183.50437835404065, -202.10403544096729, -189.3193741227978, -359.10144596226365, -168.31205371641767, -145.9990384606368, -202.81173509736089, -175.06077213782891, -141.67367055639698]\n",
      "Mean over last 500 episodes: -177.276245426\n",
      "Episode lengths, last 10: [62, 82, 64, 61, 81, 85, 72, 64, 54, 51]\n",
      "AEerror, mean over last 10: 0.292505585622\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 32375\n",
      "Episode rewards, last 10: [-186.44892687729993, -189.1239368190825, -242.26628425933677, -181.28991654866931, -208.70440560282648, -167.59200969340333, -141.37690947802534, -214.61885978005299, -203.70392200651295, -233.44494990287558]\n",
      "Mean over last 500 episodes: -234.499545294\n",
      "Episode lengths, last 10: [70, 85, 88, 65, 75, 56, 82, 87, 70, 87]\n",
      "AEerror, mean over last 10: 0.241479184116\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 67274\n",
      "Episode rewards, last 10: [-168.56498253656994, -228.41879286669365, -205.75605897080641, -161.51818916017459, -173.32220741936237, -209.93498894837433, -178.43172849073858, -159.10059788221901, -195.18809782210019, -176.29581892829907]\n",
      "Mean over last 500 episodes: -176.659470205\n",
      "Episode lengths, last 10: [66, 74, 70, 74, 80, 63, 54, 78, 68, 64]\n",
      "AEerror, mean over last 10: 0.22801667029\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 102346\n",
      "Episode rewards, last 10: [-163.10421515400549, -67.382986682581844, -237.14733570115516, -164.1460790380599, -224.65827048771843, -209.8947040180793, -211.97425784707269, -138.62548721459743, -264.93532614683727, -165.79471158883274]\n",
      "Mean over last 500 episodes: -180.044035337\n",
      "Episode lengths, last 10: [82, 78, 89, 80, 88, 81, 66, 56, 93, 52]\n",
      "AEerror, mean over last 10: 0.268438805359\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 137421\n",
      "Episode rewards, last 10: [-155.83507720684835, -179.55508070138956, -241.9969875896669, -190.06616585024733, -147.27376910369318, -169.15066555919765, -220.03015776057129, -147.46199737740838, -211.39524535790045, -176.56804775556134]\n",
      "Mean over last 500 episodes: -179.258080376\n",
      "Episode lengths, last 10: [58, 48, 82, 70, 53, 83, 73, 56, 82, 62]\n",
      "AEerror, mean over last 10: 0.442653707308\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 171963\n",
      "Episode rewards, last 10: [-185.92734601050751, -194.23979693212561, -162.981147369246, -182.18260597761997, -229.01043647145315, -168.76964861167113, -49.694201172007979, -200.66363185504304, -214.42787481031769, -162.92601442124189]\n",
      "Mean over last 500 episodes: -175.671958547\n",
      "Episode lengths, last 10: [58, 77, 61, 85, 83, 72, 93, 66, 63, 72]\n",
      "AEerror, mean over last 10: 0.246272229753\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 32006\n",
      "Episode rewards, last 10: [-8.4506635176594571, -41.104441374475613, -228.3159267477094, -201.51832357293725, -167.07976743520717, -196.39509981689258, -181.4653891482657, -188.93934023905865, -225.44762414140473, -194.05404619126028]\n",
      "Mean over last 500 episodes: -229.248482904\n",
      "Episode lengths, last 10: [82, 87, 68, 84, 63, 66, 68, 90, 69, 53]\n",
      "AEerror, mean over last 10: 0.260111210123\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 66615\n",
      "Episode rewards, last 10: [-188.77366273572346, -195.27523412766163, -195.2993092184129, -151.18722499948962, -159.28273836192341, -178.59658379343051, -149.06073980320463, -152.36175735789823, -233.61037933302373, -205.55589527600799]\n",
      "Mean over last 500 episodes: -177.621044045\n",
      "Episode lengths, last 10: [70, 66, 79, 61, 79, 65, 55, 61, 81, 71]\n",
      "AEerror, mean over last 10: 0.196151370198\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 101571\n",
      "Episode rewards, last 10: [-223.34555940112068, -208.33800820110699, -146.32805030367024, -230.69270649403674, -152.73888479607504, -183.05261962812298, -172.0356127553994, -215.39254604697635, -237.369394410246, -189.9688818802596]\n",
      "Mean over last 500 episodes: -182.908112336\n",
      "Episode lengths, last 10: [66, 67, 80, 85, 58, 62, 52, 87, 85, 70]\n",
      "AEerror, mean over last 10: 0.226392036519\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 136041\n",
      "Episode rewards, last 10: [-207.5945717942077, -181.62992158322345, -231.61881140590063, -182.66677739905552, -181.9028568886423, -161.79698813283858, -134.77752374858673, -185.74876426610567, -216.72739725010678, -184.63964357114645]\n",
      "Mean over last 500 episodes: -176.843137351\n",
      "Episode lengths, last 10: [61, 82, 72, 66, 51, 82, 54, 79, 76, 80]\n",
      "AEerror, mean over last 10: 0.204232144953\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 171490\n",
      "Episode rewards, last 10: [-182.25947688179875, -76.26688542292527, -207.60517674550854, -132.00065134387768, -200.23339225938102, -194.33668857894023, -145.32419972814043, -250.80016258360098, -196.84474745827089, -187.06419253822634]\n",
      "Mean over last 500 episodes: -180.753874259\n",
      "Episode lengths, last 10: [85, 70, 79, 69, 58, 75, 88, 81, 61, 68]\n",
      "AEerror, mean over last 10: 0.233475833334\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = produce_experiment(DDQL, ddql_combined_params, ddql_train_params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(ddql_res, window = 100, std_coef = 0.2, results_over = 1000):\n",
    "    res_lists = [k.rList for k in ddql_res]\n",
    "    res_lists = np.array(res_lists)\n",
    "    pd.DataFrame(data = res_lists)\n",
    "    mean = res_lists.mean(axis = 0)\n",
    "    std = res_lists.std(axis = 0)\n",
    "    rol_mean = np.nan_to_num(pd.Series(mean).rolling(window = window).mean())[window:]\n",
    "    rol_std = np.nan_to_num(pd.Series(std).rolling(window = window).mean())[window:]\n",
    "    plt.figure()\n",
    "    index = np.arange(window, len(rol_mean) + window)\n",
    "    plt.plot(index, rol_mean)\n",
    "    plt.fill_between(index, rol_mean-std_coef*rol_std, rol_mean+std_coef*rol_std, color='b', alpha=0.1)\n",
    "    return max(rol_mean[window:results_over]), rol_mean[window:results_over].mean(), pd.DataFrame(data = res_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQE4P8D/+coUWTKJF4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15131f250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comb_max, comb_auc, comb_df = plot_results(res, 100, 0.2)\n",
    "plt.savefig(\"comb_ll.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-176.693954741 -189.995532945\n"
     ]
    }
   ],
   "source": [
    "print comb_max, comb_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_df.to_csv(\"comb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comb_df[[i for i in range(0, 1250)]].to_csv(\"comb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "3056bb9a411640cf9466c8d6cd6a887d": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "4ebbbd03a48d47ac836e79a58584349d": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "66efea38887246bca4aed1a17b33e684": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "79df76cb64c54678ab07bc448864ca19": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "81e96bf1a1d54dd6b651b2d6abb8261d": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "83789d958c1f42ceb4918f50c600f599": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "c86aab172edb4195bed2627e9b4839b3": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "cfb6c23622c14a1fae43946dc337f109": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "e5edc5546042440093bd64fe058dd13f": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    },
    "ee47e48f9bbe4e66b0873db5cb091ea6": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
