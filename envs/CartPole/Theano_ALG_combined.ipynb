{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ATARI_wrapper():\n",
    "    def __init__(self, gamename = \"Enduro-v0\"):\n",
    "        self.state_size = (105, 80)\n",
    "        self.game_title = gamename\n",
    "        self.actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        grayimage = np.mean(state, axis = 2)\n",
    "        downscale = self.downscale2x(grayimage)\n",
    "        norm = (downscale - 128.0) / 128.0\n",
    "        return norm\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3\n",
    "    \n",
    "    def downscale2x(self, image):\n",
    "        image00 = image[0::2, 0::2]\n",
    "        image01 = image[0::2, 1::2]\n",
    "        image10 = image[1::2, 0::2]\n",
    "        image11 = image[1::2, 1::2]\n",
    "        return (image00 + image01 + image10 + image11) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, \n",
    "                 encodeunits = 10, encodenoise = 0.1, aelosscoef = 0.1, regaelosscoef = 2.0, \n",
    "                 explosscoef = 0.1, h = 50, alpha = 25):\n",
    "        self.input_var = T.tensor4('statebatch')\n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.targetE = T.fvector('targetE')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.newstates = T.tensor4(\"newstatebatch\")\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.aelosscoef = aelosscoef\n",
    "        self.regaelosscoef = regaelosscoef\n",
    "        self.explosscoef = explosscoef\n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape, encodeunits, encodenoise)\n",
    "        self.build_AVQ(grad_clipping, lr, h, alpha)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape, encodeunits, encodenoise):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.encode = lasagne.layers.DenseLayer(self.outlayer, encodeunits, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "        self.encodenoise = lasagne.layers.GaussianNoiseLayer(self.encode, sigma = encodenoise)\n",
    "        self.le3 = lasagne.layers.DenseLayer(self.encodenoise, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le2 = lasagne.layers.DenseLayer(self.le3, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le1 = lasagne.layers.DenseLayer(self.le2, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le0 = lasagne.layers.DenseLayer(self.le1, channels_number * image_shape[0] * image_shape[1])\n",
    "        self.l_aeout = lasagne.layers.ReshapeLayer(self.le0, shape=(-1, channels_number, image_shape[0], image_shape[1]))\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr, h, alpha):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        self.l_varq = lasagne.layers.DenseLayer(self.outlayer, self.n_actions, nonlinearity=lasagne.nonlinearities.softplus)\n",
    "        self.l_exp_out = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        \n",
    "        self.advantage, self.value, self.ae_out, self.enc, self.varq, self.exp_out = \\\n",
    "            lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_aeout, self.encode, self.l_varq, self.l_exp_out])\n",
    "        \n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Qout = self.advantage + self.value - self.average_advantage\n",
    "#-----\n",
    "        self.Q = self.advantage\n",
    "        self.predict = T.argmax(self.Q, axis = 1)\n",
    "        \n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, self.n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.E = self.exp_out\n",
    "        self.Epredict = T.argmax(self.E, axis = 1)\n",
    "        \n",
    "        self.E0 = T.sum(self.E * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.Q0 = T.sum(self.Q * self.actions_onehot, axis = 1)\n",
    "        self.varQ0 = T.sum(self.varq * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.Q1 = self.Q0 + (self.targetQ - self.Q0) / (h + 1)\n",
    "        self.varQ1 = (h * (alpha - 1)) / ((h + 1) * (alpha - 0.5)) * \\\n",
    "                     (self.varQ0 + T.sqr(self.targetQ - self.Q0) / (2 * (h+1) * (alpha - 1)))\n",
    "        \n",
    "        self.exp_error = T.mean(T.sqr(self.targetE - self.E0))\n",
    "        self.td_error = T.mean(T.sqr(self.Q1 - self.Q0))\n",
    "        self.var_error = T.mean(T.sqr(self.varQ1 - self.varQ0))\n",
    "        self.regae_error = 0.25 - T.mean(T.sqr(self.enc - 0.5))\n",
    "        self.ae_error = T.mean(T.sqr(self.ae_out - self.input_var))\n",
    "        \n",
    "        self.loss = self.ae_error * self.aelosscoef + self.regaelosscoef * self.regae_error + self.td_error + self.var_error + self.explosscoef * self.exp_error\n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "\n",
    "        enc_layers = [self.l2, self.l3, self.outlayer, self.encode, self.le3, self.le2, self.le1, self.le0]\n",
    "        enc_params = [l.W for l in enc_layers] + [l.b for l in enc_layers]\n",
    "        self.enc_grads = T.grad(self.ae_error, enc_params)\n",
    "        self.enc_scaled_grads = lasagne.updates.total_norm_constraint(self.enc_grads, grad_clipping)\n",
    "        self.enc_updates = lasagne.updates.adam(self.enc_scaled_grads, enc_params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Q)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.full_train_fn = theano.function([self.input_var, self.targetQ, self.targetE, self.actions], [self.ae_error, self.td_error], updates = self.updates)\n",
    "        self.train_encoder = theano.function([self.input_var], self.ae_error, updates = self.enc_updates)\n",
    "        self.get_encode = theano.function([self.input_var], [self.enc, self.regae_error])\n",
    "        self.var_fn = theano.function([self.input_var], self.varq)\n",
    "        \n",
    "        self.Eout_fn = theano.function([self.input_var], self.E)\n",
    "        self.Eactionpred_fn = theano.function([self.input_var], self.Epredict)\n",
    "        \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values(self.l_advantage, values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.buffer))\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class combined_agent:\n",
    "    def __init__(self, n_actions, actionpred_fn, Eactionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "        self.Eactionpred_fn = Eactionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = self.Eactionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.encode_unitnum = lparams[\"encode_unitnum\"]\n",
    "        self.encode_noise = lparams[\"encode_noise\"]\n",
    "        self.encode_reward_multiplier = lparams[\"encode_reward_multiplier\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.egamma = lparams[\"egamma\"]\n",
    "        self.aelosscoef = lparams[\"aelosscoef\"]\n",
    "        self.regaelosscoef = lparams[\"regaelosscoef\"]\n",
    "        self.explosscoef = lparams[\"explosscoef\"]\n",
    "        self.h = lparams[\"lambda\"]\n",
    "        self.alpha = lparams[\"alpha\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "\n",
    "        self.env = env\n",
    "        \n",
    "        AVQ_params = [self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, \n",
    "                      self.lr, self.encode_unitnum, self.encode_noise, self.aelosscoef, self.regaelosscoef, \n",
    "                      self.explosscoef, self.h, self.alpha]\n",
    "        self.mainQN = AVQ_nn(*AVQ_params)\n",
    "        self.targetQN = AVQ_nn(*AVQ_params)\n",
    "\n",
    "        self.lList = []\n",
    "        self.rList = []\n",
    "        self.aeList = []\n",
    "        self.encode_counts = defaultdict(int)\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            agent = egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            agent = boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        elif agent == \"combined\":\n",
    "            agent = combined_agent(self.env.n_actions, self.mainQN.actionpred_fn, self.mainQN.Eactionpred_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "            \n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "        \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "            episode_aeerrors = np.array([])\n",
    "            \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                experience = np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5])\n",
    "                self.experience_storage.add(experience)\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        \n",
    "                        encodes, reg_errors = self.mainQN.get_encode(old_state_batch)\n",
    "                        encodes = [\"\".join([str(k) for k in encode]) for encode in (encodes > 0.5).astype(int)]\n",
    "                        for encode in encodes:\n",
    "                            self.encode_counts[encode] += 1\n",
    "                        encode_rewards = self.encode_reward_multiplier / np.sqrt(np.array([self.encode_counts[encode] for encode in encodes]))\n",
    "                        \n",
    "                        E1 = self.mainQN.Eactionpred_fn(new_state_batch)\n",
    "                        E2 = self.targetQN.Eout_fn(new_state_batch)\n",
    "                        doubleE = E2[range(self.batch_size), E1]\n",
    "                        var1 = self.mainQN.var_fn(old_state_batch)\n",
    "                        doublevar = var1[range(self.batch_size), E1]\n",
    "                        exp_rewards_vector = doublevar + 0.01 * encode_rewards\n",
    "                        \n",
    "                        targetE = (exp_rewards_vector + (self.egamma * doubleE * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)\n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        doubleQ = Q2[range(self.batch_size), Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        aeerror, tderror = self.mainQN.full_train_fn(old_state_batch, targetQ, targetE, action_vector)\n",
    "                        episode_aeerrors = np.append(episode_aeerrors, aeerror)\n",
    "                else:\n",
    "                    train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                    old_state_batch = np.stack(train_batch[:,0])\n",
    "                    new_state_batch = np.stack(train_batch[:,3])\n",
    "                    action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                    aeerror1 = self.mainQN.train_encoder(old_state_batch)\n",
    "                    aeerror2 = self.mainQN.train_encoder(new_state_batch)\n",
    "                    episode_aeerrors = np.append(episode_aeerrors, (aeerror1 + aeerror2)/2)\n",
    "                    \n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            total_aeerror = np.mean(episode_aeerrors)\n",
    "            self.lList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            self.aeList.append(total_aeerror)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.lList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"AEerror, mean over last 10:\", str(np.mean(self.aeList[-10:]))]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.lList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-15 16:25:57,920] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "#llenv = LunarLanding_wrapper()\n",
    "cp_env = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 3,\n",
    "           \"encode_unitnum\": 10,\n",
    "           \"encode_noise\": 0.05,\n",
    "           \"encode_reward_multiplier\": 10.0,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"aelosscoef\" : 0.1,\n",
    "           \"regaelosscoef\" : 2.0,\n",
    "           \"explosscoef\" : 0.1,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"egamma\" : 0.01,\n",
    "           \"lambda\": 4,\n",
    "           \"alpha\": 2,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 500,\n",
    "           \"pretrain_steps\" : 5000,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.06,\n",
    "                                \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combinedagentinfo = {\"agent\" : \"combined\",\n",
    "                     \"params\" : {\"startE\": 0.5,\n",
    "                                 \"endE\" : 0.1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ddql = DDQL(lparams, cp_env, combinedagentinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddql.train(num_episodes = 3000, frame_limit = 500, render = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_rewards(ddql, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм с учитыванием частоты посещения состояний. Подсчет посещения состояний определяется с использованием автоэнкодера, в котором все нейроны центрального слоя (с сигмоидной функцией активации) регуляризуются для помещения значений либо у нуля, либо у единицы. Эта регуляризация осуществляется с помощью изменения оптимизируемой функции потерь и с помощью добавления слоя гауссового шума после центрального слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стабильность подобной кодировки можно ожидать из-за того, что сигмоидные нейроны быстро насыщаются, предотвращая значительные их колебания во время динамического обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждому действию агента добавляется дополнительная награда, причем она обратно пропорциональна количеству раз, которые агент бывал в этом состоянии(если точнее, то количеству раз, которые агент бывал в состояниях, имеющих ту же бинарную кодировку автоэнкодера, что и текущее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_experiment(ddql, ddql_init_params, ddql_train_params, experiment_num = 5):\n",
    "    ddql_list = [ddql(**ddql_init_params) for k in range(experiment_num)]\n",
    "    \n",
    "    for k in range(experiment_num):\n",
    "        ddql_list[k].train(**ddql_train_params)\n",
    "        \n",
    "    return ddql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql_combined_params = {\"lparams\":lparams, \"env\":cp_env, \"agent\":combinedagentinfo}\n",
    "ddql_egreedy_params = {\"lparams\":lparams, \"env\":cp_env, \"agent\":egreedyagentinfo}\n",
    "\n",
    "ddql_train_params = {\"num_episodes\":2500, \"frame_limit\":500, \"render\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 499 ================================================\n",
      "Total steps: 16364\n",
      "Episode rewards, last 10: [500.0, 14.0, 37.0, 169.0, 95.0, 191.0, 67.0, 128.0, 128.0, 13.0]\n",
      "Mean over last 500 episodes: 42.728\n",
      "Episode lengths, last 10: [499, 13, 36, 168, 94, 190, 66, 127, 127, 12]\n",
      "AEerror, mean over last 10: 0.34422588198\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 112007\n",
      "Episode rewards, last 10: [500.0, 500.0, 416.0, 152.0, 14.0, 16.0, 10.0, 28.0, 44.0, 48.0]\n",
      "Mean over last 500 episodes: 191.286\n",
      "Episode lengths, last 10: [499, 499, 415, 151, 13, 15, 9, 27, 43, 47]\n",
      "AEerror, mean over last 10: 0.743457699211\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 225778\n",
      "Episode rewards, last 10: [93.0, 94.0, 95.0, 96.0, 99.0, 109.0, 111.0, 103.0, 112.0, 125.0]\n",
      "Mean over last 500 episodes: 227.542\n",
      "Episode lengths, last 10: [92, 93, 94, 95, 98, 108, 110, 102, 111, 124]\n",
      "AEerror, mean over last 10: 0.873022741975\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 332350\n",
      "Episode rewards, last 10: [21.0, 16.0, 9.0, 315.0, 123.0, 216.0, 110.0, 98.0, 117.0, 122.0]\n",
      "Mean over last 500 episodes: 213.144\n",
      "Episode lengths, last 10: [20, 15, 8, 314, 122, 215, 109, 97, 116, 121]\n",
      "AEerror, mean over last 10: 0.57634559704\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 408611\n",
      "Episode rewards, last 10: [299.0, 123.0, 18.0, 20.0, 97.0, 45.0, 22.0, 500.0, 32.0, 91.0]\n",
      "Mean over last 500 episodes: 152.522\n",
      "Episode lengths, last 10: [298, 122, 17, 19, 96, 44, 21, 499, 31, 90]\n",
      "AEerror, mean over last 10: 0.514808730915\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 14285\n",
      "Episode rewards, last 10: [105.0, 11.0, 12.0, 70.0, 44.0, 500.0, 367.0, 92.0, 274.0, 276.0]\n",
      "Mean over last 500 episodes: 38.57\n",
      "Episode lengths, last 10: [104, 10, 11, 69, 43, 499, 366, 91, 273, 275]\n",
      "AEerror, mean over last 10: 0.529209181766\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 113459\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 221.0, 393.0, 78.0]\n",
      "Mean over last 500 episodes: 198.348\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 499, 499, 220, 392, 77]\n",
      "AEerror, mean over last 10: 0.184498021049\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 254425\n",
      "Episode rewards, last 10: [122.0, 107.0, 141.0, 126.0, 42.0, 170.0, 123.0, 117.0, 132.0, 108.0]\n",
      "Mean over last 500 episodes: 281.932\n",
      "Episode lengths, last 10: [121, 106, 140, 125, 41, 169, 122, 116, 131, 107]\n",
      "AEerror, mean over last 10: 0.54535065678\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 373142\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 384.0]\n",
      "Mean over last 500 episodes: 237.434\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 499, 499, 499, 499, 383]\n",
      "AEerror, mean over last 10: 0.178726177495\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 483289\n",
      "Episode rewards, last 10: [141.0, 141.0, 82.0, 56.0, 174.0, 336.0, 315.0, 500.0, 397.0, 376.0]\n",
      "Mean over last 500 episodes: 220.294\n",
      "Episode lengths, last 10: [140, 140, 81, 55, 173, 335, 314, 499, 396, 375]\n",
      "AEerror, mean over last 10: 0.551503825556\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 283\n",
      "Episode rewards, last 10: [15.0, 13.0, 11.0, 16.0, 14.0, 22.0, 12.0, 29.0, 16.0, 11.0]\n",
      "Mean over last 500 episodes: 10.566\n",
      "Episode lengths, last 10: [14, 12, 10, 15, 13, 21, 11, 28, 15, 10]\n",
      "AEerror, mean over last 10: 0.169844373821\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 73876\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 108.0, 48.0, 24.0, 58.0, 17.0, 11.0]\n",
      "Mean over last 500 episodes: 147.186\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 107, 47, 23, 57, 16, 10]\n",
      "AEerror, mean over last 10: 0.274794525391\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 181282\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 470.0, 500.0, 13.0, 16.0, 11.0, 168.0, 147.0]\n",
      "Mean over last 500 episodes: 214.812\n",
      "Episode lengths, last 10: [499, 499, 499, 469, 499, 12, 15, 10, 167, 146]\n",
      "AEerror, mean over last 10: 0.212450090037\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 281795\n",
      "Episode rewards, last 10: [35.0, 20.0, 56.0, 76.0, 50.0, 105.0, 155.0, 88.0, 114.0, 18.0]\n",
      "Mean over last 500 episodes: 201.026\n",
      "Episode lengths, last 10: [34, 19, 55, 75, 49, 104, 154, 87, 113, 17]\n",
      "AEerror, mean over last 10: 0.516381354387\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 380997\n",
      "Episode rewards, last 10: [56.0, 47.0, 30.0, 55.0, 282.0, 336.0, 407.0, 317.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 198.404\n",
      "Episode lengths, last 10: [55, 46, 29, 54, 281, 335, 406, 316, 499, 499]\n",
      "AEerror, mean over last 10: 0.235960505663\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 0\n",
      "Episode rewards, last 10: [9.0, 10.0, 8.0, 10.0, 9.0, 10.0, 10.0, 9.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 9.404\n",
      "Episode lengths, last 10: [8, 9, 7, 9, 8, 9, 9, 8, 9, 9]\n",
      "AEerror, mean over last 10: 0.754640500343\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 44435\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 160.0, 70.0, 29.0]\n",
      "Mean over last 500 episodes: 89.466\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 499, 499, 159, 69, 28]\n",
      "AEerror, mean over last 10: 0.184135512655\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 157152\n",
      "Episode rewards, last 10: [124.0, 500.0, 131.0, 138.0, 115.0, 115.0, 19.0, 155.0, 110.0, 113.0]\n",
      "Mean over last 500 episodes: 225.434\n",
      "Episode lengths, last 10: [123, 499, 130, 137, 114, 114, 18, 154, 109, 112]\n",
      "AEerror, mean over last 10: 0.642618241345\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 262432\n",
      "Episode rewards, last 10: [500.0, 402.0, 334.0, 500.0, 207.0, 500.0, 403.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 210.56\n",
      "Episode lengths, last 10: [499, 401, 333, 499, 206, 499, 402, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.337406205343\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 372257\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 349.0, 39.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 219.65\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 348, 38, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.190074029148\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 1472\n",
      "Episode rewards, last 10: [33.0, 12.0, 12.0, 15.0, 11.0, 27.0, 30.0, 85.0, 52.0, 24.0]\n",
      "Mean over last 500 episodes: 12.944\n",
      "Episode lengths, last 10: [32, 11, 11, 14, 10, 26, 29, 84, 51, 23]\n",
      "AEerror, mean over last 10: 0.18418216578\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 64578\n",
      "Episode rewards, last 10: [167.0, 120.0, 116.0, 122.0, 107.0, 89.0, 20.0, 16.0, 11.0, 84.0]\n",
      "Mean over last 500 episodes: 126.212\n",
      "Episode lengths, last 10: [166, 119, 115, 121, 106, 88, 19, 15, 10, 83]\n",
      "AEerror, mean over last 10: 0.547549435395\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 169948\n",
      "Episode rewards, last 10: [500.0, 294.0, 215.0, 188.0, 99.0, 227.0, 126.0, 188.0, 13.0, 12.0]\n",
      "Mean over last 500 episodes: 210.74\n",
      "Episode lengths, last 10: [499, 293, 214, 187, 98, 226, 125, 187, 12, 11]\n",
      "AEerror, mean over last 10: 0.688757813365\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 264580\n",
      "Episode rewards, last 10: [11.0, 148.0, 147.0, 134.0, 131.0, 124.0, 140.0, 136.0, 122.0, 122.0]\n",
      "Mean over last 500 episodes: 189.264\n",
      "Episode lengths, last 10: [10, 147, 146, 133, 130, 123, 139, 135, 121, 121]\n",
      "AEerror, mean over last 10: 0.630116499334\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 357220\n",
      "Episode rewards, last 10: [140.0, 117.0, 158.0, 485.0, 401.0, 168.0, 136.0, 103.0, 103.0, 11.0]\n",
      "Mean over last 500 episodes: 185.28\n",
      "Episode lengths, last 10: [139, 116, 157, 484, 400, 167, 135, 102, 102, 10]\n",
      "AEerror, mean over last 10: 0.661634558992\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 7196\n",
      "Episode rewards, last 10: [20.0, 11.0, 12.0, 9.0, 20.0, 72.0, 94.0, 16.0, 26.0, 31.0]\n",
      "Mean over last 500 episodes: 24.392\n",
      "Episode lengths, last 10: [19, 10, 11, 8, 19, 71, 93, 15, 25, 30]\n",
      "AEerror, mean over last 10: 0.431320249778\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 112758\n",
      "Episode rewards, last 10: [239.0, 500.0, 500.0, 18.0, 329.0, 81.0, 120.0, 347.0, 101.0, 106.0]\n",
      "Mean over last 500 episodes: 211.124\n",
      "Episode lengths, last 10: [238, 499, 499, 17, 328, 80, 119, 346, 100, 105]\n",
      "AEerror, mean over last 10: 0.533771451755\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 205730\n",
      "Episode rewards, last 10: [12.0, 32.0, 10.0, 10.0, 11.0, 15.0, 15.0, 10.0, 100.0, 150.0]\n",
      "Mean over last 500 episodes: 185.944\n",
      "Episode lengths, last 10: [11, 31, 9, 9, 10, 14, 14, 9, 99, 149]\n",
      "AEerror, mean over last 10: 0.390174799659\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 309102\n",
      "Episode rewards, last 10: [22.0, 10.0, 29.0, 27.0, 30.0, 30.0, 18.0, 14.0, 19.0, 26.0]\n",
      "Mean over last 500 episodes: 206.744\n",
      "Episode lengths, last 10: [21, 9, 28, 26, 29, 29, 17, 13, 18, 25]\n",
      "AEerror, mean over last 10: 0.646423128208\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 379376\n",
      "Episode rewards, last 10: [90.0, 20.0, 32.0, 500.0, 500.0, 16.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 140.548\n",
      "Episode lengths, last 10: [89, 19, 31, 499, 499, 15, 499, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.403604892819\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 14264\n",
      "Episode rewards, last 10: [22.0, 157.0, 141.0, 256.0, 226.0, 500.0, 270.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 38.528\n",
      "Episode lengths, last 10: [21, 156, 140, 255, 225, 499, 269, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.298546659258\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 118854\n",
      "Episode rewards, last 10: [91.0, 85.0, 79.0, 105.0, 116.0, 96.0, 131.0, 500.0, 204.0, 56.0]\n",
      "Mean over last 500 episodes: 209.18\n",
      "Episode lengths, last 10: [90, 84, 78, 104, 115, 95, 130, 499, 203, 55]\n",
      "AEerror, mean over last 10: 0.420502748773\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 216493\n",
      "Episode rewards, last 10: [57.0, 63.0, 68.0, 49.0, 57.0, 109.0, 198.0, 51.0, 24.0, 13.0]\n",
      "Mean over last 500 episodes: 195.278\n",
      "Episode lengths, last 10: [56, 62, 67, 48, 56, 108, 197, 50, 23, 12]\n",
      "AEerror, mean over last 10: 0.411133247033\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 315661\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 92.0, 11.0, 15.0]\n",
      "Mean over last 500 episodes: 198.336\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 499, 499, 91, 10, 14]\n",
      "AEerror, mean over last 10: 0.237626635853\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 434834\n",
      "Episode rewards, last 10: [15.0, 13.0, 20.0, 29.0, 32.0, 23.0, 104.0, 500.0, 500.0, 467.0]\n",
      "Mean over last 500 episodes: 238.346\n",
      "Episode lengths, last 10: [14, 12, 19, 28, 31, 22, 103, 499, 499, 466]\n",
      "AEerror, mean over last 10: 0.26914240007\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 0\n",
      "Episode rewards, last 10: [12.0, 9.0, 10.0, 10.0, 10.0, 9.0, 10.0, 9.0, 12.0, 14.0]\n",
      "Mean over last 500 episodes: 9.99\n",
      "Episode lengths, last 10: [11, 8, 9, 9, 9, 8, 9, 8, 11, 13]\n",
      "AEerror, mean over last 10: 0.39967666221\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 4738\n",
      "Episode rewards, last 10: [10.0, 9.0, 9.0, 9.0, 8.0, 9.0, 8.0, 9.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 9.486\n",
      "Episode lengths, last 10: [9, 8, 8, 8, 7, 8, 7, 8, 9, 9]\n",
      "AEerror, mean over last 10: 0.484312090058\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 9564\n",
      "Episode rewards, last 10: [11.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.0, 8.0, 9.0]\n",
      "Mean over last 500 episodes: 9.652\n",
      "Episode lengths, last 10: [10, 9, 9, 9, 9, 9, 9, 8, 7, 8]\n",
      "AEerror, mean over last 10: 0.551460232728\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 14238\n",
      "Episode rewards, last 10: [10.0, 10.0, 9.0, 8.0, 10.0, 10.0, 10.0, 9.0, 8.0, 10.0]\n",
      "Mean over last 500 episodes: 9.348\n",
      "Episode lengths, last 10: [9, 9, 8, 7, 9, 9, 9, 8, 7, 9]\n",
      "AEerror, mean over last 10: 0.5562910553\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 18923\n",
      "Episode rewards, last 10: [8.0, 10.0, 10.0, 8.0, 10.0, 9.0, 9.0, 9.0, 9.0, 10.0]\n",
      "Mean over last 500 episodes: 9.37\n",
      "Episode lengths, last 10: [7, 9, 9, 7, 9, 8, 8, 8, 8, 9]\n",
      "AEerror, mean over last 10: 0.533571805281\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 9651\n",
      "Episode rewards, last 10: [38.0, 266.0, 356.0, 312.0, 165.0, 127.0, 148.0, 99.0, 13.0, 26.0]\n",
      "Mean over last 500 episodes: 29.302\n",
      "Episode lengths, last 10: [37, 265, 355, 311, 164, 126, 147, 98, 12, 25]\n",
      "AEerror, mean over last 10: 0.489147268971\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 101613\n",
      "Episode rewards, last 10: [369.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 183.924\n",
      "Episode lengths, last 10: [368, 499, 499, 499, 499, 499, 499, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.426646445588\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 202968\n",
      "Episode rewards, last 10: [454.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 371.0, 20.0]\n",
      "Mean over last 500 episodes: 202.71\n",
      "Episode lengths, last 10: [453, 499, 499, 499, 499, 499, 499, 499, 370, 19]\n",
      "AEerror, mean over last 10: 0.273880096592\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 279798\n",
      "Episode rewards, last 10: [500.0, 500.0, 378.0, 49.0, 20.0, 500.0, 209.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 153.66\n",
      "Episode lengths, last 10: [499, 499, 377, 48, 19, 499, 208, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.115667615566\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 358872\n",
      "Episode rewards, last 10: [209.0, 195.0, 331.0, 500.0, 500.0, 500.0, 500.0, 413.0, 137.0, 15.0]\n",
      "Mean over last 500 episodes: 158.148\n",
      "Episode lengths, last 10: [208, 194, 330, 499, 499, 499, 499, 412, 136, 14]\n",
      "AEerror, mean over last 10: 0.210145521418\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 0\n",
      "Episode rewards, last 10: [9.0, 10.0, 9.0, 10.0, 9.0, 9.0, 9.0, 10.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 9.338\n",
      "Episode lengths, last 10: [8, 9, 8, 9, 8, 8, 8, 9, 9, 9]\n",
      "AEerror, mean over last 10: 0.658772896236\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 4382\n",
      "Episode rewards, last 10: [8.0, 9.0, 10.0, 9.0, 9.0, 9.0, 10.0, 8.0, 11.0, 10.0]\n",
      "Mean over last 500 episodes: 9.426\n",
      "Episode lengths, last 10: [7, 8, 9, 8, 8, 8, 9, 7, 10, 9]\n",
      "AEerror, mean over last 10: 0.563587543931\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 9075\n",
      "Episode rewards, last 10: [9.0, 9.0, 10.0, 10.0, 10.0, 9.0, 10.0, 10.0, 9.0, 10.0]\n",
      "Mean over last 500 episodes: 9.386\n",
      "Episode lengths, last 10: [8, 8, 9, 9, 9, 8, 9, 9, 8, 9]\n",
      "AEerror, mean over last 10: 0.591148694921\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 13754\n",
      "Episode rewards, last 10: [10.0, 10.0, 9.0, 10.0, 11.0, 9.0, 8.0, 10.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 9.358\n",
      "Episode lengths, last 10: [9, 9, 8, 9, 10, 8, 7, 9, 9, 9]\n",
      "AEerror, mean over last 10: 0.583688816716\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 18444\n",
      "Episode rewards, last 10: [9.0, 9.0, 9.0, 9.0, 11.0, 10.0, 9.0, 9.0, 8.0, 9.0]\n",
      "Mean over last 500 episodes: 9.38\n",
      "Episode lengths, last 10: [8, 8, 8, 8, 10, 9, 8, 8, 7, 8]\n",
      "AEerror, mean over last 10: 0.581624984869\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = produce_experiment(DDQL, ddql_combined_params, ddql_train_params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(ddql_res, window = 100, std_coef = 0.2, results_over = 1000):\n",
    "    res_lists = [k.rList for k in ddql_res]\n",
    "    res_lists = np.array(res_lists)\n",
    "    pd.DataFrame(data = res_lists)\n",
    "    mean = res_lists.mean(axis = 0)\n",
    "    std = res_lists.std(axis = 0)\n",
    "    rol_mean = np.nan_to_num(pd.Series(mean).rolling(window = window).mean())\n",
    "    rol_std = np.nan_to_num(pd.Series(std).rolling(window = window).mean())\n",
    "    plt.figure()\n",
    "    index = np.arange(len(rol_mean))\n",
    "    plt.plot(index, rol_mean)\n",
    "    plt.fill_between(index, rol_mean-std_coef*rol_std, rol_mean+std_coef*rol_std, color='b', alpha=0.1)\n",
    "    return max(rol_mean[0:results_over]), rol_mean[0:results_over].mean(), pd.DataFrame(data = res_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsNAzDMPUOSz0DMMwdc7/Bz3ViaS6IdFrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13669b850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comb_max, comb_auc, comb_df = plot_results(res, 100, 0.2)\n",
    "plt.savefig(\"comb_cp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212.204 82.749782\n"
     ]
    }
   ],
   "source": [
    "print comb_max, comb_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_df.to_csv(\"comb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "0bf54e92f7d74614b082327634968034": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "24f2d6ee12a5475e97d8d4a97cb46ddd": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "33c226670ede431e992c1021a507e39c": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "428b3ab0fb7c4fdb98728c68072d4938": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "569b005f440b4f9386040956ed1d67dd": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "61544b63fdb14b21a224be922fc12230": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "62a854fdff8548caa88af66e1503a407": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "633ca4b07d5948f5b96562cf90e02ac1": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "694af01b36ea42478fc9e5acb55e4296": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "6dedc44158db44f382def77791698e7b": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "8c56778e71c74fd5a2457a08fb405436": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "aaf62282363e47a79905cebfa2d32b3c": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "ae22b4e2b5d3456d8520aa1b305f62a7": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "b366adf412fb4e3aa6f4db823167c3d2": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "bf616c1d0c3047abac0b067a89c115d9": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "c18f922b63304242871f81df87d63894": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "c7b969932d9d43ddad9f976b6c3f4230": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "cfb956b891a74578b6ee604db7eb12a8": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "d5c62a97602a4bf8b0a78000a980cee3": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "dd8885d3779c4827bfaa250276024e6b": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "e9990c1927d845edbf74d1843bbd09c2": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "fa0c58c904d04b7e9dacf20c27e6060b": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "fcb7e9fbe447411a96af4fee8fd4db19": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
