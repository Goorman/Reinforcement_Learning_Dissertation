{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ATARI_wrapper():\n",
    "    def __init__(self, gamename = \"Enduro-v0\"):\n",
    "        self.state_size = (105, 80)\n",
    "        self.game_title = gamename\n",
    "        self.actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        grayimage = np.mean(state, axis = 2)\n",
    "        downscale = self.downscale2x(grayimage)\n",
    "        norm = (downscale - 128.0) / 128.0\n",
    "        return norm\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3\n",
    "    \n",
    "    def downscale2x(self, image):\n",
    "        image00 = image[0::2, 0::2]\n",
    "        image01 = image[0::2, 1::2]\n",
    "        image10 = image[1::2, 0::2]\n",
    "        image11 = image[1::2, 1::2]\n",
    "        return (image00 + image01 + image10 + image11) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, \n",
    "                 encodeunits = 10, encodenoise = 0.1, aelosscoef = 0.1, regaelosscoef = 2.0):\n",
    "        self.input_var = T.tensor4('statebatch')\n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.newstates = T.tensor4(\"newstatebatch\")\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.aelosscoef = aelosscoef\n",
    "        self.regaelosscoef = regaelosscoef\n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape, encodeunits, encodenoise)\n",
    "        self.build_AVQ(grad_clipping, lr)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape, encodeunits, encodenoise):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.encode = lasagne.layers.DenseLayer(self.outlayer, encodeunits, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "        self.encodenoise = lasagne.layers.GaussianNoiseLayer(self.encode, sigma = encodenoise)\n",
    "        self.le3 = lasagne.layers.DenseLayer(self.encodenoise, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le2 = lasagne.layers.DenseLayer(self.le3, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le1 = lasagne.layers.DenseLayer(self.le2, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le0 = lasagne.layers.DenseLayer(self.le1, channels_number * image_shape[0] * image_shape[1])\n",
    "        self.l_aeout = lasagne.layers.ReshapeLayer(self.le0, shape=(-1, channels_number, image_shape[0], image_shape[1]))\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        \n",
    "        self.advantage, self.value, self.ae_out, self.enc = lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_aeout, self.encode])\n",
    "        \n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Qout = self.advantage + self.value - self.average_advantage\n",
    "        self.Qout = self.advantage\n",
    "        self.predict = T.argmax(self.Qout, axis = 1)\n",
    "        \n",
    "        self.Q = T.sum(self.Qout * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.regae_error = 0.25 - T.mean(T.sqr(self.enc - 0.5))\n",
    "        self.ae_error = T.mean(T.sqr(self.ae_out - self.input_var))\n",
    "        self.td_error = T.mean(T.sqr(self.targetQ - self.Q))\n",
    "        \n",
    "        self.loss = self.ae_error * self.aelosscoef + self.regaelosscoef * self.regae_error + self.td_error\n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "\n",
    "        enc_layers = [self.l2, self.l3, self.outlayer, self.encode, self.le3, self.le2, self.le1, self.le0]\n",
    "        enc_params = [l.W for l in enc_layers] + [l.b for l in enc_layers]\n",
    "        self.enc_grads = T.grad(self.ae_error, enc_params)\n",
    "        self.enc_scaled_grads = lasagne.updates.total_norm_constraint(self.enc_grads, grad_clipping)\n",
    "        self.enc_updates = lasagne.updates.adam(self.enc_scaled_grads, enc_params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Qout)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.train_fn = theano.function([self.input_var, self.targetQ, self.actions], [self.ae_error, self.td_error], updates = self.updates)\n",
    "        self.train_encoder = theano.function([self.input_var], self.ae_error, updates = self.enc_updates)\n",
    "        self.get_encode = theano.function([self.input_var], [self.enc, self.regae_error])\n",
    "        \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values(self.l_advantage, values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.buffer))\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.encode_unitnum = lparams[\"encode_unitnum\"]\n",
    "        self.encode_noise = lparams[\"encode_noise\"]\n",
    "        self.encode_reward_multiplier = lparams[\"encode_reward_multiplier\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "\n",
    "        \n",
    "        self.env = env\n",
    "        AVQ_params = [self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, \n",
    "                      self.lr, self.encode_unitnum, self.encode_noise]\n",
    "        self.mainQN = AVQ_nn(*AVQ_params)\n",
    "        self.targetQN = AVQ_nn(*AVQ_params)\n",
    "\n",
    "        self.lList = []\n",
    "        self.rList = []\n",
    "        self.aeList = []\n",
    "        self.encode_counts = defaultdict(int)\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            agent = egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            agent = boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "            \n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "        \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "            episode_aeerrors = np.array([])\n",
    "            \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                experience = np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5])\n",
    "                self.experience_storage.add(experience)\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        \n",
    "                        encodes, reg_errors = self.mainQN.get_encode(old_state_batch)\n",
    "                        encodes = [\"\".join([str(k) for k in encode]) for encode in (encodes > 0.5).astype(int)]\n",
    "                        for encode in encodes:\n",
    "                            self.encode_counts[encode] += 1\n",
    "                        encode_rewards = self.encode_reward_multiplier / np.sqrt(np.array([self.encode_counts[encode] for encode in encodes]))\n",
    "                        rewards_vector = rewards_vector + encode_rewards \n",
    "                        \n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)\n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        \n",
    "                        doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        aeerror, tderror = self.mainQN.train_fn(old_state_batch, targetQ, action_vector)\n",
    "                        episode_aeerrors = np.append(episode_aeerrors, aeerror)\n",
    "                else:\n",
    "                    train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                    old_state_batch = np.stack(train_batch[:,0])\n",
    "                    new_state_batch = np.stack(train_batch[:,3])\n",
    "                    action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                    aeerror1 = self.mainQN.train_encoder(old_state_batch)\n",
    "                    aeerror2 = self.mainQN.train_encoder(new_state_batch)\n",
    "                    episode_aeerrors = np.append(episode_aeerrors, (aeerror1 + aeerror2)/2)\n",
    "                    \n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            total_aeerror = np.mean(episode_aeerrors)\n",
    "            self.lList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            self.aeList.append(total_aeerror)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.lList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"AEerror, mean over last 10:\", str(np.mean(self.aeList[-10:]))]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.lList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-14 15:28:32,345] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "#llenv = LunarLanding_wrapper()\n",
    "cp_env = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 3,\n",
    "           \"encode_unitnum\": 10,\n",
    "           \"encode_noise\": 0.05,\n",
    "           \"encode_reward_multiplier\": 10.0,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 500,\n",
    "           \"pretrain_steps\" : 5000,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.06,\n",
    "                                \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql = DDQL(lparams, cpenv, egreedyagentinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddql.train(num_episodes = 3000, frame_limit = 500, render = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_rewards(ddql, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм с учитыванием частоты посещения состояний. Подсчет посещения состояний определяется с использованием автоэнкодера, в котором все нейроны центрального слоя (с сигмоидной функцией активации) регуляризуются для помещения значений либо у нуля, либо у единицы. Эта регуляризация осуществляется с помощью изменения оптимизируемой функции потерь и с помощью добавления слоя гауссового шума после центрального слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стабильность подобной кодировки можно ожидать из-за того, что сигмоидные нейроны быстро насыщаются, предотвращая значительные их колебания во время динамического обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждому действию агента добавляется дополнительная награда, причем она обратно пропорциональна количеству раз, которые агент бывал в этом состоянии(если точнее, то количеству раз, которые агент бывал в состояниях, имеющих ту же бинарную кодировку автоэнкодера, что и текущее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_experiment(ddql, ddql_init_params, ddql_train_params, experiment_num = 5):\n",
    "    ddql_list = [ddql(**ddql_init_params) for k in range(experiment_num)]\n",
    "    \n",
    "    for k in range(experiment_num):\n",
    "        ddql_list[k].train(**ddql_train_params)\n",
    "        \n",
    "    return ddql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql_egreedy_params = {\"lparams\":lparams, \"env\":cp_env, \"agent\":egreedyagentinfo}\n",
    "\n",
    "ddql_train_params = {\"num_episodes\":2500, \"frame_limit\":500, \"render\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 499 ================================================\n",
      "Total steps: 35336\n",
      "Episode rewards, last 10: [103.0, 45.0, 33.0, 17.0, 36.0, 25.0, 27.0, 34.0, 500.0, 129.0]\n",
      "Mean over last 500 episodes: 80.672\n",
      "Episode lengths, last 10: [102, 44, 32, 16, 35, 24, 26, 33, 499, 128]\n",
      "AEerror, mean over last 10: 0.404199413493\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 125882\n",
      "Episode rewards, last 10: [55.0, 39.0, 9.0, 29.0, 28.0, 14.0, 19.0, 265.0, 32.0, 13.0]\n",
      "Mean over last 500 episodes: 181.092\n",
      "Episode lengths, last 10: [54, 38, 8, 28, 27, 13, 18, 264, 31, 12]\n",
      "AEerror, mean over last 10: 0.416016125851\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 215340\n",
      "Episode rewards, last 10: [472.0, 274.0, 271.0, 14.0, 10.0, 27.0, 193.0, 187.0, 159.0, 139.0]\n",
      "Mean over last 500 episodes: 178.916\n",
      "Episode lengths, last 10: [471, 273, 270, 13, 9, 26, 192, 186, 158, 138]\n",
      "AEerror, mean over last 10: 0.417729565599\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 310170\n",
      "Episode rewards, last 10: [61.0, 54.0, 12.0, 27.0, 38.0, 73.0, 37.0, 57.0, 84.0, 234.0]\n",
      "Mean over last 500 episodes: 189.66\n",
      "Episode lengths, last 10: [60, 53, 11, 26, 37, 72, 36, 56, 83, 233]\n",
      "AEerror, mean over last 10: 0.465546069152\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 394088\n",
      "Episode rewards, last 10: [84.0, 90.0, 91.0, 107.0, 112.0, 224.0, 23.0, 44.0, 42.0, 133.0]\n",
      "Mean over last 500 episodes: 167.836\n",
      "Episode lengths, last 10: [83, 89, 90, 106, 111, 223, 22, 43, 41, 132]\n",
      "AEerror, mean over last 10: 0.444121031196\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 15738\n",
      "Episode rewards, last 10: [123.0, 106.0, 22.0, 114.0, 175.0, 500.0, 339.0, 142.0, 358.0, 162.0]\n",
      "Mean over last 500 episodes: 41.476\n",
      "Episode lengths, last 10: [122, 105, 21, 113, 174, 499, 338, 141, 357, 161]\n",
      "AEerror, mean over last 10: 0.530862295438\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 92176\n",
      "Episode rewards, last 10: [13.0, 23.0, 13.0, 33.0, 37.0, 129.0, 208.0, 128.0, 102.0, 48.0]\n",
      "Mean over last 500 episodes: 152.876\n",
      "Episode lengths, last 10: [12, 22, 12, 32, 36, 128, 207, 127, 101, 47]\n",
      "AEerror, mean over last 10: 0.359450488389\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 187411\n",
      "Episode rewards, last 10: [12.0, 454.0, 325.0, 158.0, 500.0, 223.0, 12.0, 10.0, 11.0, 31.0]\n",
      "Mean over last 500 episodes: 190.47\n",
      "Episode lengths, last 10: [11, 453, 324, 157, 499, 222, 11, 9, 10, 30]\n",
      "AEerror, mean over last 10: 0.542115798071\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 288376\n",
      "Episode rewards, last 10: [107.0, 108.0, 101.0, 100.0, 38.0, 92.0, 30.0, 215.0, 108.0, 111.0]\n",
      "Mean over last 500 episodes: 201.93\n",
      "Episode lengths, last 10: [106, 107, 100, 99, 37, 91, 29, 214, 107, 110]\n",
      "AEerror, mean over last 10: 0.637536149998\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 364797\n",
      "Episode rewards, last 10: [15.0, 19.0, 23.0, 22.0, 62.0, 54.0, 48.0, 29.0, 48.0, 52.0]\n",
      "Mean over last 500 episodes: 152.842\n",
      "Episode lengths, last 10: [14, 18, 22, 21, 61, 53, 47, 28, 47, 51]\n",
      "AEerror, mean over last 10: 0.7223323535\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 13355\n",
      "Episode rewards, last 10: [234.0, 151.0, 134.0, 147.0, 143.0, 133.0, 144.0, 136.0, 151.0, 96.0]\n",
      "Mean over last 500 episodes: 36.71\n",
      "Episode lengths, last 10: [233, 150, 133, 146, 142, 132, 143, 135, 150, 95]\n",
      "AEerror, mean over last 10: 0.539584894612\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 121029\n",
      "Episode rewards, last 10: [170.0, 12.0, 158.0, 131.0, 137.0, 118.0, 143.0, 139.0, 279.0, 187.0]\n",
      "Mean over last 500 episodes: 215.348\n",
      "Episode lengths, last 10: [169, 11, 157, 130, 136, 117, 142, 138, 278, 186]\n",
      "AEerror, mean over last 10: 0.606965190474\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 240272\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 13.0, 288.0, 500.0]\n",
      "Mean over last 500 episodes: 238.486\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 499, 499, 12, 287, 499]\n",
      "AEerror, mean over last 10: 0.863330352484\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 361519\n",
      "Episode rewards, last 10: [147.0, 96.0, 17.0, 28.0, 19.0, 17.0, 55.0, 23.0, 31.0, 133.0]\n",
      "Mean over last 500 episodes: 242.494\n",
      "Episode lengths, last 10: [146, 95, 16, 27, 18, 16, 54, 22, 30, 132]\n",
      "AEerror, mean over last 10: 0.486910263663\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 466788\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 142.0, 55.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 210.538\n",
      "Episode lengths, last 10: [499, 499, 499, 141, 54, 499, 499, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.138697876017\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 33935\n",
      "Episode rewards, last 10: [453.0, 78.0, 67.0, 40.0, 12.0, 32.0, 66.0, 78.0, 127.0, 47.0]\n",
      "Mean over last 500 episodes: 77.87\n",
      "Episode lengths, last 10: [452, 77, 66, 39, 11, 31, 65, 77, 126, 46]\n",
      "AEerror, mean over last 10: 0.424880858196\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 124282\n",
      "Episode rewards, last 10: [15.0, 24.0, 23.0, 13.0, 89.0, 131.0, 267.0, 14.0, 135.0, 332.0]\n",
      "Mean over last 500 episodes: 180.694\n",
      "Episode lengths, last 10: [14, 23, 22, 12, 88, 130, 266, 13, 134, 331]\n",
      "AEerror, mean over last 10: 0.258375927224\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 221329\n",
      "Episode rewards, last 10: [22.0, 11.0, 43.0, 500.0, 500.0, 500.0, 500.0, 500.0, 425.0, 164.0]\n",
      "Mean over last 500 episodes: 194.094\n",
      "Episode lengths, last 10: [21, 10, 42, 499, 499, 499, 499, 499, 424, 163]\n",
      "AEerror, mean over last 10: 0.25284128148\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 326581\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 262.0, 500.0, 500.0, 500.0, 443.0, 12.0]\n",
      "Mean over last 500 episodes: 210.504\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 261, 499, 499, 499, 442, 11]\n",
      "AEerror, mean over last 10: 0.247385155371\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 434541\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 95.0, 48.0, 437.0, 117.0, 302.0]\n",
      "Mean over last 500 episodes: 215.92\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 94, 47, 436, 116, 301]\n",
      "AEerror, mean over last 10: 0.153163839293\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 1682\n",
      "Episode rewards, last 10: [21.0, 10.0, 13.0, 11.0, 18.0, 10.0, 15.0, 11.0, 15.0, 11.0]\n",
      "Mean over last 500 episodes: 13.364\n",
      "Episode lengths, last 10: [20, 9, 12, 10, 17, 9, 14, 10, 14, 10]\n",
      "AEerror, mean over last 10: 0.398035971468\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 81505\n",
      "Episode rewards, last 10: [140.0, 127.0, 160.0, 159.0, 160.0, 131.0, 128.0, 134.0, 178.0, 113.0]\n",
      "Mean over last 500 episodes: 159.646\n",
      "Episode lengths, last 10: [139, 126, 159, 158, 159, 130, 127, 133, 177, 112]\n",
      "AEerror, mean over last 10: 0.705770714733\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 180223\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 475.0, 50.0, 45.0, 500.0, 20.0]\n",
      "Mean over last 500 episodes: 197.436\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 474, 49, 44, 499, 19]\n",
      "AEerror, mean over last 10: 0.582751176062\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 276743\n",
      "Episode rewards, last 10: [34.0, 89.0, 36.0, 93.0, 275.0, 107.0, 110.0, 107.0, 113.0, 162.0]\n",
      "Mean over last 500 episodes: 193.04\n",
      "Episode lengths, last 10: [33, 88, 35, 92, 274, 106, 109, 106, 112, 161]\n",
      "AEerror, mean over last 10: 0.536227975675\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 391596\n",
      "Episode rewards, last 10: [227.0, 500.0, 345.0, 500.0, 500.0, 475.0, 11.0, 500.0, 19.0, 41.0]\n",
      "Mean over last 500 episodes: 229.706\n",
      "Episode lengths, last 10: [226, 499, 344, 499, 499, 474, 10, 499, 18, 40]\n",
      "AEerror, mean over last 10: 0.632972288491\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 15490\n",
      "Episode rewards, last 10: [71.0, 12.0, 10.0, 17.0, 11.0, 41.0, 11.0, 11.0, 65.0, 111.0]\n",
      "Mean over last 500 episodes: 40.98\n",
      "Episode lengths, last 10: [70, 11, 9, 16, 10, 40, 10, 10, 64, 110]\n",
      "AEerror, mean over last 10: 0.505180539437\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 122670\n",
      "Episode rewards, last 10: [50.0, 500.0, 500.0, 146.0, 19.0, 44.0, 13.0, 30.0, 33.0, 169.0]\n",
      "Mean over last 500 episodes: 214.36\n",
      "Episode lengths, last 10: [49, 499, 499, 145, 18, 43, 12, 29, 32, 168]\n",
      "AEerror, mean over last 10: 0.509074165834\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 233980\n",
      "Episode rewards, last 10: [139.0, 135.0, 121.0, 143.0, 121.0, 125.0, 55.0, 55.0, 12.0, 36.0]\n",
      "Mean over last 500 episodes: 222.62\n",
      "Episode lengths, last 10: [138, 134, 120, 142, 120, 124, 54, 54, 11, 35]\n",
      "AEerror, mean over last 10: 0.781456165398\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 329625\n",
      "Episode rewards, last 10: [57.0, 59.0, 37.0, 46.0, 249.0, 54.0, 47.0, 202.0, 213.0, 203.0]\n",
      "Mean over last 500 episodes: 191.29\n",
      "Episode lengths, last 10: [56, 58, 36, 45, 248, 53, 46, 201, 212, 202]\n",
      "AEerror, mean over last 10: 0.166522028954\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 422954\n",
      "Episode rewards, last 10: [467.0, 84.0, 188.0, 54.0, 201.0, 189.0, 151.0, 132.0, 112.0, 104.0]\n",
      "Mean over last 500 episodes: 186.658\n",
      "Episode lengths, last 10: [466, 83, 187, 53, 200, 188, 150, 131, 111, 103]\n",
      "AEerror, mean over last 10: 0.423341667352\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 44569\n",
      "Episode rewards, last 10: [142.0, 283.0, 500.0, 500.0, 500.0, 500.0, 500.0, 500.0, 390.0, 47.0]\n",
      "Mean over last 500 episodes: 99.138\n",
      "Episode lengths, last 10: [141, 282, 499, 499, 499, 499, 499, 499, 389, 46]\n",
      "AEerror, mean over last 10: 0.264565156721\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 136906\n",
      "Episode rewards, last 10: [457.0, 308.0, 91.0, 69.0, 92.0, 500.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 184.674\n",
      "Episode lengths, last 10: [456, 307, 90, 68, 91, 499, 499, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.309244077256\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 222359\n",
      "Episode rewards, last 10: [10.0, 25.0, 72.0, 66.0, 500.0, 500.0, 203.0, 500.0, 103.0, 500.0]\n",
      "Mean over last 500 episodes: 170.906\n",
      "Episode lengths, last 10: [9, 24, 71, 65, 499, 499, 202, 499, 102, 499]\n",
      "AEerror, mean over last 10: 0.291188497243\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 314659\n",
      "Episode rewards, last 10: [92.0, 15.0, 18.0, 13.0, 14.0, 108.0, 122.0, 98.0, 137.0, 132.0]\n",
      "Mean over last 500 episodes: 184.6\n",
      "Episode lengths, last 10: [91, 14, 17, 12, 13, 107, 121, 97, 136, 131]\n",
      "AEerror, mean over last 10: 0.338505266073\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 420500\n",
      "Episode rewards, last 10: [102.0, 147.0, 121.0, 97.0, 106.0, 94.0, 111.0, 96.0, 214.0, 13.0]\n",
      "Mean over last 500 episodes: 211.682\n",
      "Episode lengths, last 10: [101, 146, 120, 96, 105, 93, 110, 95, 213, 12]\n",
      "AEerror, mean over last 10: 0.273338757157\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 2052\n",
      "Episode rewards, last 10: [12.0, 25.0, 19.0, 14.0, 12.0, 11.0, 9.0, 11.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 14.104\n",
      "Episode lengths, last 10: [11, 24, 18, 13, 11, 10, 8, 10, 9, 9]\n",
      "AEerror, mean over last 10: 0.363369532646\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 7712\n",
      "Episode rewards, last 10: [10.0, 12.0, 10.0, 11.0, 11.0, 8.0, 12.0, 10.0, 9.0, 10.0]\n",
      "Mean over last 500 episodes: 11.32\n",
      "Episode lengths, last 10: [9, 11, 9, 10, 10, 7, 11, 9, 8, 9]\n",
      "AEerror, mean over last 10: 0.451841705569\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 12606\n",
      "Episode rewards, last 10: [10.0, 10.0, 10.0, 8.0, 10.0, 10.0, 10.0, 9.0, 10.0, 8.0]\n",
      "Mean over last 500 episodes: 9.788\n",
      "Episode lengths, last 10: [9, 9, 9, 7, 9, 9, 9, 8, 9, 7]\n",
      "AEerror, mean over last 10: 0.489530083577\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 17437\n",
      "Episode rewards, last 10: [9.0, 9.0, 11.0, 10.0, 11.0, 10.0, 9.0, 11.0, 9.0, 11.0]\n",
      "Mean over last 500 episodes: 9.662\n",
      "Episode lengths, last 10: [8, 8, 10, 9, 10, 9, 8, 10, 8, 10]\n",
      "AEerror, mean over last 10: 0.54935266542\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 22264\n",
      "Episode rewards, last 10: [11.0, 8.0, 10.0, 9.0, 8.0, 9.0, 10.0, 9.0, 9.0, 9.0]\n",
      "Mean over last 500 episodes: 9.654\n",
      "Episode lengths, last 10: [10, 7, 9, 8, 7, 8, 9, 8, 8, 8]\n",
      "AEerror, mean over last 10: 0.576293698092\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 9525\n",
      "Episode rewards, last 10: [130.0, 385.0, 454.0, 500.0, 129.0, 410.0, 10.0, 24.0, 23.0, 25.0]\n",
      "Mean over last 500 episodes: 29.05\n",
      "Episode lengths, last 10: [129, 384, 453, 499, 128, 409, 9, 23, 22, 24]\n",
      "AEerror, mean over last 10: 0.557928202287\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 100982\n",
      "Episode rewards, last 10: [13.0, 14.0, 28.0, 16.0, 53.0, 500.0, 500.0, 311.0, 13.0, 31.0]\n",
      "Mean over last 500 episodes: 182.914\n",
      "Episode lengths, last 10: [12, 13, 27, 15, 52, 499, 499, 310, 12, 30]\n",
      "AEerror, mean over last 10: 0.486026669688\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 196511\n",
      "Episode rewards, last 10: [72.0, 89.0, 68.0, 60.0, 49.0, 74.0, 170.0, 498.0, 297.0, 35.0]\n",
      "Mean over last 500 episodes: 191.058\n",
      "Episode lengths, last 10: [71, 88, 67, 59, 48, 73, 169, 497, 296, 34]\n",
      "AEerror, mean over last 10: 0.252968771802\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 285238\n",
      "Episode rewards, last 10: [65.0, 12.0, 175.0, 155.0, 500.0, 500.0, 500.0, 500.0, 74.0, 36.0]\n",
      "Mean over last 500 episodes: 177.454\n",
      "Episode lengths, last 10: [64, 11, 174, 154, 499, 499, 499, 499, 73, 35]\n",
      "AEerror, mean over last 10: 0.357536982445\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 393170\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 165.0, 52.0, 500.0, 14.0, 32.0, 17.0, 76.0]\n",
      "Mean over last 500 episodes: 215.864\n",
      "Episode lengths, last 10: [499, 499, 499, 164, 51, 499, 13, 31, 16, 75]\n",
      "AEerror, mean over last 10: 0.234510362071\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 2699\n",
      "Episode rewards, last 10: [10.0, 14.0, 11.0, 12.0, 10.0, 21.0, 10.0, 15.0, 10.0, 18.0]\n",
      "Mean over last 500 episodes: 15.398\n",
      "Episode lengths, last 10: [9, 13, 10, 11, 9, 20, 9, 14, 9, 17]\n",
      "AEerror, mean over last 10: 0.30812881231\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 8194\n",
      "Episode rewards, last 10: [11.0, 11.0, 15.0, 11.0, 10.0, 12.0, 11.0, 9.0, 9.0, 10.0]\n",
      "Mean over last 500 episodes: 10.99\n",
      "Episode lengths, last 10: [10, 10, 14, 10, 9, 11, 10, 8, 8, 9]\n",
      "AEerror, mean over last 10: 0.434554755996\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 13031\n",
      "Episode rewards, last 10: [10.0, 11.0, 9.0, 9.0, 10.0, 9.0, 11.0, 8.0, 10.0, 9.0]\n",
      "Mean over last 500 episodes: 9.674\n",
      "Episode lengths, last 10: [9, 10, 8, 8, 9, 8, 10, 7, 9, 8]\n",
      "AEerror, mean over last 10: 0.520789124306\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 17839\n",
      "Episode rewards, last 10: [9.0, 11.0, 10.0, 10.0, 9.0, 9.0, 10.0, 10.0, 10.0, 9.0]\n",
      "Mean over last 500 episodes: 9.616\n",
      "Episode lengths, last 10: [8, 10, 9, 9, 8, 8, 9, 9, 9, 8]\n",
      "AEerror, mean over last 10: 0.520676481146\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 22712\n",
      "Episode rewards, last 10: [9.0, 9.0, 10.0, 9.0, 10.0, 9.0, 9.0, 10.0, 8.0, 10.0]\n",
      "Mean over last 500 episodes: 9.746\n",
      "Episode lengths, last 10: [8, 8, 9, 8, 9, 8, 8, 9, 7, 9]\n",
      "AEerror, mean over last 10: 0.522928088506\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = produce_experiment(DDQL, ddql_egreedy_params, ddql_train_params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(ddql_res, window = 100, std_coef = 0.2, results_over = 1000):\n",
    "    res_lists = [k.rList for k in ddql_res]\n",
    "    res_lists = np.array(res_lists)\n",
    "    pd.DataFrame(data = res_lists)\n",
    "    mean = res_lists.mean(axis = 0)\n",
    "    std = res_lists.std(axis = 0)\n",
    "    rol_mean = np.nan_to_num(pd.Series(mean).rolling(window = window).mean())\n",
    "    rol_std = np.nan_to_num(pd.Series(std).rolling(window = window).mean())\n",
    "    plt.figure()\n",
    "    index = np.arange(len(rol_mean))\n",
    "    plt.plot(index, rol_mean)\n",
    "    plt.fill_between(index, rol_mean-std_coef*rol_std, rol_mean+std_coef*rol_std, color='b', alpha=0.1)\n",
    "    return max(rol_mean[0:results_over]), rol_mean[0:results_over].mean(), pd.DataFrame(data = res_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsNAzDMMsc1joGYZhljn/Pw8oEnq2qzlgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13cddd290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_max, count_auc, count_df = plot_results(res, 100, 0.2)\n",
    "plt.savefig(\"count_cp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176.103 88.535724\n"
     ]
    }
   ],
   "source": [
    "print count_max, count_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_df.to_csv(\"count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "276c8eb4010e4cdb97dd3c2a13417ffd": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "41121df64b994a7bbf4d9ddb8bcc4c70": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "632370d7f90340fcb545a4c09ca67dea": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "98986e1fd57c4bef8a2f5c5598e738bb": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "a0dc0bad943f417f92eb7c47b35ee9ad": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "a8fd011534a74c0c98b1e3064ad7a5e8": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "c7ce587cd4054a59a5f938ace833d9da": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "c95c8a7d425a41e49478da7843321900": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "f7bd652c80d64145a9d8c77dd1dc74ea": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    },
    "fe5346e309e4442489d74c1caad41be3": {
     "views": [
      {
       "cell_index": 29
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
