{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ATARI_wrapper():\n",
    "    def __init__(self, gamename = \"Enduro-v0\"):\n",
    "        self.state_size = (105, 80)\n",
    "        self.game_title = gamename\n",
    "        self.actions = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        grayimage = np.mean(state, axis = 2)\n",
    "        downscale = self.downscale2x(grayimage)\n",
    "        norm = (downscale - 128.0) / 128.0\n",
    "        return norm\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3\n",
    "    \n",
    "    def downscale2x(self, image):\n",
    "        image00 = image[0::2, 0::2]\n",
    "        image01 = image[0::2, 1::2]\n",
    "        image10 = image[1::2, 0::2]\n",
    "        image11 = image[1::2, 1::2]\n",
    "        return (image00 + image01 + image10 + image11) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LunarLanding_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 8)\n",
    "        self.game_title = \"LunarLander-v2\"\n",
    "        self.actions = [\"DO_NOTHING\", \"FIRE_LEFT\", \"FIRE_MAIN\", \"FIRE_RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CartPole_wrapper():\n",
    "    def __init__(self):\n",
    "        self.state_size = (1, 4)\n",
    "        self.game_title = \"CartPole-v0\"\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "        self.n_actions = len(self.actions)\n",
    "        try:\n",
    "            self.env = gym.make(self.game_title)\n",
    "        except:\n",
    "            print (\"ERROR : Can't find \" + self.game_title + \" environment.\")\n",
    "            return None\n",
    "        self.env.reset()\n",
    "    \n",
    "    def processState(self, state):\n",
    "        return state.reshape(1, -1)\n",
    "    \n",
    "    def processAction(self, action):\n",
    "        return action\n",
    "    \n",
    "    def make_reset(self):\n",
    "        state = self.env.reset()\n",
    "    \n",
    "        return self.processState(state)\n",
    "    \n",
    "    def make_step(self, action, render = False):\n",
    "        action = self.processAction(action)\n",
    "    \n",
    "        state, ret1, ret2, ret3 = self.env.step(action)\n",
    "    \n",
    "        if render:\n",
    "            self.env.render()\n",
    "    \n",
    "        return self.processState(state), ret1, ret2, ret3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AVQ_nn():\n",
    "    def __init__(self, channels_number = 4, image_shape = (1, 8), n_actions = 4, grad_clipping = 10, lr = 0.0001, aelosscoef = 0.1):\n",
    "        self.input_var = T.tensor4('statebatch')\n",
    "        self.targetQ = T.fvector('targetQ')\n",
    "        self.actions = T.ivector('actions')\n",
    "        self.newstates = T.tensor4(\"newstatebatch\")\n",
    "        self.actions_onehot = T.extra_ops.to_one_hot(self.actions, n_actions, dtype=np.float32)\n",
    "        \n",
    "        self.aelosscoef = aelosscoef\n",
    "        self.n_actions = n_actions\n",
    "        self.build_network(channels_number, image_shape)\n",
    "        self.build_AVQ(grad_clipping, lr)\n",
    "        self.compile_network()\n",
    "        \n",
    "    def build_network(self, channels_number, image_shape):\n",
    "        self.l1 = lasagne.layers.InputLayer(shape=(None, channels_number, image_shape[0], image_shape[1]), \n",
    "                                            input_var = self.input_var)\n",
    "        self.l2 = lasagne.layers.DenseLayer(self.l1, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.l3 = lasagne.layers.DenseLayer(self.l2, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.outlayer = lasagne.layers.DenseLayer(self.l3, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.encode = lasagne.layers.DenseLayer(self.outlayer, 4, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le3 = lasagne.layers.DenseLayer(self.encode, 20, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le2 = lasagne.layers.DenseLayer(self.le3, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le1 = lasagne.layers.DenseLayer(self.le2, 60, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.le0 = lasagne.layers.DenseLayer(self.le1, channels_number * image_shape[0] * image_shape[1])\n",
    "        self.l_aeout = lasagne.layers.ReshapeLayer(self.le0, shape=(-1, channels_number, image_shape[0], image_shape[1]))\n",
    "    \n",
    "        self.actionlayer = lasagne.layers.InputLayer(shape=(None, self.n_actions), input_var = self.actions_onehot)\n",
    "        self.prins = lasagne.layers.ConcatLayer([self.encode, self.actionlayer], axis = 1)\n",
    "        self.pr1 = lasagne.layers.DenseLayer(self.prins, 40, nonlinearity=lasagne.nonlinearities.leaky_rectify)\n",
    "        self.prencode = lasagne.layers.DenseLayer(self.pr1, 4)\n",
    "    \n",
    "    def build_AVQ(self, grad_clipping, lr):\n",
    "        self.l_advantage = lasagne.layers.DenseLayer(self.outlayer, self.n_actions)\n",
    "        self.l_value = lasagne.layers.DenseLayer(self.outlayer, 1)\n",
    "        \n",
    "        self.advantage, self.value, self.ae_out, self.enc, self.prenc = lasagne.layers.get_output([self.l_advantage, self.l_value, self.l_aeout, self.encode, self.prencode])\n",
    "        self.targetenc = lasagne.layers.get_output(self.encode, inputs = self.newstates)\n",
    "        \n",
    "        self.average_advantage = T.mean(self.advantage, keepdims = True, axis = 1)\n",
    "        \n",
    "#        self.Qout = self.advantage + self.value - self.average_advantage\n",
    "        self.Qout = self.advantage\n",
    "        self.predict = T.argmax(self.Qout, axis = 1)\n",
    "        \n",
    "        self.Q = T.sum(self.Qout * self.actions_onehot, axis = 1)\n",
    "        \n",
    "        self.ae_error = T.mean(T.sqr(self.ae_out - self.input_var))\n",
    "        self.td_error = T.mean(T.sqr(self.targetQ - self.Q))\n",
    "        \n",
    "        self.loss = self.ae_error * self.aelosscoef + self.td_error\n",
    "        params = self.get_all_params()\n",
    "        self.all_grads = T.grad(self.loss, params)\n",
    "        self.scaled_grads = lasagne.updates.total_norm_constraint(self.all_grads, grad_clipping)\n",
    "        self.updates = lasagne.updates.adam(self.scaled_grads, params, learning_rate=lr)\n",
    "        \n",
    "        self.pr_loss_batch = T.mean(T.sqr(self.targetenc - self.prenc), axis = 1)\n",
    "        self.pr_loss = T.mean(self.pr_loss_batch)\n",
    "        pr_params = [self.pr1.W, self.pr1.b, self.prencode.W, self.prencode.b]\n",
    "        self.pr_grads = T.grad(self.pr_loss, pr_params)\n",
    "        self.pr_scaled_grads = lasagne.updates.total_norm_constraint(self.pr_grads, grad_clipping)\n",
    "        self.pr_updates = lasagne.updates.adam(self.pr_scaled_grads, pr_params, learning_rate=lr)\n",
    "        \n",
    "        enc_layers = [self.l2, self.l3, self.outlayer, self.encode, self.le3, self.le2, self.le1, self.le0]\n",
    "        enc_params = [l.W for l in enc_layers] + [l.b for l in enc_layers]\n",
    "        self.enc_grads = T.grad(self.ae_error, enc_params)\n",
    "        self.enc_scaled_grads = lasagne.updates.total_norm_constraint(self.enc_grads, grad_clipping)\n",
    "        self.enc_updates = lasagne.updates.adam(self.enc_scaled_grads, enc_params, learning_rate=lr)\n",
    "        \n",
    "    def compile_network(self):\n",
    "        self.Qout_fn = theano.function([self.input_var], self.Qout)\n",
    "        self.actionpred_fn = theano.function([self.input_var], self.predict)\n",
    "        self.train_fn = theano.function([self.input_var, self.targetQ, self.actions], [self.ae_error, self.td_error], updates = self.updates)\n",
    "        self.train_predfn = theano.function([self.input_var, self.actions, self.newstates], self.pr_loss_batch, updates = self.pr_updates)\n",
    "        self.train_encoder = theano.function([self.input_var], self.ae_error, updates = self.enc_updates)\n",
    "        \n",
    "    def get_all_params(self):\n",
    "#        return lasagne.layers.get_all_params([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_params(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def get_all_params_values(self):\n",
    "#        return lasagne.layers.get_all_param_values([self.l_advantage, self.l_value], trainable = True)\n",
    "        return lasagne.layers.get_all_param_values(self.l_advantage, trainable = True)\n",
    "    \n",
    "    def set_all_params_values(self, values):\n",
    "#        return lasagne.layers.set_all_param_values([self.l_advantage, self.l_value], values, trainable = True)\n",
    "        return lasagne.layers.set_all_param_values(self.l_advantage, values, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 10000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        size = min(size, len(self.buffer))\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class window_aggregator():\n",
    "    def __init__(self, window_length, state_shape):\n",
    "        self.state_shape = state_shape\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        assert len(self.state_shape) == 2\n",
    "        assert self.window_length >= 1\n",
    "        \n",
    "        self.start_aggregator_shape = (window_length, state_shape[0], state_shape[1])\n",
    "                                             \n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.aggregator = np.zeros(self.start_aggregator_shape)\n",
    "                                       \n",
    "    def add_state(self, state):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        self.aggregator = np.append(self.aggregator, state, axis = 0)\n",
    "    \n",
    "    def get_window(self):\n",
    "        return self.aggregator[-self.window_length:,:,:]                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class egreedy_agent():\n",
    "    def __init__(self, n_actions, actionpred_fn, startE = 1, endE = 0.1, anneling_steps = 50000):\n",
    "        self.startE = startE\n",
    "        self.endE = endE\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.stepE = (self.startE - self.endE) / self.anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.actionpred_fn = actionpred_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        if current_step > self.anneling_steps:\n",
    "            epsilon = self.endE\n",
    "        else:\n",
    "            epsilon = self.startE - self.stepE * current_step\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            a = np.random.randint(0, self.n_actions)\n",
    "        else:\n",
    "            a = self.actionpred_fn(np.expand_dims(state, axis = 0))[0]\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class boltzman_agent:\n",
    "    def __init__(self, n_actions, Qout_fn, startT = 1000, endT = 0.1, anneling_steps = 50000):\n",
    "        self.startT = startT\n",
    "        self.endT = endT\n",
    "        self.anneling_steps = anneling_steps\n",
    "        self.logstep = (np.log(startT) - np.log(endT)) / anneling_steps\n",
    "        self.n_actions = n_actions\n",
    "        self.Qout_fn = Qout_fn\n",
    "    \n",
    "    def choose_action(self, state, current_step):\n",
    "        scores = self.Qout_fn(np.expand_dims(state, axis = 0))[0]\n",
    "        if current_step > self.anneling_steps:\n",
    "            exponents = np.exp((scores - np.max(scores)) / self.endT)\n",
    "        else:\n",
    "            current_temp = self.startT / np.exp(self.logstep * current_step)\n",
    "            exponents = np.exp((scores - np.max(scores)) / current_temp)\n",
    "        probs = exponents / np.sum(exponents)\n",
    "        return np.random.choice(self.n_actions, p = probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DDQL():\n",
    "    def __init__(self, lparams, env, agent = {\"agent\":\"egreedy\", \"params\":{}}):\n",
    "        self.grad_clip = lparams[\"grad_clipping\"]\n",
    "        self.lr = lparams[\"learning_rate\"]\n",
    "        self.window_size = lparams[\"window_size\"]\n",
    "        self.batch_size = lparams[\"batch_size\"]\n",
    "        self.gamma = lparams[\"gamma\"]\n",
    "        self.MQN_updatefreq = lparams[\"MQN_updatefreq\"]\n",
    "        self.TQN_updatefreq = lparams[\"TQN_updatefreq\"]\n",
    "        self.TQN_updaterate = lparams[\"TQN_updaterate\"]\n",
    "        self.print_freq = lparams[\"print_freq\"]\n",
    "        self.pretrain_steps = lparams[\"pretrain_steps\"]\n",
    "        self.buffer_size = lparams[\"buffer_size\"]\n",
    "        self.pretrain_over = False\n",
    "        self.maxprerror = None\n",
    "        \n",
    "        self.env = env\n",
    "        self.mainQN = AVQ_nn(self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, self.lr)\n",
    "        self.targetQN = AVQ_nn(self.window_size, self.env.state_size, self.env.n_actions, self.grad_clip, self.lr)\n",
    "\n",
    "        self.lList = []\n",
    "        self.rList = []\n",
    "        self.aeList = []\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.window = window_aggregator(self.window_size, self.env.state_size)\n",
    "        self.experience_storage = experience_buffer(self.buffer_size)\n",
    "        self.agent = self.getAgent(agent[\"agent\"], agent[\"params\"])\n",
    "            \n",
    "    def getAgent(self, agent, agentparams):\n",
    "        if agent == \"egreedy\":\n",
    "            agent = egreedy_agent(self.env.n_actions, self.mainQN.actionpred_fn, **agentparams)\n",
    "        elif agent == \"boltzman\":\n",
    "            agent = boltzman_agent(self.env.n_actions, self.mainQN.Qout_fn, **agentparams)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown agent\")\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    def updateTarget(self, completeupdate = False):\n",
    "        if completeupdate:\n",
    "            self.targetQN.set_all_params_values(self.mainQN.get_all_params_values())\n",
    "        else:\n",
    "            targetparams = self.targetQN.get_all_params_values()\n",
    "            mainparams = self.mainQN.get_all_params_values()\n",
    "            ur = self.TQN_updaterate\n",
    "        \n",
    "            assert len(targetparams) == len(mainparams)\n",
    "            for k in range(0, len(targetparams)):\n",
    "                targetparams[k] = targetparams[k] * (1.0 - ur) + mainparams[k] * ur\n",
    "        \n",
    "            self.targetQN.set_all_params_values(targetparams)\n",
    "            \n",
    "    def train(self, num_episodes, frame_limit, render = True):\n",
    "        self.updateTarget(True)\n",
    "        self.window.reset()\n",
    "        \n",
    "        for episode_num in tqdm_notebook(range(num_episodes), desc = \"RL train\"):\n",
    "            state = self.env.make_reset()\n",
    "            self.window.add_state(state)\n",
    "            window_state = self.window.get_window()\n",
    "            episode_rewards = np.zeros(frame_limit)\n",
    "            episode_aeerrors = np.array([])\n",
    "            \n",
    "            for iteration in xrange(0, frame_limit):\n",
    "                action = self.agent.choose_action(window_state, self.total_steps)\n",
    "                new_state, reward, gameover, _ = self.env.make_step(action, render)\n",
    "                self.window.add_state(new_state)\n",
    "                new_window_state = self.window.get_window()\n",
    "                experience = np.reshape(np.array([window_state, action, reward, new_window_state, gameover]),[1,5])\n",
    "                self.experience_storage.add(experience)\n",
    "                episode_rewards[iteration] = reward\n",
    "                \n",
    "                if self.pretrain_over:\n",
    "                    self.total_steps += 1\n",
    "                \n",
    "                    if self.total_steps % (self.TQN_updatefreq) == 0:\n",
    "                        self.updateTarget()\n",
    "                \n",
    "                    if self.total_steps % (self.MQN_updatefreq) == 0:\n",
    "                        train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                        old_state_batch = np.stack(train_batch[:,0])\n",
    "                        new_state_batch = np.stack(train_batch[:,3])\n",
    "                        action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                        end_multiplier = -(train_batch[:,4] - 1)\n",
    "                        rewards_vector = train_batch[:,2]\n",
    "                        errors_vector = self.mainQN.train_predfn(old_state_batch, action_vector, new_state_batch)\n",
    "                        if self.maxprerror == None:\n",
    "                            self.maxprerror = np.max(errors_vector)\n",
    "                        else:\n",
    "                            self.maxprerror = max(self.maxprerror, np.max(errors_vector))\n",
    "                        rewards_vector = rewards_vector + errors_vector * 50.0 / (self.maxprerror * self.total_steps)\n",
    "                        Q1 = self.mainQN.actionpred_fn(new_state_batch)\n",
    "                        Q2 = self.targetQN.Qout_fn(new_state_batch)\n",
    "                        \n",
    "                        doubleQ = Q2[range(self.batch_size),Q1]\n",
    "                        targetQ = (rewards_vector + (self.gamma * doubleQ * end_multiplier)).astype(np.float32)\n",
    "                        \n",
    "                        aeerror, tderror = self.mainQN.train_fn(old_state_batch, targetQ, action_vector)\n",
    "                        episode_aeerrors = np.append(episode_aeerrors, aeerror)\n",
    "                else:\n",
    "                    train_batch = self.experience_storage.sample(self.batch_size)\n",
    "                    old_state_batch = np.stack(train_batch[:,0])\n",
    "                    new_state_batch = np.stack(train_batch[:,3])\n",
    "                    action_vector = (train_batch[:,1]).astype(np.int32)\n",
    "                    aeerror1 = self.mainQN.train_encoder(old_state_batch)\n",
    "                    aeerror2 = self.mainQN.train_encoder(new_state_batch)\n",
    "                    episode_aeerrors = np.append(episode_aeerrors, (aeerror1 + aeerror2)/2)\n",
    "                    errors_vector = self.mainQN.train_predfn(old_state_batch, action_vector, new_state_batch)\n",
    "                    \n",
    "                    if self.maxprerror == None:\n",
    "                        self.maxprerror = np.max(errors_vector)\n",
    "                    else:\n",
    "                        self.maxprerror = max(self.maxprerror, np.max(errors_vector))\n",
    "                    \n",
    "                    self.pretrain_steps -= 1;\n",
    "                    if self.pretrain_steps <= 0:\n",
    "                        self.pretrain_over = True\n",
    "                        \n",
    "                state = new_state\n",
    "                window_state = new_window_state\n",
    "            \n",
    "                if gameover:\n",
    "                    self.window.reset()\n",
    "                    break\n",
    "    \n",
    "            total_reward = np.sum(episode_rewards)\n",
    "            total_aeerror = np.mean(episode_aeerrors)\n",
    "            self.lList.append(iteration)\n",
    "            self.rList.append(total_reward)\n",
    "            self.aeList.append(total_aeerror)\n",
    "            if len(self.rList) % self.print_freq == 0:\n",
    "                tqdm.write(\" \".join([\"========= Episode\", str(episode_num), \"================================================\"]))\n",
    "                tqdm.write(\" \".join([\"Total steps:\", str(self.total_steps)]))\n",
    "                tqdm.write(\" \".join([\"Episode rewards, last 10:\", str(self.rList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"Mean over last\", str(self.print_freq), \"episodes:\", str(np.mean(self.rList[-self.print_freq:]))]))\n",
    "                tqdm.write(\" \".join([\"Episode lengths, last 10:\", str(self.lList[-10:])]))\n",
    "                tqdm.write(\" \".join([\"AEerror, mean over last 10:\", str(np.mean(self.aeList[-10:]))]))\n",
    "                tqdm.write(\"===================================================================\" + \"=\" * len(str(episode_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ddqlmodel, meanwindow = 250):\n",
    "    rlist = [ddqlmodel.rList[0]] * meanwindow + ddqlmodel.rList\n",
    "    x = np.cumsum(ddqlmodel.lList)\n",
    "    y = [np.mean(rlist[k:k+meanwindow]) for k in range(len(rlist) - meanwindow)]\n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-14 15:30:41,561] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "#llenv = LunarLanding_wrapper()\n",
    "cp_env = CartPole_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lparams = {\"grad_clipping\" : 50,\n",
    "           \"learning_rate\" : 0.005,\n",
    "           \"window_size\" : 1,\n",
    "           \"batch_size\" : 8,\n",
    "           \"buffer_size\" : 128,\n",
    "           \"gamma\" : 0.98,\n",
    "           \"MQN_updatefreq\" : 1,\n",
    "           \"TQN_updatefreq\" : 4,\n",
    "           \"TQN_updaterate\" : 0.2,\n",
    "           \"print_freq\" : 500,\n",
    "           \"pretrain_steps\" : 5000,\n",
    "           \"render\" : False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "egreedyagentinfo = {\"agent\" : \"egreedy\",\n",
    "                    \"params\" : {\"startE\": 0.5,\n",
    "                                \"endE\" : 0.1,\n",
    "                                \"anneling_steps\":1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boltzmanagentinfo = {\"agent\" : \"boltzman\",\n",
    "                     \"params\" : {\"startT\": 10,\n",
    "                                 \"endT\" : 1,\n",
    "                                 \"anneling_steps\":10000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddql = DDQL(lparams, cpenv, egreedyagentinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ddql.train(num_episodes = 15000, frame_limit = 500, render = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_rewards(ddql, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "График наград у стратегии исследования с поощрением исследования. Так как эта стратегия влияет на работу агента модифицируя награды у наборов (s_t, a_t, r_t, s_t+1), то с этим алгоритмом применимы предыдущие стратегии исследования(е-жадная и больцман). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produce_experiment(ddql, ddql_init_params, ddql_train_params, experiment_num = 5):\n",
    "    ddql_list = [ddql(**ddql_init_params) for k in range(experiment_num)]\n",
    "    \n",
    "    for k in range(experiment_num):\n",
    "        ddql_list[k].train(**ddql_train_params)\n",
    "        \n",
    "    return ddql_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddql_egreedy_params = {\"lparams\":lparams, \"env\":cp_env, \"agent\":egreedyagentinfo}\n",
    "\n",
    "ddql_train_params = {\"num_episodes\":2500, \"frame_limit\":500, \"render\":False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Episode 499 ================================================\n",
      "Total steps: 23228\n",
      "Episode rewards, last 10: [249.0, 500.0, 63.0, 50.0, 355.0, 500.0, 500.0, 500.0, 500.0, 257.0]\n",
      "Mean over last 500 episodes: 56.456\n",
      "Episode lengths, last 10: [248, 499, 62, 49, 354, 499, 499, 499, 499, 256]\n",
      "AEerror, mean over last 10: 0.200180227662\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 117438\n",
      "Episode rewards, last 10: [131.0, 69.0, 19.0, 13.0, 26.0, 54.0, 245.0, 88.0, 199.0, 150.0]\n",
      "Mean over last 500 episodes: 188.42\n",
      "Episode lengths, last 10: [130, 68, 18, 12, 25, 53, 244, 87, 198, 149]\n",
      "AEerror, mean over last 10: 0.577955947951\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 207329\n",
      "Episode rewards, last 10: [13.0, 120.0, 203.0, 98.0, 96.0, 311.0, 265.0, 80.0, 273.0, 283.0]\n",
      "Mean over last 500 episodes: 179.782\n",
      "Episode lengths, last 10: [12, 119, 202, 97, 95, 310, 264, 79, 272, 282]\n",
      "AEerror, mean over last 10: 0.508435854814\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 289132\n",
      "Episode rewards, last 10: [55.0, 182.0, 13.0, 16.0, 21.0, 13.0, 11.0, 19.0, 14.0, 27.0]\n",
      "Mean over last 500 episodes: 163.606\n",
      "Episode lengths, last 10: [54, 181, 12, 15, 20, 12, 10, 18, 13, 26]\n",
      "AEerror, mean over last 10: 0.641377862363\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 378681\n",
      "Episode rewards, last 10: [10.0, 15.0, 87.0, 203.0, 193.0, 59.0, 12.0, 500.0, 136.0, 500.0]\n",
      "Mean over last 500 episodes: 179.098\n",
      "Episode lengths, last 10: [9, 14, 86, 202, 192, 58, 11, 499, 135, 499]\n",
      "AEerror, mean over last 10: 0.592230008518\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 40130\n",
      "Episode rewards, last 10: [262.0, 226.0, 162.0, 135.0, 69.0, 36.0, 158.0, 119.0, 95.0, 22.0]\n",
      "Mean over last 500 episodes: 90.26\n",
      "Episode lengths, last 10: [261, 225, 161, 134, 68, 35, 157, 118, 94, 21]\n",
      "AEerror, mean over last 10: 0.637985760468\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 153691\n",
      "Episode rewards, last 10: [178.0, 50.0, 131.0, 500.0, 103.0, 119.0, 23.0, 16.0, 19.0, 13.0]\n",
      "Mean over last 500 episodes: 227.122\n",
      "Episode lengths, last 10: [177, 49, 130, 499, 102, 118, 22, 15, 18, 12]\n",
      "AEerror, mean over last 10: 0.578644143022\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 267157\n",
      "Episode rewards, last 10: [133.0, 135.0, 336.0, 234.0, 229.0, 203.0, 23.0, 282.0, 102.0, 42.0]\n",
      "Mean over last 500 episodes: 226.932\n",
      "Episode lengths, last 10: [132, 134, 335, 233, 228, 202, 22, 281, 101, 41]\n",
      "AEerror, mean over last 10: 0.668943825571\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 358803\n",
      "Episode rewards, last 10: [123.0, 159.0, 295.0, 108.0, 63.0, 84.0, 244.0, 118.0, 22.0, 18.0]\n",
      "Mean over last 500 episodes: 183.292\n",
      "Episode lengths, last 10: [122, 158, 294, 107, 62, 83, 243, 117, 21, 17]\n",
      "AEerror, mean over last 10: 0.7237237183\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 462621\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 500.0, 177.0, 500.0, 19.0, 40.0, 30.0]\n",
      "Mean over last 500 episodes: 207.636\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 499, 176, 499, 18, 39, 29]\n",
      "AEerror, mean over last 10: 0.472976756687\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 1582\n",
      "Episode rewards, last 10: [9.0, 10.0, 9.0, 10.0, 8.0, 10.0, 9.0, 10.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 13.164\n",
      "Episode lengths, last 10: [8, 9, 8, 9, 7, 9, 8, 9, 9, 9]\n",
      "AEerror, mean over last 10: 0.26175589773\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 6498\n",
      "Episode rewards, last 10: [9.0, 11.0, 9.0, 10.0, 10.0, 12.0, 10.0, 9.0, 11.0, 10.0]\n",
      "Mean over last 500 episodes: 9.832\n",
      "Episode lengths, last 10: [8, 10, 8, 9, 9, 11, 9, 8, 10, 9]\n",
      "AEerror, mean over last 10: 0.253108603036\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 11509\n",
      "Episode rewards, last 10: [10.0, 9.0, 12.0, 11.0, 9.0, 10.0, 14.0, 10.0, 11.0, 11.0]\n",
      "Mean over last 500 episodes: 10.022\n",
      "Episode lengths, last 10: [9, 8, 11, 10, 8, 9, 13, 9, 10, 10]\n",
      "AEerror, mean over last 10: 0.234910307718\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 16479\n",
      "Episode rewards, last 10: [11.0, 10.0, 10.0, 10.0, 10.0, 12.0, 9.0, 9.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 9.94\n",
      "Episode lengths, last 10: [10, 9, 9, 9, 9, 11, 8, 8, 9, 9]\n",
      "AEerror, mean over last 10: 0.220013613291\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 21465\n",
      "Episode rewards, last 10: [9.0, 10.0, 11.0, 11.0, 10.0, 9.0, 13.0, 8.0, 9.0, 9.0]\n",
      "Mean over last 500 episodes: 9.972\n",
      "Episode lengths, last 10: [8, 9, 10, 10, 9, 8, 12, 7, 8, 8]\n",
      "AEerror, mean over last 10: 0.223861141565\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 29205\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 216.0, 90.0, 15.0, 25.0, 68.0, 11.0]\n",
      "Mean over last 500 episodes: 68.41\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 215, 89, 14, 24, 67, 10]\n",
      "AEerror, mean over last 10: 0.409661565888\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 132479\n",
      "Episode rewards, last 10: [154.0, 16.0, 430.0, 500.0, 500.0, 500.0, 500.0, 13.0, 12.0, 47.0]\n",
      "Mean over last 500 episodes: 206.548\n",
      "Episode lengths, last 10: [153, 15, 429, 499, 499, 499, 499, 12, 11, 46]\n",
      "AEerror, mean over last 10: 0.413899183028\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 212805\n",
      "Episode rewards, last 10: [84.0, 500.0, 362.0, 11.0, 10.0, 57.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 160.652\n",
      "Episode lengths, last 10: [83, 499, 361, 10, 9, 56, 499, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.580384344211\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 313898\n",
      "Episode rewards, last 10: [158.0, 42.0, 40.0, 54.0, 500.0, 292.0, 500.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 202.186\n",
      "Episode lengths, last 10: [157, 41, 39, 53, 499, 291, 499, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.374166291704\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 415909\n",
      "Episode rewards, last 10: [500.0, 451.0, 500.0, 16.0, 15.0, 33.0, 16.0, 26.0, 74.0, 47.0]\n",
      "Mean over last 500 episodes: 204.022\n",
      "Episode lengths, last 10: [499, 450, 499, 15, 14, 32, 15, 25, 73, 46]\n",
      "AEerror, mean over last 10: 0.536002882634\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 11942\n",
      "Episode rewards, last 10: [22.0, 24.0, 37.0, 500.0, 227.0, 67.0, 242.0, 500.0, 500.0, 291.0]\n",
      "Mean over last 500 episodes: 33.884\n",
      "Episode lengths, last 10: [21, 23, 36, 499, 226, 66, 241, 499, 499, 290]\n",
      "AEerror, mean over last 10: 0.338179912777\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 115666\n",
      "Episode rewards, last 10: [127.0, 11.0, 500.0, 500.0, 500.0, 419.0, 88.0, 12.0, 32.0, 29.0]\n",
      "Mean over last 500 episodes: 207.448\n",
      "Episode lengths, last 10: [126, 10, 499, 499, 499, 418, 87, 11, 31, 28]\n",
      "AEerror, mean over last 10: 0.622976139907\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 214752\n",
      "Episode rewards, last 10: [125.0, 12.0, 150.0, 24.0, 27.0, 303.0, 205.0, 215.0, 132.0, 112.0]\n",
      "Mean over last 500 episodes: 198.172\n",
      "Episode lengths, last 10: [124, 11, 149, 23, 26, 302, 204, 214, 131, 111]\n",
      "AEerror, mean over last 10: 0.634863793311\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 286365\n",
      "Episode rewards, last 10: [14.0, 69.0, 11.0, 17.0, 23.0, 27.0, 19.0, 32.0, 15.0, 92.0]\n",
      "Mean over last 500 episodes: 143.226\n",
      "Episode lengths, last 10: [13, 68, 10, 16, 22, 26, 18, 31, 14, 91]\n",
      "AEerror, mean over last 10: 0.478538226656\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 375763\n",
      "Episode rewards, last 10: [93.0, 23.0, 500.0, 11.0, 500.0, 500.0, 204.0, 229.0, 13.0, 20.0]\n",
      "Mean over last 500 episodes: 178.796\n",
      "Episode lengths, last 10: [92, 22, 499, 10, 499, 499, 203, 228, 12, 19]\n",
      "AEerror, mean over last 10: 0.483182307194\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 1495\n",
      "Episode rewards, last 10: [10.0, 9.0, 11.0, 11.0, 9.0, 10.0, 10.0, 9.0, 9.0, 9.0]\n",
      "Mean over last 500 episodes: 12.99\n",
      "Episode lengths, last 10: [9, 8, 10, 10, 8, 9, 9, 8, 8, 8]\n",
      "AEerror, mean over last 10: 0.759777570414\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 6419\n",
      "Episode rewards, last 10: [9.0, 10.0, 9.0, 13.0, 11.0, 12.0, 11.0, 10.0, 11.0, 10.0]\n",
      "Mean over last 500 episodes: 9.848\n",
      "Episode lengths, last 10: [8, 9, 8, 12, 10, 11, 10, 9, 10, 9]\n",
      "AEerror, mean over last 10: 0.662805932511\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 11367\n",
      "Episode rewards, last 10: [8.0, 9.0, 11.0, 10.0, 8.0, 10.0, 12.0, 10.0, 10.0, 9.0]\n",
      "Mean over last 500 episodes: 9.896\n",
      "Episode lengths, last 10: [7, 8, 10, 9, 7, 9, 11, 9, 9, 8]\n",
      "AEerror, mean over last 10: 0.769023555496\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 16307\n",
      "Episode rewards, last 10: [10.0, 9.0, 9.0, 10.0, 11.0, 9.0, 10.0, 10.0, 10.0, 10.0]\n",
      "Mean over last 500 episodes: 9.88\n",
      "Episode lengths, last 10: [9, 8, 8, 9, 10, 8, 9, 9, 9, 9]\n",
      "AEerror, mean over last 10: 0.758731151081\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 21215\n",
      "Episode rewards, last 10: [10.0, 10.0, 9.0, 8.0, 9.0, 13.0, 10.0, 9.0, 8.0, 8.0]\n",
      "Mean over last 500 episodes: 9.816\n",
      "Episode lengths, last 10: [9, 9, 8, 7, 8, 12, 9, 8, 7, 7]\n",
      "AEerror, mean over last 10: 0.700863272253\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 9588\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 451.0, 33.0, 500.0, 500.0, 454.0, 90.0, 77.0]\n",
      "Mean over last 500 episodes: 29.176\n",
      "Episode lengths, last 10: [499, 499, 499, 450, 32, 499, 499, 453, 89, 76]\n",
      "AEerror, mean over last 10: 0.305300792031\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 114838\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 249.0, 306.0, 21.0, 70.0, 500.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 210.5\n",
      "Episode lengths, last 10: [499, 499, 499, 248, 305, 20, 69, 499, 499, 499]\n",
      "AEerror, mean over last 10: 0.235052324576\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 222933\n",
      "Episode rewards, last 10: [215.0, 228.0, 368.0, 167.0, 500.0, 261.0, 500.0, 500.0, 500.0, 154.0]\n",
      "Mean over last 500 episodes: 216.19\n",
      "Episode lengths, last 10: [214, 227, 367, 166, 499, 260, 499, 499, 499, 153]\n",
      "AEerror, mean over last 10: 0.308567863115\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 318123\n",
      "Episode rewards, last 10: [500.0, 171.0, 18.0, 36.0, 10.0, 500.0, 500.0, 500.0, 500.0, 13.0]\n",
      "Mean over last 500 episodes: 190.38\n",
      "Episode lengths, last 10: [499, 170, 17, 35, 9, 499, 499, 499, 499, 12]\n",
      "AEerror, mean over last 10: 0.212366307813\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 417513\n",
      "Episode rewards, last 10: [500.0, 500.0, 500.0, 500.0, 15.0, 13.0, 43.0, 500.0, 339.0, 14.0]\n",
      "Mean over last 500 episodes: 198.78\n",
      "Episode lengths, last 10: [499, 499, 499, 499, 14, 12, 42, 499, 338, 13]\n",
      "AEerror, mean over last 10: 0.603412340372\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 6098\n",
      "Episode rewards, last 10: [190.0, 206.0, 191.0, 210.0, 230.0, 308.0, 176.0, 43.0, 33.0, 17.0]\n",
      "Mean over last 500 episodes: 22.196\n",
      "Episode lengths, last 10: [189, 205, 190, 209, 229, 307, 175, 42, 32, 16]\n",
      "AEerror, mean over last 10: 0.359073143467\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 115061\n",
      "Episode rewards, last 10: [338.0, 500.0, 448.0, 89.0, 150.0, 122.0, 54.0, 11.0, 101.0, 148.0]\n",
      "Mean over last 500 episodes: 217.926\n",
      "Episode lengths, last 10: [337, 499, 447, 88, 149, 121, 53, 10, 100, 147]\n",
      "AEerror, mean over last 10: 0.618279056026\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 197000\n",
      "Episode rewards, last 10: [26.0, 11.0, 98.0, 116.0, 110.0, 22.0, 31.0, 132.0, 500.0, 500.0]\n",
      "Mean over last 500 episodes: 163.878\n",
      "Episode lengths, last 10: [25, 10, 97, 115, 109, 21, 30, 131, 499, 499]\n",
      "AEerror, mean over last 10: 0.448157847773\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 282402\n",
      "Episode rewards, last 10: [42.0, 84.0, 13.0, 25.0, 12.0, 71.0, 30.0, 242.0, 286.0, 169.0]\n",
      "Mean over last 500 episodes: 170.804\n",
      "Episode lengths, last 10: [41, 83, 12, 24, 11, 70, 29, 241, 285, 168]\n",
      "AEerror, mean over last 10: 0.368937624423\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 371064\n",
      "Episode rewards, last 10: [63.0, 72.0, 49.0, 68.0, 26.0, 55.0, 60.0, 14.0, 56.0, 88.0]\n",
      "Mean over last 500 episodes: 177.324\n",
      "Episode lengths, last 10: [62, 71, 48, 67, 25, 54, 59, 13, 55, 87]\n",
      "AEerror, mean over last 10: 0.318508808521\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 1497\n",
      "Episode rewards, last 10: [9.0, 11.0, 8.0, 12.0, 8.0, 10.0, 12.0, 9.0, 8.0, 10.0]\n",
      "Mean over last 500 episodes: 12.994\n",
      "Episode lengths, last 10: [8, 10, 7, 11, 7, 9, 11, 8, 7, 9]\n",
      "AEerror, mean over last 10: 0.229283467025\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 6373\n",
      "Episode rewards, last 10: [10.0, 10.0, 10.0, 10.0, 10.0, 9.0, 10.0, 11.0, 9.0, 10.0]\n",
      "Mean over last 500 episodes: 9.752\n",
      "Episode lengths, last 10: [9, 9, 9, 9, 9, 8, 9, 10, 8, 9]\n",
      "AEerror, mean over last 10: 0.206939575602\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 11347\n",
      "Episode rewards, last 10: [9.0, 9.0, 11.0, 10.0, 12.0, 10.0, 9.0, 9.0, 14.0, 12.0]\n",
      "Mean over last 500 episodes: 9.948\n",
      "Episode lengths, last 10: [8, 8, 10, 9, 11, 9, 8, 8, 13, 11]\n",
      "AEerror, mean over last 10: 0.224835513761\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 16240\n",
      "Episode rewards, last 10: [10.0, 10.0, 9.0, 10.0, 9.0, 11.0, 10.0, 10.0, 11.0, 10.0]\n",
      "Mean over last 500 episodes: 9.786\n",
      "Episode lengths, last 10: [9, 9, 8, 9, 8, 10, 9, 9, 10, 9]\n",
      "AEerror, mean over last 10: 0.234659503335\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 21176\n",
      "Episode rewards, last 10: [9.0, 9.0, 10.0, 9.0, 11.0, 10.0, 10.0, 10.0, 8.0, 17.0]\n",
      "Mean over last 500 episodes: 9.872\n",
      "Episode lengths, last 10: [8, 8, 9, 8, 10, 9, 9, 9, 7, 16]\n",
      "AEerror, mean over last 10: 0.212087970469\n",
      "=======================================================================\n",
      "\n",
      "========= Episode 499 ================================================\n",
      "Total steps: 1561\n",
      "Episode rewards, last 10: [11.0, 10.0, 10.0, 9.0, 9.0, 10.0, 9.0, 10.0, 10.0, 9.0]\n",
      "Mean over last 500 episodes: 13.122\n",
      "Episode lengths, last 10: [10, 9, 9, 8, 8, 9, 8, 9, 9, 8]\n",
      "AEerror, mean over last 10: 0.798893468346\n",
      "======================================================================\n",
      "========= Episode 999 ================================================\n",
      "Total steps: 6493\n",
      "Episode rewards, last 10: [15.0, 8.0, 10.0, 8.0, 9.0, 9.0, 9.0, 12.0, 9.0, 8.0]\n",
      "Mean over last 500 episodes: 9.864\n",
      "Episode lengths, last 10: [14, 7, 9, 7, 8, 8, 8, 11, 8, 7]\n",
      "AEerror, mean over last 10: 0.735387327485\n",
      "======================================================================\n",
      "========= Episode 1499 ================================================\n",
      "Total steps: 11403\n",
      "Episode rewards, last 10: [8.0, 10.0, 10.0, 10.0, 8.0, 9.0, 8.0, 9.0, 9.0, 10.0]\n",
      "Mean over last 500 episodes: 9.82\n",
      "Episode lengths, last 10: [7, 9, 9, 9, 7, 8, 7, 8, 8, 9]\n",
      "AEerror, mean over last 10: 0.743999173882\n",
      "=======================================================================\n",
      "========= Episode 1999 ================================================\n",
      "Total steps: 16391\n",
      "Episode rewards, last 10: [10.0, 11.0, 9.0, 9.0, 11.0, 10.0, 10.0, 10.0, 13.0, 9.0]\n",
      "Mean over last 500 episodes: 9.976\n",
      "Episode lengths, last 10: [9, 10, 8, 8, 10, 9, 9, 9, 12, 8]\n",
      "AEerror, mean over last 10: 0.796388807217\n",
      "=======================================================================\n",
      "========= Episode 2499 ================================================\n",
      "Total steps: 21306\n",
      "Episode rewards, last 10: [10.0, 11.0, 10.0, 9.0, 10.0, 8.0, 9.0, 9.0, 12.0, 10.0]\n",
      "Mean over last 500 episodes: 9.83\n",
      "Episode lengths, last 10: [9, 10, 9, 8, 9, 7, 8, 8, 11, 9]\n",
      "AEerror, mean over last 10: 0.853385124102\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = produce_experiment(DDQL, ddql_egreedy_params, ddql_train_params, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(ddql_res, window = 100, std_coef = 0.2, results_over = 1000):\n",
    "    res_lists = [k.rList for k in ddql_res]\n",
    "    res_lists = np.array(res_lists)\n",
    "    pd.DataFrame(data = res_lists)\n",
    "    mean = res_lists.mean(axis = 0)\n",
    "    std = res_lists.std(axis = 0)\n",
    "    rol_mean = np.nan_to_num(pd.Series(mean).rolling(window = window).mean())\n",
    "    rol_std = np.nan_to_num(pd.Series(std).rolling(window = window).mean())\n",
    "    plt.figure()\n",
    "    index = np.arange(len(rol_mean))\n",
    "    plt.plot(index, rol_mean)\n",
    "    plt.fill_between(index, rol_mean-std_coef*rol_std, rol_mean+std_coef*rol_std, color='b', alpha=0.1)\n",
    "    return max(rol_mean[0:results_over]), rol_mean[0:results_over].mean(), pd.DataFrame(data = res_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnmEYpsH5/wEk21lx5PML2wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116612c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inc_max, inc_auc, inc_df = plot_results(res, 100, 0.2)\n",
    "plt.savefig(\"inc_cp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.567 75.383885\n"
     ]
    }
   ],
   "source": [
    "print inc_max, inc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inc_df.to_csv(\"inc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {
    "10ff868b77e745fdb2ff913c000e37e1": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "3a71886fb71d4964ba06ea0b90f0d2ec": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "5e60f72ef143478686a9890a6c748a93": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "672ef21c890c4f1d958d0a9d4547a13f": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "7045d7e86dad4801b1cef44818b60361": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "859bc363a87641cba114ff9845c58274": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "878b8117d2fd4a50ba1abf2271ffaef1": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "8cdc574945a542ad88e7d418e09f6729": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "9484ee9c96974cdf940cb1fd06b56bc0": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "ad4281ff204640a98844cb0c13dce786": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "c1a95eda7e7d441788f8458d41e4040f": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "c4f63b99dbae4b4ab8296bf61016da2b": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "e29672c32b144b3ba8bd13c4a2c1e889": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "e6dfbad2ad264f0187bb0fcc501fb5d3": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "ed4c7c3609b44a59a42f1d83fe0b7e54": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
